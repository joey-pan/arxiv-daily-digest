[
  {
    "id": "2601.16836",
    "title": "ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models",
    "abstract": "While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.",
    "authors": [
      "Chenxi Ruan",
      "Yu Xiao",
      "Yihan Hou",
      "Guosheng Hu",
      "Wei Zeng"
    ],
    "published": "2026-01-23",
    "updated": "2026-01-23",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.16836.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16836",
    "primary_category": "cs.CV",
    "relevance_score": 6.0,
    "summary": {
      "title_zh": "ColorConceptBench：一个用于评估文本到图像模型中概率性颜色-概念理解能力的基准",
      "core_contribution": "提出了首个系统评估文本到图像模型对隐含颜色概念理解能力的人工标注基准，揭示了当前模型在抽象语义颜色关联上的根本性缺陷。",
      "method": "构建了包含1,281个隐含颜色概念和6,369个人工标注的数据集，通过概率性颜色分布而非显式颜色名称来评估模型。对七种主流文本到图像模型进行了系统性测试，并尝试了模型缩放、引导增强等标准干预措施。",
      "findings": "当前文本到图像模型对抽象语义的颜色关联缺乏敏感性，且这种局限性无法通过常规干预手段（如扩大模型规模或增强引导）有效改善，表明实现类人的颜色语义理解需要模型学习与表征隐含意义的方式发生根本性转变。"
    }
  },
  {
    "id": "2601.19433",
    "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
    "abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.",
    "authors": [
      "Jisheng Chu",
      "Wenrui Li",
      "Rui Zhao",
      "Wangmeng Zuo",
      "Shifeng Chen",
      "Xiaopeng Fan"
    ],
    "published": "2026-01-27",
    "updated": "2026-01-27",
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.19433.pdf",
    "abs_url": "https://arxiv.org/abs/2601.19433",
    "primary_category": "cs.CV",
    "relevance_score": 4.0,
    "summary": {
      "title_zh": "RoamScene3D：通过自适应对象感知漫游实现沉浸式文本到3D场景生成",
      "core_contribution": "提出了RoamScene3D框架，通过语义推理与几何约束相结合，解决了现有方法在生成3D场景时存在的空间盲区、轨迹僵化以及遮挡内容推断困难的问题，显著提升了场景的一致性与真实感。",
      "method": "首先，利用视觉语言模型构建编码对象关系的场景图，以指导相机感知显著对象边界并规划自适应的漫游轨迹。其次，针对静态2D先验的不足，提出了运动注入修复模型，该模型在集成了真实相机轨迹的合成全景数据集上进行微调，从而能够适应相机运动，更合理地填充因视角变化产生的空洞。",
      "findings": "大量实验表明，该方法在生成一致且逼真的3D场景方面显著优于现有先进方法。通过语义推理和几何约束的有效结合，RoamScene3D能够更好地理解场景语义布局，并自适应地探索场景以推断被遮挡的内容。"
    }
  },
  {
    "id": "2601.18585",
    "title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization",
    "abstract": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.",
    "authors": [
      "Chenxi Liu",
      "Selena Ling",
      "Alec Jacobson"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18585.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18585",
    "primary_category": "cs.CV",
    "relevance_score": 4.0,
    "summary": {
      "title_zh": "GimmBO：基于贝叶斯优化的交互式生成图像模型融合",
      "core_contribution": "提出了一个名为GimmBO的交互式系统，通过偏好贝叶斯优化（PBO）帮助用户高效探索扩散模型适配器（adapter）的权重融合空间，解决了手动调节权重时维度灾难和效率低下的问题。",
      "method": "该方法采用两阶段贝叶斯优化后端：首先，针对真实使用中观察到的权重稀疏性和范围受限特性，设计了一种高效的采样策略；其次，通过交互式偏好反馈引导优化过程，使用户能够以直观的偏好比较（而非精确数值）来探索高维权重组合空间。",
      "findings": "实验表明，GimmBO在模拟用户测试和真实用户研究中均表现出更快的收敛速度、更高的成功率，且显著优于传统贝叶斯优化和线性搜索基线；框架还展示了良好的扩展性，可支持多种适配器融合场景。"
    }
  },
  {
    "id": "2601.16981",
    "title": "SyncLight: Controllable and Consistent Multi-View Relighting",
    "abstract": "We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.",
    "authors": [
      "David Serrano-Lozano",
      "Anand Bhattad",
      "Luis Herranz",
      "Jean-François Lalonde",
      "Javier Vazquez-Corral"
    ],
    "published": "2026-01-23",
    "updated": "2026-01-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.16981.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16981",
    "primary_category": "cs.CV",
    "relevance_score": 3.0,
    "summary": {
      "title_zh": "SyncLight：可控且一致的多视角重光照",
      "core_contribution": "提出了首个能够在未标定的多视角静态场景图像中实现参数化、一致重光照的方法，解决了现有生成方法在多视角应用中难以保持严格光照一致性的关键问题。",
      "method": "该方法采用基于潜在桥匹配公式训练的多视角扩散Transformer模型，通过单次推理即可对整个图像集进行高保真重光照。训练使用了一个大规模混合数据集，包含多样化的合成环境（来自现有资源和新设计场景）以及在标定光照下的高保真实世界多视角采集数据。",
      "findings": "尽管仅使用图像对进行训练，SyncLight能够零样本泛化到任意数量的视角，有效地将光照变化传播到所有视图中，且无需相机位姿信息。这为多视角采集系统实现了实用的重光照工作流程。"
    }
  },
  {
    "id": "2601.19717",
    "title": "DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization",
    "abstract": "3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.",
    "authors": [
      "Yitong Yang",
      "Xuexin Liu",
      "Yinglin Wang",
      "Jing Wang",
      "Hao Dou",
      "Changshuo Wang",
      "Shuting He"
    ],
    "published": "2026-01-27",
    "updated": "2026-01-27",
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.19717.pdf",
    "abs_url": "https://arxiv.org/abs/2601.19717",
    "primary_category": "cs.CV",
    "relevance_score": 3.0,
    "summary": {
      "title_zh": "DiffStyle3D：通过注意力优化实现一致的3D高斯风格化",
      "core_contribution": "提出了一种基于扩散模型的新型3D高斯风格化范式，通过直接在潜在空间进行优化，解决了现有方法在保持多视图一致性方面的不足。",
      "method": "该方法首先引入注意力感知损失，通过在自注意力空间对齐风格特征并保留内容特征来实现风格迁移。其次，受3D风格化几何不变性启发，提出几何引导的多视图一致性方法，将几何信息融入自注意力以建模跨视图对应关系。此外，基于几何信息构建几何感知掩码，避免多视图重叠区域的冗余优化。",
      "findings": "大量实验表明，DiffStyle3D在风格化质量和视觉真实感方面均优于现有先进方法，能够生成更一致且高质量的风格化3D内容。"
    }
  }
]