[
  {
    "id": "2601.17673",
    "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
    "abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.",
    "authors": [
      "Weiyu Zhang",
      "Yuan Hu",
      "Yong Li",
      "Yu Liu"
    ],
    "published": "2026-01-25",
    "updated": "2026-01-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.17673.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17673",
    "primary_category": "cs.CV",
    "relevance_score": 5.0,
    "summary": {
      "title_zh": "Uni-RS：一种面向遥感、空间保真的统一理解与生成模型",
      "core_contribution": "本文提出了首个面向遥感领域的统一多模态模型Uni-RS，旨在显式解决遥感图像理解与生成任务之间的空间不对称性问题，即缓解现有模型在文本生成图像时难以忠实执行空间关系的“空间反转诅咒”。",
      "method": "首先，引入显式的空间布局规划模块，将文本指令转化为空间布局规划，实现几何规划与视觉合成的解耦。其次，通过空间感知查询监督机制，使可学习查询偏向于指令中明确指定的空间关系。最后，开发图像-描述空间布局变换方法，让模型系统性地接触几何一致的空间变换数据以增强泛化能力。",
      "findings": "在多个基准测试上的实验表明，该方法显著提升了文本到图像生成任务中的空间忠实度，同时在图像描述、视觉定位、视觉问答等多模态理解任务上保持了强大的性能。"
    }
  },
  {
    "id": "2601.19606",
    "title": "GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining",
    "abstract": "Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.",
    "authors": [
      "Shentong Mo",
      "Zehua Chen",
      "Jun Zhu"
    ],
    "published": "2026-01-27",
    "updated": "2026-01-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.19606.pdf",
    "abs_url": "https://arxiv.org/abs/2601.19606",
    "primary_category": "cs.CV",
    "relevance_score": 3.0,
    "summary": {
      "title_zh": "GMS-CAVP：通过多尺度对比与生成式预训练改进音视频对应关系",
      "core_contribution": "提出了一种结合多尺度对比学习和扩散生成目标的新型音视频对应建模框架，显著提升了跨模态检索与生成任务的性能。",
      "method": "首先，设计多尺度对比学习策略，从细粒度到粗粒度捕捉音视频的语义与时序对应关系；其次，引入基于扩散模型的生成目标，实现音视频间的跨模态转换与合成；最终通过判别与生成目标的统一框架，促进更深层次的跨模态理解。",
      "findings": "在VGGSound、AudioSet和Panda70M数据集上的实验表明，GMS-CAVP在生成与检索任务上均优于现有方法，验证了多尺度建模与生成式目标对提升音视频对应关系的有效性。"
    }
  },
  {
    "id": "2601.17917",
    "title": "Streaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding",
    "abstract": "Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.",
    "authors": [
      "Zhongyu Xiao",
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Yong Luo",
      "Jia Liu",
      "Jie Xu",
      "Han Hu"
    ],
    "published": "2026-01-25",
    "updated": "2026-01-27",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.17917.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17917",
    "primary_category": "cs.LG",
    "relevance_score": 4.0,
    "summary": {
      "title_zh": "Streaming-dLLM：通过后缀剪枝与动态解码加速扩散大语言模型",
      "core_contribution": "提出了Streaming-dLLM，一个无需训练即可在空间和时间维度上同时优化扩散大语言模型推理效率的框架，通过剪枝冗余后缀标记和动态调整去噪过程来显著加速生成。",
      "method": "在空间维度上，引入衰减引导的后缀建模，通过剪枝信息稀疏的后缀区域中的冗余掩码标记来近似完整上下文；在时间维度上，采用动态置信度感知策略与提前退出机制，允许模型对已收敛的标记跳过不必要的去噪迭代。",
      "findings": "实验表明，Streaming-dLLM在保持生成质量的同时，最高可实现68.2倍的推理加速，有效解决了扩散解码过程中的空间冗余与时间效率低下的问题。"
    }
  },
  {
    "id": "2601.17857",
    "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction",
    "abstract": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.",
    "authors": [
      "Lan Yang",
      "Minghan Yang",
      "Ke Li",
      "Honggang Zhang",
      "Kaiyue Pang",
      "Yi-Zhe Song"
    ],
    "published": "2026-01-25",
    "updated": "2026-01-25",
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.17857.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17857",
    "primary_category": "cs.CV",
    "relevance_score": 5.0,
    "summary": {
      "title_zh": "SynMind：减少基于fMRI的图像重建中的语义幻觉",
      "core_contribution": "本文提出SynMind框架，通过将fMRI信号解析为明确的、句子级别的语义描述，显著减少了图像重建中的语义错位问题，实现了更符合人类视觉感知的重建结果。",
      "method": "该方法首先利用基于视觉语言模型生成合成、类人的多粒度文本表示，以捕获目标图像的物体身份和空间结构；然后将这些明确的语义编码与视觉先验结合，共同作为预训练扩散模型的条件输入，从而在重建过程中分离语义推理与外观生成。",
      "findings": "实验表明，SynMind在多数定量指标上优于现有方法，且仅使用较小的Stable Diffusion 1.4模型和单张消费级GPU即可超越基于SDXL的对比方法；大规模人工评估证实其重建结果与人类视觉感知更一致；神经可视化分析显示，SynMind激活了更广泛且语义相关的大脑区域，减少了对高级视觉区域的过度依赖。"
    }
  },
  {
    "id": "2601.17883",
    "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
    "abstract": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.",
    "authors": [
      "Dingkun Liu",
      "Yuheng Chen",
      "Zhu Chen",
      "Zhenyao Cui",
      "Yaozhi Wen",
      "Jiayu An",
      "Jingwei Luo",
      "Dongrui Wu"
    ],
    "published": "2026-01-25",
    "updated": "2026-01-25",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.17883.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17883",
    "primary_category": "cs.LG",
    "relevance_score": 1.0,
    "summary": {
      "title_zh": "脑电图基础模型：进展、基准测试与开放性问题",
      "core_contribution": "本文首次对现有EEG基础模型进行了系统性的回顾与公平全面的基准测试，揭示了当前模型在跨被试泛化与少样本校准等实际场景中的表现与局限。",
      "method": "首先回顾了50个代表性模型，将其设计选择统一归纳为数据标准化、模型架构和自监督预训练策略三个维度；随后在涵盖9种BCI范式的13个EEG数据集上，对12个开源基础模型及专业基线模型进行了评估，重点考察了留一被试跨被试泛化与少样本被试内校准两种实际部署场景，并对比了全参数微调与线性探测两种迁移策略。",
      "findings": "实验结果表明：1）线性探测通常不足以充分迁移预训练表征；2）从头训练的专业模型在许多任务上仍具有竞争力；3）在当前数据规模和训练方式下，更大的基础模型未必带来更好的泛化性能。"
    }
  }
]