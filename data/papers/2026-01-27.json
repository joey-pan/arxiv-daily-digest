[
  {
    "id": "2601.18698",
    "title": "Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge",
    "abstract": "Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.",
    "authors": [
      "Xiao Liu",
      "Jiawei Zhang"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18698.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18698",
    "primary_category": "cs.CV",
    "relevance_score": 15.0,
    "summary": {
      "title_zh": "视频生成模型具有地理公平性吗？基于旅游景点的全球视觉知识评估",
      "core_contribution": "提出了一个系统框架（GAP）和基准数据集（GEOATTRACTION-500），用于评估文本到视频生成模型在地理视觉知识上的公平性与准确性，并发现当前最先进的模型（如Sora 2）展现出的地理偏见比预期更弱。",
      "method": "研究设计了地理景点地标探测框架（GAP），通过构建包含全球500个不同地区和知名度景点的基准数据集（GEOATTRACTION-500），采用多维度评估指标：包括全局结构对齐、基于关键点的细粒度对齐以及视觉语言模型判断，并将这些指标与人类评估结果进行验证，以区分视频整体质量与景点特定知识。",
      "findings": "对Sora 2模型的评估显示，与常见的地理偏见假设相反，模型在不同地区、发展水平和文化群体中表现出相对均匀的地理视觉知识，仅与景点知名度有微弱关联。这表明当前文本到视频模型对全球视觉知识的表达比预期更均衡，既展现了其在全球应用中的潜力，也强调了随着系统发展需要持续评估。"
    }
  },
  {
    "id": "2601.18543",
    "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
    "abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}.",
    "authors": [
      "Kaixun Jiang",
      "Yuzheng Wang",
      "Junjie Zhou",
      "Pandeng Li",
      "Zhihang Liu",
      "Chen-Wei Xie",
      "Zhaoyu Chen",
      "Yun Zheng",
      "Wenqiang Zhang"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18543.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18543",
    "primary_category": "cs.CV",
    "relevance_score": 15.0,
    "summary": {
      "title_zh": "GenAgent：通过智能体多模态推理扩展文本到图像生成",
      "core_contribution": "提出了GenAgent，一个通过智能体框架将视觉理解与生成解耦的统一多模态模型，它利用多轮自主交互和链式思维来迭代优化图像生成，避免了传统统一模型的高训练成本和能力权衡问题。",
      "method": "GenAgent采用智能体框架，由多模态模型负责视觉理解，将图像生成模型作为可调用的工具。通过两阶段训练策略：首先使用高质量工具调用和反思数据进行监督微调以启动智能体行为；然后进行端到端的智能体强化学习，结合最终图像质量的点奖励和反思准确性的对奖励，并利用轨迹重采样增强多轮探索。",
      "findings": "GenAgent显著提升了基础生成器（FLUX.1-dev）在GenEval++（+23.6%）和WISE（+14%）基准上的性能。框架展现出三个关键特性：1）能够泛化到不同能力的生成器；2）测试时可通过增加交互轮次持续提升效果；3）能根据任务自适应调整推理过程。"
    }
  },
  {
    "id": "2601.18346",
    "title": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception",
    "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.",
    "authors": [
      "Sijing Wu",
      "Yunhao Li",
      "Zicheng Zhang",
      "Qi Jia",
      "Xinyue Li",
      "Huiyu Duan",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18346.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18346",
    "primary_category": "cs.CV",
    "relevance_score": 14.0,
    "summary": {
      "title_zh": "Q-Bench-Portrait：面向人像图像质量感知的多模态大语言模型基准测试",
      "core_contribution": "提出了首个专门针对人像图像质量感知的综合性基准测试Q-Bench-Portrait，并系统评估了25个开源与闭源多模态大语言模型在该领域的表现。",
      "method": "研究构建了一个包含2,765个图像-问题-答案三元组的数据集，涵盖自然图像、合成失真图像、AI生成图像、艺术图像和计算机图形图像等多种人像来源。基准测试设计了涵盖技术失真、AIGC特有失真和美学等多个质量维度的问题，并包含单选、多选、判断和开放式等多种问题形式，同时覆盖全局和局部两个分析层次。",
      "findings": "评估发现，当前的多模态大语言模型虽然具备一定的人像图像感知能力，但其表现仍然有限且不够精确，与人类判断存在明显差距。该基准测试有助于推动通用及领域专用MLLM在人像图像感知能力方面的进一步研究。"
    }
  },
  {
    "id": "2601.18321",
    "title": "Integrating Fine-Grained Audio-Visual Evidence for Robust Multimodal Emotion Reasoning",
    "abstract": "Multimodal emotion analysis is shifting from static classification to generative reasoning. Beyond simple label prediction, robust affective reasoning must synthesize fine-grained signals such as facial micro-expressions and prosodic which shifts to decode the latent causality within complex social contexts. However, current Multimodal Large Language Models (MLLMs) face significant limitations in fine-grained perception, primarily due to data scarcity and insufficient cross-modal fusion. As a result, these models often exhibit unimodal dominance which leads to hallucinations in complex multimodal interactions, particularly when visual and acoustic cues are subtle, ambiguous, or even contradictory (e.g., in sarcastic scenery). To address this, we introduce SABER-LLM, a framework designed for robust multimodal reasoning. First, we construct SABER, a large-scale emotion reasoning dataset comprising 600K video clips, annotated with a novel six-dimensional schema that jointly captures audiovisual cues and causal logic. Second, we propose the structured evidence decomposition paradigm, which enforces a \"perceive-then-reason\" separation between evidence extraction and reasoning to alleviate unimodal dominance. The ability to perceive complex scenes is further reinforced by consistency-aware direct preference optimization, which explicitly encourages alignment among modalities under ambiguous or conflicting perceptual conditions. Experiments on EMER, EmoBench-M, and SABER-Test demonstrate that SABER-LLM significantly outperforms open-source baselines and achieves robustness competitive with closed-source models in decoding complex emotional dynamics. The dataset and model are available at https://github.com/zxzhao0/SABER-LLM.",
    "authors": [
      "Zhixian Zhao",
      "Wenjie Tian",
      "Xiaohai Tian",
      "Jun Zhang",
      "Lei Xie"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.MM",
      "cs.CL",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18321.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18321",
    "primary_category": "cs.MM",
    "relevance_score": 14.0,
    "summary": {
      "title_zh": "融合细粒度视听证据以实现鲁棒的多模态情感推理",
      "core_contribution": "本文提出了SABER-LLM框架，通过构建大规模细粒度情感推理数据集SABER，并设计结构化证据分解范式，有效缓解了多模态大语言模型在复杂场景下的单模态主导与幻觉问题，提升了情感推理的鲁棒性。",
      "method": "方法主要包括：1）构建了包含60万视频片段的大规模情感推理数据集SABER，采用一种新颖的六维标注方案，联合捕捉视听线索与因果逻辑；2）提出了结构化证据分解范式，强制将证据提取与推理过程分离，遵循“先感知后推理”的原则；3）引入一致性感知的直接偏好优化，在感知模糊或冲突的条件下显式鼓励多模态间的对齐，以增强对复杂场景的感知能力。",
      "findings": "在EMER、EmoBench-M和SABER-Test等基准上的实验表明，SABER-LLM显著优于开源基线模型，并且在解码复杂情感动态方面达到了与闭源模型相竞争的鲁棒性。该框架能有效处理视听线索微妙、模糊甚至矛盾的场景（如讽刺情景）。"
    }
  },
  {
    "id": "2601.18785",
    "title": "Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System",
    "abstract": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.",
    "authors": [
      "Tiffany Wang",
      "Yuqian Sun",
      "Yi Wang",
      "Melissa Roemmele",
      "John Joon Young Chung",
      "Max Kreminski"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18785.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18785",
    "primary_category": "cs.HC",
    "relevance_score": 11.0,
    "summary": {
      "title_zh": "基于大语言模型的交互式叙事设计技术：以Dramamancer系统为例",
      "core_contribution": "提出了一个利用大语言模型（LLM）将作者创作的故事框架转化为玩家驱动叙事体验的新范式，并通过Dramamancer系统展示了如何弥合作者意图与玩家自主性之间的鸿沟。",
      "method": "研究以Dramamancer系统为案例，该系统采用大语言模型作为核心引擎，将作者预先设计的故事纲要（story schemas）动态转化为交互式叙事内容。系统设计侧重于利用LLM的生成能力，在保持作者预设故事结构的同时，实时响应玩家的选择与输入，生成连贯的剧情分支与对话。",
      "findings": "论文概述了与此类LLM驱动的交互叙事系统相关的关键设计技术与评估考量，表明大语言模型能够有效支撑一种兼顾作者叙事框架与玩家高度自主性的新型交互故事创作与体验模式。"
    }
  },
  {
    "id": "2601.18512",
    "title": "Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research",
    "abstract": "This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.",
    "authors": [
      "Antonio Garzon-Vico",
      "Krithika Sharon Komalapati",
      "Arsalan Shahid",
      "Jan Rosier"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18512.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18512",
    "primary_category": "cs.CL",
    "relevance_score": 11.0,
    "summary": {
      "title_zh": "利用大语言模型构建虚拟高层管理者：一种组织研究方法",
      "core_contribution": "本研究提出了一个利用大语言模型构建真实高层管理者虚拟人格的方法框架，为在难以直接接触高管的情况下进行组织研究提供了可信且互补的工具。",
      "method": "该方法基于真实CEO的沟通文本和道德基础理论，构建能模拟个体领导者决策的大语言模型参与者。研究通过三个阶段，以人类参与者为基准，对这些虚拟CEO的结构效度、信度和行为保真度进行了评估。",
      "findings": "结果表明，基于理论框架构建的虚拟人格能够近似人类样本中观察到的道德判断，说明基于大语言模型的虚拟人格在无法直接接触高管的研究情境中，可以作为组织研究的有效工具。"
    }
  },
  {
    "id": "2601.18486",
    "title": "Demographic Probing of Large Language Models Lacks Construct Validity",
    "abstract": "Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.",
    "authors": [
      "Manuel Tonneau",
      "Neil K. R. Seghal",
      "Niyati Malhotra",
      "Victor Orozco-Olvera",
      "Ana María Muñoz Boudet",
      "Lakshmi Subramanian",
      "Sharath Chandra Guntuku",
      "Valentin Hofmann"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18486.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18486",
    "primary_category": "cs.CL",
    "relevance_score": 11.0,
    "summary": {
      "title_zh": "大语言模型的人口统计学探测缺乏构念效度",
      "core_contribution": "本文揭示了当前广泛使用的人口统计学探测方法在评估大语言模型行为时缺乏构念效度，即单一人口线索（如姓名、方言）并不能稳定、一致地代表模型对特定人口群体的响应模式。",
      "method": "研究在美国语境下，聚焦种族和性别，通过模拟现实中的寻求建议互动场景进行实验。研究者使用了多种意图代表同一人口群体（如不同种族或性别）的线索（例如不同的名字、方言特征），系统比较这些线索引发的大语言模型行为变化。同时，分析了线索本身对人口属性的编码强度差异以及潜在的语言混淆因素对模型行为的影响。",
      "findings": "关键发现包括：1) 意图代表同一人口群体的不同线索仅能引发模型行为部分重叠的变化，而不同群体间的行为区分度弱且不均匀；2) 基于不同线索估计的模型行为差异（即“差距”）不稳定，其大小和方向会随线索改变而变动；3) 这种不一致性部分源于线索编码人口属性的强度差异，以及独立影响模型行为的语言混淆因素。这些结果表明，单一线索的探测无法获得关于大语言模型如何处理人口信息的单一、稳定的表征。"
    }
  },
  {
    "id": "2601.18631",
    "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
    "abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.",
    "authors": [
      "Mingyang Song",
      "Haoyu Sun",
      "Jiawei Gu",
      "Linjie Li",
      "Luxin Xu",
      "Ranjay Krishna",
      "Yu Cheng"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18631.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18631",
    "primary_category": "cs.AI",
    "relevance_score": 10.0,
    "summary": {
      "title_zh": "AdaReasoner：面向迭代式视觉推理的动态工具编排框架",
      "core_contribution": "提出AdaReasoner，一种学习将工具使用作为通用推理技能的多模态模型家族，能够动态编排多种工具进行多步推理，并泛化至未见过的工具和任务。",
      "method": "方法包括：（1）构建可扩展的数据流水线，使模型接触长视野、多步骤的工具交互；（2）提出Tool-GRPO强化学习算法，基于最终任务成功率优化工具选择与序列规划；（3）引入自适应学习机制，动态调节工具使用频率与时机。",
      "findings": "AdaReasoner展现出强大的工具自适应与泛化能力：能自主选择有益工具、抑制无关工具，并根据任务需求动态调整工具使用频率。在多个挑战性基准测试中取得领先性能，使7B基础模型平均提升24.9%，并在VSP、Jigsaw等任务上超越GPT-5等强商业系统。"
    }
  },
  {
    "id": "2601.18572",
    "title": "One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization",
    "abstract": "Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.",
    "authors": [
      "Franziska Weeber",
      "Vera Neplenbroek",
      "Jan Batzner",
      "Sebastian Padó"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18572.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18572",
    "primary_category": "cs.CL",
    "relevance_score": 10.0,
    "summary": {
      "title_zh": "一种人设，多种线索，不同结果：社会人口学线索如何影响大语言模型的个性化",
      "core_contribution": "本文揭示了仅使用单一社会人口学线索（如姓名或明确属性）来研究大语言模型个性化偏见的局限性，并论证了不同线索会导致模型响应产生显著差异，从而挑战了基于单一线索得出的结论的可靠性。",
      "method": "研究比较了六种常用的人设提示线索（如姓名、代词、显性描述等），在七个开源和专有大语言模型上，针对四项写作和咨询任务进行了系统性评估。该方法旨在检验不同线索在引发模型个性化响应时的一致性与差异性，并关注提示的鲁棒性和线索在真实交互中的外部有效性。",
      "findings": "关键发现表明，尽管不同线索引发的模型响应总体上高度相关，但它们在不同人设（社会人口学子群）上产生的响应存在实质性方差。因此，基于单一线索得出的关于模型个性化或偏见的结论可能具有误导性。研究建议未来的个性化研究应评估多种具有外部有效性的线索。"
    }
  },
  {
    "id": "2601.18615",
    "title": "Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem",
    "abstract": "This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.",
    "authors": [
      "Ramiro Valdes Jara",
      "Adam Meyers"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18615.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18615",
    "primary_category": "cs.LG",
    "relevance_score": 9.0,
    "summary": {
      "title_zh": "用于求解心电逆问题的无几何条件扩散建模",
      "core_contribution": "提出了一种无几何、纯数据驱动的条件扩散模型框架，用于求解心电逆问题，能够生成多个可能的概率性重建结果，而非单一确定性估计，从而更好地捕捉该问题的非唯一性和欠定性本质。",
      "method": "该方法采用条件扩散模型，学习从含噪声的体表信号到心表电位的概率映射。它利用扩散模型的生成特性，无需构建患者特定的几何网格，完全基于数据驱动。模型通过反向扩散过程，在给定体表测量值的条件下，采样生成可能的心表电位分布。",
      "findings": "在真实心电逆问题数据集上的实验表明，与卷积神经网络、长短期记忆网络和基于Transformer的确定性基线模型相比，所提出的扩散方法取得了更高的重建精度，证明了扩散模型作为无创心脏电生理成像鲁棒工具的潜力。"
    }
  },
  {
    "id": "2601.18585",
    "title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization",
    "abstract": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.",
    "authors": [
      "Chenxi Liu",
      "Selena Ling",
      "Alec Jacobson"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18585.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18585",
    "primary_category": "cs.CV",
    "relevance_score": 9.0,
    "summary": {
      "title_zh": "GimmBO：基于贝叶斯优化的交互式生成图像模型融合",
      "core_contribution": "提出了GimmBO系统，通过基于偏好的贝叶斯优化（PBO）支持对扩散模型适配器融合空间的交互式探索，解决了高维权重手动调整效率低下的问题。",
      "method": "方法采用两阶段贝叶斯优化后端，针对实际使用中观察到的稀疏性和权重范围受限的特点进行设计，以提高高维空间中的采样效率和收敛速度；系统允许用户通过交互反馈（如偏好选择）来引导优化过程。",
      "findings": "实验表明，与基线方法相比，GimmBO在模拟和真实用户研究中均表现出更快的收敛速度、更高的任务成功率，且框架可通过扩展灵活适应多种应用场景。"
    }
  },
  {
    "id": "2601.18493",
    "title": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment",
    "abstract": "Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines. To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.",
    "authors": [
      "Sara Tehrani",
      "Yonghao Xu",
      "Leif Haglund",
      "Amanda Berg",
      "Michael Felsberg"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18493.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18493",
    "primary_category": "cs.CV",
    "relevance_score": 9.0,
    "summary": {
      "title_zh": "DisasterInsight：一个面向功能感知与接地气灾害评估的多模态基准",
      "core_contribution": "提出了一个名为DisasterInsight的多模态基准，旨在评估视觉-语言模型在真实灾害分析任务中的表现，特别是功能理解和指令鲁棒性；并提出了一个通过领域适应微调得到的基线模型DI-Chat。",
      "method": "1. 将xBD数据集重构为约11.2万个以建筑物为中心的实例。2. 支持跨多个任务的多样化指令评估，包括建筑物功能分类、损坏程度与灾害类型分类、计数以及符合人道主义评估指南的结构化报告生成。3. 提出DI-Chat基线模型，通过参数高效的LoRA方法，在灾害特定的指令数据上对现有视觉-语言模型骨干进行微调得到。",
      "findings": "1. 对最先进的通用和遥感视觉-语言模型的广泛实验揭示了跨任务的显著性能差距，特别是在损害理解和结构化报告生成方面。2. DI-Chat在损害程度与灾害类型分类以及报告生成质量上取得了显著改进。3. 建筑物功能分类对所有评估模型来说仍然具有挑战性。4. DisasterInsight为研究灾害图像中的接地气多模态推理提供了一个统一的基准。"
    }
  },
  {
    "id": "2601.18353",
    "title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books",
    "abstract": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.",
    "authors": [
      "Tuhin Chakrabarty",
      "Paramveer S. Dhillon"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18353.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18353",
    "primary_category": "cs.AI",
    "relevance_score": 9.0,
    "summary": {
      "title_zh": "优秀写作能否是生成式的？基于高质量书籍微调催生专家级AI写作",
      "core_contribution": "本研究通过行为实验证明，对作者完整作品进行微调的大型语言模型（LLM）所生成的文本，在专家评审的盲测中已能超越专业人类作家的模仿作品，并引发了创作者对“优秀写作”本质的认同危机。",
      "method": "研究设计了行为实验，让28位艺术硕士（MFA）专业作家与三个LLM模型竞赛模仿50位广受好评的作家风格。实验比较了两种AI条件：上下文提示与基于作者全部作品微调。由28位专家评委和131位普通评委进行盲测配对比较，并后续对专家作家进行了访谈。",
      "findings": "在上下文提示条件下，专家评委82.7%更偏好人类作品；但在对作者作品微调后，该偏好逆转，62%的专家评委更偏好AI作品。普通评委则始终更偏好AI写作。专家作家访谈揭示，他们对AI作品的偏好触发了身份危机，动摇了其审美自信，并引发了对“优秀写作”构成要素的根本性质疑。"
    }
  },
  {
    "id": "2601.18271",
    "title": "Designing large language model prompts to extract scores from messy text: A shared dataset and challenge",
    "abstract": "In some areas of computing, natural language processing and information science, progress is made by sharing datasets and challenging the community to design the best algorithm for an associated task. This article introduces a shared dataset of 1446 short texts, each of which describes a research quality score on the UK scale of 1* to 4*. This is a messy collection, with some texts not containing scores and others including invalid scores or strange formats. With this dataset there is also a description of what constitutes a valid score and a \"gold standard\" of the correct scores for these texts (including missing values). The challenge is to design a prompt for Large Language Models (LLMs) to extract the scores from these texts as accurately as possible. The format for the response should be a number and no other text so there are two aspects to the challenge: ensuring that the LLM returns only a number, and instructing it to deduce the correct number for the text. As part of this, the LLM prompt needs to explain when to return the missing value code, -1, instead of a number when the text does not clearly contain one. The article also provides an example of a simple prompt. The purpose of the challenge is twofold: to get an effective solution to this problem, and to increase understanding of prompt design and LLM capabilities for complex numerical tasks. The initial solution suggested has an accuracy of 72.6%, so the challenge is to beat this.",
    "authors": [
      "Mike Thelwall"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18271.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18271",
    "primary_category": "cs.DL",
    "relevance_score": 9.0,
    "summary": {
      "title_zh": "设计大型语言模型提示以从杂乱文本中提取评分：一个共享数据集与挑战",
      "core_contribution": "本文引入了一个共享数据集和挑战任务，旨在推动社区设计最佳提示，使大型语言模型能够从非结构化的短文本中准确提取研究质量评分，并促进对提示工程及大模型处理复杂数值任务能力的理解。",
      "method": "研究提供了一个包含1446条杂乱短文本的数据集，每条文本描述了一个基于英国1*至4*标准的研究质量评分，其中部分文本不包含有效评分或格式混乱。挑战要求设计一个大型语言模型提示，使其能够准确提取文本中的评分（或对无明确评分的文本返回缺失值代码-1），且输出必须仅为数字格式。论文还提供了一个简单提示作为基线示例。",
      "findings": "论文提出的初始解决方案（简单提示）在数据集上的准确率为72.6%，为后续研究设立了可超越的基准。该挑战旨在激励社区开发更有效的提示策略，以提升大模型从杂乱文本中提取结构化数值信息的准确性和鲁棒性。"
    }
  },
  {
    "id": "2601.18759",
    "title": "UI Remix: Supporting UI Design Through Interactive Example Retrieval and Remixing",
    "abstract": "Designing user interfaces (UIs) is a critical step when launching products, building portfolios, or personalizing projects, yet end users without design expertise often struggle to articulate their intent and to trust design choices. Existing example-based tools either promote broad exploration, which can cause overwhelm and design drift, or require adapting a single example, risking design fixation. We present UI Remix, an interactive system that supports mobile UI design through an example-driven design workflow. Powered by a multimodal retrieval-augmented generation (MMRAG) model, UI Remix enables iterative search, selection, and adaptation of examples at both the global (whole interface) and local (component) level. To foster trust, it presents source transparency cues such as ratings, download counts, and developer information. In an empirical study with 24 end users, UI Remix significantly improved participants' ability to achieve their design goals, facilitated effective iteration, and encouraged exploration of alternative designs. Participants also reported that source transparency cues enhanced their confidence in adapting examples. Our findings suggest new directions for AI-assisted, example-driven systems that empower end users to design with greater control, trust, and openness to exploration.",
    "authors": [
      "Junling Wang",
      "Hongyi Lan",
      "Xiaotian Su",
      "Mustafa Doga Dogan",
      "April Yi Wang"
    ],
    "published": "2026-01-26",
    "updated": "2026-01-26",
    "categories": [
      "cs.HC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2601.18759.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18759",
    "primary_category": "cs.HC",
    "relevance_score": 8.0,
    "summary": {
      "title_zh": "UI Remix：通过交互式示例检索与混搭支持用户界面设计",
      "core_contribution": "提出了UI Remix系统，通过一个基于示例的交互式设计工作流，帮助非专业设计者进行移动UI设计，解决了现有工具在广泛探索与单一示例适应之间的权衡问题。",
      "method": "系统采用多模态检索增强生成（MMRAG）模型，支持用户在全局（整个界面）和局部（组件）两个层面进行迭代式的示例搜索、选择与适配。同时，系统通过展示评分、下载量、开发者信息等来源透明度提示，以增强用户对设计选择的信任。",
      "findings": "在24名终端用户的实证研究中，UI Remix显著提升了参与者实现设计目标的能力，促进了有效的设计迭代，并鼓励了对替代设计的探索。参与者反馈表明，来源透明度提示增强了他们适配示例的信心，证明了系统在提升用户控制感、信任度和探索开放性方面的有效性。"
    }
  }
]