<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-03</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | äººæœºäº¤äº’: 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.00883</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.00883" target="_blank">DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models</a>
      </h3>
      <p class="paper-title-zh">DIAMONDï¼šæµåŒ¹é…æ¨¡å‹ä¸­ç”¨äºç¼“è§£ä¼ªå½±çš„å®šå‘æ¨ç†æ–¹æ³•</p>
      <p class="paper-authors">Alicja Polowczyk, Agnieszka Polowczyk, Piotr Borycki, Joanna WaczyÅ„ska, Jacek Tabor ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡è½¨è¿¹æ ¡æ­£æ¥ä¸»åŠ¨ç¼“è§£å›¾åƒä¼ªå½±çš„æ–¹æ³•DIAMONDï¼Œä¸ºç°ä»£ç”Ÿæˆæ¶æ„æä¾›äº†ä¸€æ¡æ— éœ€é¢å¤–è®­ç»ƒæˆ–ä¿®æ”¹æƒé‡çš„é›¶æ ·æœ¬é«˜ä¿çœŸå›¾åƒåˆæˆè·¯å¾„ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åœ¨ç”Ÿæˆè½¨è¿¹çš„æ¯ä¸€æ­¥é‡å»ºå¯¹å¹²å‡€æ ·æœ¬çš„ä¼°è®¡ï¼Œä»è€Œä¸»åŠ¨å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹è¿œç¦»å¯èƒ½å¯¼è‡´ä¼ªå½±çš„æ½œåœ¨çŠ¶æ€ã€‚å®ƒæ— éœ€å¯¹æ¨¡å‹æƒé‡è¿›è¡Œä¾µå…¥å¼ä¿®æ”¹ï¼Œä¹Ÿé¿å…äº†è®¡ç®—æ˜‚è´µçš„åŒºåŸŸç»†åŒ–è¿‡ç¨‹ï¼Œæ˜¯ä¸€ç§å®Œå…¨åœ¨æ¨ç†é˜¶æ®µè¿è¡Œçš„è®­ç»ƒå…è´¹æ–¹æ³•ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒDIAMONDèƒ½æœ‰æ•ˆå‡å°‘FLUXç­‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„è§†è§‰å’Œè§£å‰–ç»“æ„ä¼ªå½±ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•å¯æ‰©å±•è‡³æ ‡å‡†æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸã€æ— ä¼ªå½±çš„å›¾åƒåˆæˆï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æ¨¡å‹æƒé‡ä¿®æ”¹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.00883" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.00883.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">äººæœºäº¤äº’</span>
          <span class="paper-id">2602.00738</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.00738" target="_blank">Iconix: Controlling Semantics and Style in Progressive Icon Grids Generation</a>
      </h3>
      <p class="paper-title-zh">Iconixï¼šåœ¨æ¸è¿›å¼å›¾æ ‡ç½‘æ ¼ç”Ÿæˆä¸­æ§åˆ¶è¯­ä¹‰ä¸é£æ ¼</p>
      <p class="paper-authors">Zhida Sun, Xiaodong Wang, Zhenyao Zhang, Min Lu, Dani Lischinski ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåä¸ºIconixçš„äººæœºååŒåˆ›æ„ç³»ç»Ÿï¼Œé€šè¿‡æ„å»ºè¯­ä¹‰æ”¯æ¶å’Œæ¸è¿›å¼ç®€åŒ–ï¼Œå¸®åŠ©è®¾è®¡å¸ˆåœ¨è¯­ä¹‰ä¸°å¯Œåº¦å’Œè§†è§‰å¤æ‚åº¦ä¸¤ä¸ªç»´åº¦ä¸Šç”Ÿæˆé£æ ¼ä¸€è‡´çš„å›¾æ ‡ç½‘æ ¼ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ç³»ç»Ÿé¦–å…ˆæ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ¦‚å¿µæ„å»ºä¸€ä¸ªåŒ…å«ç›¸å…³åˆ†æè§†è§’çš„è¯­ä¹‰æ”¯æ¶ï¼›ç„¶åé‡‡ç”¨é“¾å¼ã€å›¾åƒæ¡ä»¶ç”ŸæˆæŠ€æœ¯ï¼Œç”Ÿæˆé£æ ¼ä¸€è‡´çš„å›¾æ ‡èŒƒä¾‹ï¼›æœ€åå°†æ¯ä¸ªèŒƒä¾‹è‡ªåŠ¨æç‚¼ä¸ºä»è¯¦ç»†åˆ°æŠ½è±¡çš„æ¸è¿›åºåˆ—ï¼Œå½¢æˆå¯å¯¼èˆªçš„äºŒç»´ç½‘æ ¼ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ä¸€é¡¹32äººçš„å—è¯•è€…å†…å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿å·¥ä½œæµç¨‹ç›¸æ¯”ï¼Œå‚ä¸è€…ä½¿ç”¨Iconixèƒ½æ›´å…·åˆ›æ„åœ°ç”Ÿæˆå›¾æ ‡ç½‘æ ¼ï¼Œå·¥ä½œè´Ÿè·æ›´ä½ï¼Œå¹¶èƒ½æ¢ç´¢ä¸€ç³»åˆ—è¿è´¯çš„è®¾è®¡å˜ä½“ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è¯­ä¹‰æ”¯æ¶ä¸æ¸è¿›å¼ç®€åŒ–ç›¸ç»“åˆï¼Œæœ‰æ•ˆæ”¯æŒäº†è§†è§‰æŠ½è±¡åŒ–è®¾è®¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Visual communication often needs stylistically consistent icons that span concrete and abstract meanings, for use in diverse contexts. We present Iconix, a human-AI co-creative system that organizes icon generation along two axes: semantic richness (what is depicted) and visual complexity (how much detail). Given a user-specified concept, Iconix constructs a semantic scaffold of related analytical perspectives and employs chained, image-conditioned generation to produce a coherent style of exemplars. Each exemplar is then automatically distilled into a progressive sequence, from detailed and elaborate to abstract and simple. The resulting two-dimensional grid exposes a navigable space, helping designers reason jointly about figurative content and visual abstraction. A within-subjects study (N = 32) found that compared to a baseline workflow, participants produced icon grids more creatively, reported lower workload, and explored a coherent range of design variations. We discuss implications for human-machine co-creative approaches that couple semantic scaffolding with progressive simplification to support visual abstraction.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.00738" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.00738.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.00627</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.00627" target="_blank">FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization</a>
      </h3>
      <p class="paper-title-zh">FaceSnapï¼šç”¨äºå…è°ƒä¼˜è‚–åƒå®šåˆ¶çš„å¢å¼ºèº«ä»½ä¿çœŸç½‘ç»œ</p>
      <p class="paper-authors">Benxiang Zhai, Yifang Xu, Guofeng Zhang, Yang Li, Sidan Du</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºä¸€ç§åŸºäºStable Diffusionçš„å…è°ƒä¼˜è‚–åƒå®šåˆ¶æ–¹æ³•FaceSnapï¼Œä»…éœ€å•å¼ å‚è€ƒå›¾åƒå³å¯åœ¨å•æ¬¡æ¨ç†ä¸­ç”Ÿæˆé«˜åº¦èº«ä»½ä¸€è‡´ä¸”ç»†èŠ‚ä¿çœŸçš„è‚–åƒï¼Œå¹¶å…·å¤‡å³æ’å³ç”¨ä¸è·¨æ¨¡å‹æ‰©å±•èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºStable Diffusionæ¡†æ¶ï¼Œè®¾è®¡äº†ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š1ï¼‰é¢éƒ¨å±æ€§æ··åˆå™¨ï¼Œèåˆä½å±‚å…·ä½“ç‰¹å¾ä¸é«˜å±‚æŠ½è±¡ç‰¹å¾ä»¥æä¾›å…¨é¢çš„ç”ŸæˆæŒ‡å¯¼ï¼›2ï¼‰å…³é”®ç‚¹é¢„æµ‹å™¨ï¼Œé€šè¿‡ä¸åŒå§¿æ€çš„å…³é”®ç‚¹ä¿æŒå‚è€ƒèº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶æä¾›å¤šæ ·åŒ–çš„ç©ºé—´æ§åˆ¶æ¡ä»¶ï¼›3ï¼‰èº«ä»½ä¿æŒæ¨¡å—ï¼Œå°†ä¸Šè¿°ä¿¡æ¯æ³¨å…¥UNetä¸­ä»¥é©±åŠ¨å›¾åƒç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒFaceSnapåœ¨ä¸ªæ€§åŒ–è‚–åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨èº«ä»½ä¿çœŸåº¦å’Œç»†èŠ‚ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œä¸”æ— éœ€å¾®è°ƒå³å¯å®ç°é«˜è´¨é‡è¾“å‡ºã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.00627" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.00627.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.00618</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.00618" target="_blank">Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting</a>
      </h3>
      <p class="paper-title-zh">Tune-Your-Styleï¼šåŸºäºé«˜æ–¯æ³¼æº…çš„å¯è°ƒå¼ºåº¦ä¸‰ç»´é£æ ¼è¿ç§»</p>
      <p class="paper-authors">Yian Zhao, Rushi Ye, Ruochong Zheng, Zesen Cheng, Chaoran Feng ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ã€å¼ºåº¦å¯è°ƒçš„ä¸‰ç»´é£æ ¼è¿ç§»èŒƒå¼ï¼Œå…è®¸ç”¨æˆ·çµæ´»è°ƒæ•´æ³¨å…¥åœºæ™¯çš„é£æ ¼å¼ºåº¦ï¼Œä»¥æ»¡è¶³ä¸åŒçš„å†…å®¹-é£æ ¼å¹³è¡¡éœ€æ±‚ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†ä¸‰ç»´é£æ ¼è¿ç§»çš„å¯å®šåˆ¶æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é¦–å…ˆå¼•å…¥é«˜æ–¯ç¥ç»å…ƒæ¥æ˜¾å¼å»ºæ¨¡é£æ ¼å¼ºåº¦ï¼Œå¹¶å‚æ•°åŒ–ä¸€ä¸ªå¯å­¦ä¹ çš„é£æ ¼è°ƒèŠ‚å™¨ä»¥å®ç°å¼ºåº¦å¯æ§çš„é£æ ¼æ³¨å…¥ã€‚ä¸ºäº†ä¿ƒè¿›å¯è°ƒé£æ ¼åŒ–çš„å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æå‡ºäº†å¯è°ƒé£æ ¼åŒ–å¼•å¯¼ï¼šé€šè¿‡è·¨è§†å›¾é£æ ¼å¯¹é½ä»æ‰©æ•£æ¨¡å‹è·å¾—å¤šè§†å›¾ä¸€è‡´çš„é£æ ¼åŒ–è§†å›¾ï¼Œç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡è°ƒåˆ¶æ¥è‡ªé£æ ¼åŒ–è§†å›¾çš„â€œå…¨é£æ ¼å¼•å¯¼â€ä¸æ¥è‡ªåˆå§‹æ¸²æŸ“çš„â€œé›¶é£æ ¼å¼•å¯¼â€ä¹‹é—´çš„å¹³è¡¡ï¼Œæ¥æä¾›ç¨³å®šé«˜æ•ˆçš„è®­ç»ƒæŒ‡å¯¼ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½ç”Ÿæˆè§†è§‰ä¸Šå¸å¼•äººçš„ç»“æœï¼Œè€Œä¸”ä¸ºä¸‰ç»´é£æ ¼è¿ç§»æä¾›äº†çµæ´»çš„å¯å®šåˆ¶æ€§ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è°ƒèŠ‚é£æ ¼å¼ºåº¦å‚æ•°ï¼Œåœ¨ä¿æŒåŸå§‹åœºæ™¯å†…å®¹ä¸å®Œå…¨åº”ç”¨å‚è€ƒé£æ ¼ä¹‹é—´å®ç°è¿ç»­ã€å¹³æ»‘çš„è¿‡æ¸¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.00618" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.00618.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.00508</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.00508" target="_blank">DuoGen: Towards General Purpose Interleaved Multimodal Generation</a>
      </h3>
      <p class="paper-title-zh">DuoGenï¼šè¿ˆå‘é€šç”¨äº¤é”™å¤šæ¨¡æ€ç”Ÿæˆ</p>
      <p class="paper-authors">Min Shi, Xiaohui Zeng, Jiannan Huang, Yin Cui, Francesco Ferroni ç­‰ (16 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªé€šç”¨äº¤é”™å¤šæ¨¡æ€ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„æ•°æ®æ„å»ºä¸ä¸¤é˜¶æ®µè§£è€¦è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†äº¤é”™ç”Ÿæˆä»»åŠ¡ä¸­æ–‡æœ¬è´¨é‡ã€å›¾åƒä¿çœŸåº¦ä¸å›¾æ–‡å¯¹é½æ•ˆæœã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. æ•°æ®æ–¹é¢ï¼šç»“åˆä»ç²¾é€‰ç½‘ç«™é‡å†™çš„å¤šæ¨¡æ€å¯¹è¯ä¸è¦†ç›–æ—¥å¸¸åœºæ™¯çš„å¤šæ ·åŒ–åˆæˆæ•°æ®ï¼Œæ„å»ºäº†å¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚2. æ¶æ„æ–¹é¢ï¼šåˆ©ç”¨é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ä¸è§†é¢‘ç”Ÿæˆé¢„è®­ç»ƒçš„æ‰©æ•£Transformerçš„è§†è§‰ç”Ÿæˆèƒ½åŠ›ï¼Œé¿å…æ˜‚è´µçš„å•æ¨¡æ€é¢„è®­ç»ƒã€‚3. é‡‡ç”¨ä¸¤é˜¶æ®µè§£è€¦ç­–ç•¥ï¼šå…ˆå¯¹MLLMè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå†ç”¨ç²¾é€‰çš„äº¤é”™å›¾æ–‡åºåˆ—å°†DiTä¸å…¶å¯¹é½ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å…¬å¼€åŠæ–°æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDuoGenåœ¨æ–‡æœ¬è´¨é‡ã€å›¾åƒä¿çœŸåº¦å’Œå›¾æ–‡å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼›åœ¨ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œå…¶æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.00508" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.00508.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>