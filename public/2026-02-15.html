<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-15</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.11146</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.11146" target="_blank">Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling</a>
      </h3>
      <p class="paper-title-zh">è¶…è¶ŠåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¥–åŠ±ï¼šæ‰©æ•£åŸç”Ÿçš„æ½œåœ¨å¥–åŠ±å»ºæ¨¡</p>
      <p class="paper-authors">Gongye Liu, Bo Yang, Yida Zhi, Zhizhou Zhong, Lei Ke ç­‰ (11 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†DiNa-LRMï¼Œä¸€ç§ç›´æ¥åœ¨æ‰©æ•£æ¨¡å‹çš„å™ªå£°çŠ¶æ€ä¸Šè¿›è¡Œåå¥½å­¦ä¹ çš„æ‰©æ•£åŸç”Ÿæ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»ŸåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¥–åŠ±å‡½æ•°å­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜å’Œåƒç´ åŸŸä¸åŒ¹é…é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åŸºäºé¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸»å¹²ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ—¶é—´æ­¥æ¡ä»¶å¥–åŠ±å¤´ã€‚å…¶æ ¸å¿ƒæ˜¯æå‡ºäº†ä¸€ä¸ªå™ªå£°æ ¡å‡†çš„ç‘Ÿæ–¯é¡¿ä¼¼ç„¶å‡½æ•°ï¼Œè¯¥å‡½æ•°å…·æœ‰ä¾èµ–äºæ‰©æ•£å™ªå£°çš„ä¸ç¡®å®šæ€§åº¦é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ”¯æŒæ¨ç†æ—¶çš„å™ªå£°é›†æˆï¼Œæä¾›äº†ä¸€ç§æ‰©æ•£åŸç”Ÿçš„æµ‹è¯•æ—¶ç¼©æ”¾å’Œé²æ£’å¥–åŠ±æœºåˆ¶ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å›¾åƒå¯¹é½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDiNa-LRMæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºæ‰©æ•£çš„å¥–åŠ±åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”ä»¥æå°çš„è®¡ç®—æˆæœ¬å®ç°äº†ä¸æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç›¸ç«äº‰çš„æ€§èƒ½ã€‚åœ¨åå¥½ä¼˜åŒ–ä»»åŠ¡ä¸­ï¼ŒDiNa-LRMæ”¹å–„äº†ä¼˜åŒ–åŠ¨æ€ï¼Œèƒ½å¤Ÿå®ç°æ›´å¿«ã€æ›´èµ„æºé«˜æ•ˆçš„æ¨¡å‹å¯¹é½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.11146" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.11146.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.11105</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.11105" target="_blank">FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference</a>
      </h3>
      <p class="paper-title-zh">FastFlowï¼šåˆ©ç”¨å¤šè‡‚èµŒåšæœºæ¨ç†åŠ é€Ÿç”ŸæˆæµåŒ¹é…æ¨¡å‹</p>
      <p class="paper-authors">Divya Jyoti Bajpai, Dhruv Bhardwaj, Soumya Roy, Tejas Duseja, Harsh Agarwal ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†FastFlowï¼Œä¸€ç§å³æ’å³ç”¨çš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è·³è¿‡å†—ä½™çš„å»å™ªæ­¥éª¤æ¥åŠ é€ŸæµåŒ¹é…æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯åœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°é€šç”¨åŠ é€Ÿã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> FastFlowé€šè¿‡åˆ†æå»å™ªè·¯å¾„ï¼Œè¯†åˆ«å¯¹è·¯å¾„è°ƒæ•´å½±å“è¾ƒå°çš„æ­¥éª¤ï¼Œå¹¶åˆ©ç”¨å…ˆå‰é¢„æµ‹çš„æœ‰é™å·®åˆ†é€Ÿåº¦ä¼°è®¡æ¥è¿‘ä¼¼è¿™äº›æ­¥éª¤ï¼Œä»è€Œè·³è¿‡å®Œæ•´çš„ç¥ç»ç½‘ç»œè®¡ç®—ã€‚è¯¥æ–¹æ³•å°†â€œå®‰å…¨è·³è¿‡å¤šå°‘æ­¥â€çš„å†³ç­–å»ºæ¨¡ä¸ºå¤šè‡‚èµŒåšæœºé—®é¢˜ï¼ŒåŠ¨æ€å­¦ä¹ é€Ÿåº¦ä¸æ€§èƒ½å¹³è¡¡çš„æœ€ä¼˜è·³è¿‡ç­–ç•¥ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒFastFlowåœ¨å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šå‡èƒ½å®ç°è¶…è¿‡2.6å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡è¾“å‡ºï¼Œä¸”æ— éœ€é’ˆå¯¹ä¸åŒä»»åŠ¡é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´æ¨¡å‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.11105" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.11105.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.10764</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.10764" target="_blank">Dual-End Consistency Model</a>
      </h3>
      <p class="paper-title-zh">åŒç«¯ä¸€è‡´æ€§æ¨¡å‹</p>
      <p class="paper-authors">Linwei Dong, Ruoyu Guo, Ge Bai, Zehuan Yuan, Yawei Luo ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºåŒç«¯ä¸€è‡´æ€§æ¨¡å‹ï¼ˆDE-CMï¼‰ï¼Œé€šè¿‡é€‰æ‹©å…³é”®å­è½¨è¿¹ç°‡è§£å†³äº†ç°æœ‰ä¸€è‡´æ€§æ¨¡å‹è®­ç»ƒä¸ç¨³å®šå’Œé‡‡æ ·ä¸çµæ´»ä¸¤å¤§ç“¶é¢ˆï¼Œå®ç°äº†é«˜æ•ˆä¸”ç¨³å®šçš„å•æ­¥ç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é¦–å…ˆåˆ†æå‘ç°è®­ç»ƒä¸ç¨³å®šæºäºè‡ªç›‘ç£é¡¹çš„å‘æ•£ï¼Œé‡‡æ ·ä¸çµæ´»æºäºè¯¯å·®ç´¯ç§¯ï¼›è¿›è€Œå°†æ¦‚ç‡æµODEè½¨è¿¹åˆ†è§£ï¼Œé€‰æ‹©ä¸‰ä¸ªå…³é”®å­è½¨è¿¹ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œç»“åˆè¿ç»­æ—¶é—´ä¸€è‡´æ€§ç›®æ ‡å®ç°å°‘æ­¥è’¸é¦ï¼Œå¹¶åˆ©ç”¨æµåŒ¹é…ä½œä¸ºè¾¹ç•Œæ­£åˆ™åŒ–å™¨ç¨³å®šè®­ç»ƒï¼›æ­¤å¤–æå‡ºå™ªå£°åˆ°å«å™ªï¼ˆN2Nï¼‰æ˜ å°„ï¼Œå¯å°†å™ªå£°æ˜ å°„è‡³ä»»æ„ç‚¹ä»¥ç¼“è§£é¦–æ­¥è¯¯å·®ç´¯ç§¯ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNet 256Ã—256æ•°æ®é›†ä¸Šï¼ŒDE-CMåœ¨å•æ­¥ç”Ÿæˆä¸­å–å¾—äº†1.70çš„FIDåˆ†æ•°ï¼Œä¼˜äºç°æœ‰åŸºäºä¸€è‡´æ€§æ¨¡å‹çš„å•æ­¥ç”Ÿæˆæ–¹æ³•ï¼Œè¾¾åˆ°äº†å½“å‰æœ€ä¼˜æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.10764" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.10764.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.10757</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.10757" target="_blank">Text-to-Vector Conversion for Residential Plan Design</a>
      </h3>
      <p class="paper-title-zh">ç”¨äºä½å®…å¹³é¢è®¾è®¡çš„æ–‡æœ¬åˆ°çŸ¢é‡è½¬æ¢</p>
      <p class="paper-authors">Egor Bazhenov, Stepan Kasai, Viacheslav Shalamov, Valeria Efimova</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§ä»æ–‡æœ¬æè¿°ç”ŸæˆçŸ¢é‡ä½å®…å¹³é¢å›¾çš„æ–°æ–¹æ³•ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å°†æ …æ ¼å¹³é¢å›¾çŸ¢é‡åŒ–æˆç»“æ„åŒ–çŸ¢é‡å›¾åƒçš„æ–°ç®—æ³•ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡æ–‡æœ¬æè¿°ç›´æ¥ç”Ÿæˆç”±æ•°å­¦å›¾å…ƒå®šä¹‰çš„çŸ¢é‡å›¾å½¢ï¼Œè€Œéæ …æ ¼å›¾åƒã€‚å…¶è®¾è®¡ç‰¹åˆ«è€ƒè™‘äº†å»ºç­‘å¹³é¢å›¾ä¸­å¸¸è§çš„ç›´è§’ç‰¹å¾å’Œçµæ´»å¸ƒå±€éœ€æ±‚ï¼Œä»è€Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½æ›´è‡ªç„¶åœ°å¤„ç†è¿™äº›ç»“æ„ã€‚åŒæ—¶ï¼Œè®ºæ–‡æå‡ºçš„çŸ¢é‡åŒ–ç®—æ³•èƒ½å°†ç°æœ‰çš„æ …æ ¼å¹³é¢å›¾è½¬æ¢ä¸ºç»“æ„åŒ–çš„çŸ¢é‡æ ¼å¼ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„çŸ¢é‡ä½å®…å¹³é¢å›¾åœ¨åŸºäºCLIPScoreçš„è§†è§‰è´¨é‡è¯„ä¼°ä¸Šæ¯”ç°æœ‰è§£å†³æ–¹æ¡ˆé«˜å‡ºçº¦5%ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„çŸ¢é‡åŒ–ç®—æ³•ç”Ÿæˆçš„çŸ¢é‡å›¾åƒï¼Œå…¶CLIPScoreä¹Ÿæ¯”å…¶ä»–æ–¹æ³•çš„ç»“æœé«˜å‡ºçº¦4%ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Computer graphics, comprising both raster and vector components, is a fundamental part of modern science, industry, and digital communication. While raster graphics offer ease of use, its pixel-based structure limits scalability. Vector graphics, defined by mathematical primitives, provides scalability without quality loss, however, it is more complex to produce. For design and architecture, the versatility of vector graphics is paramount, despite its computational demands. This paper introduces a novel method for generating vector residential plans from textual descriptions. Our approach surpasses existing solutions by approximately 5% in CLIPScore-based visual quality, benefiting from its inherent handling of right angles and flexible settings. Additionally, we present a new algorithm for vectorizing raster plans into structured vector images. Such images have a better CLIPscore compared to others by about 4%.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.10757" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.10757.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.10662</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.10662" target="_blank">Dynamic Frequency Modulation for Controllable Text-driven Image Generation</a>
      </h3>
      <p class="paper-title-zh">ç”¨äºå¯æ§æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆçš„åŠ¨æ€é¢‘ç‡è°ƒåˆ¶</p>
      <p class="paper-authors">Tiandong Shi, Ling Zhao, Ji Qi, Jiayi Ma, Chengli Peng</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ã€åŸºäºåŠ¨æ€è¡°å‡é¢‘ç‡åŠ æƒå‡½æ•°çš„è°ƒåˆ¶æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå›¾åƒæ•´ä½“ç»“æ„æ¡†æ¶ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå®ç°é’ˆå¯¹æ€§çš„è¯­ä¹‰ä¿®æ”¹ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•å¯¹å†…éƒ¨ç‰¹å¾å›¾çš„ç»éªŒæ€§é€‰æ‹©ä¾èµ–ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è®ºæ–‡ä»é¢‘ç‡è§†è§’åˆ†æç”Ÿæˆè¿‡ç¨‹ä¸­å™ªå£°æ½œåœ¨å˜é‡çš„é¢‘è°±å¯¹ç»“æ„æ¡†æ¶å’Œç»†ç²’åº¦çº¹ç†åˆ†å±‚ç”Ÿæˆçš„å½±å“ã€‚åŸºäºæ­¤å‘ç°ï¼Œè®¾è®¡äº†ä¸€ç§é¢‘ç‡ç›¸å…³çš„åŠ¨æ€è¡°å‡åŠ æƒå‡½æ•°ï¼Œç›´æ¥å¯¹å™ªå£°æ½œåœ¨å˜é‡è¿›è¡Œæ“ä½œï¼Œä»è€Œåœ¨å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ§ä¸åŒé¢‘ç‡æˆåˆ†çš„è´¡çŒ®ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œé€šè¿‡è°ƒåˆ¶é¢‘ç‡æˆåˆ†å³å¯å®ç°è¯­ä¹‰æ§åˆ¶ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒå‘ç°ï¼Œåœ¨ç”Ÿæˆæ—©æœŸï¼Œä½é¢‘æˆåˆ†ä¸»è¦è´Ÿè´£å»ºç«‹å›¾åƒçš„ç»“æ„æ¡†æ¶ï¼Œå…¶å½±å“åŠ›éšæ—¶é—´è¡°å‡ï¼›è€Œé«˜é¢‘æˆåˆ†åˆ™åœ¨åæœŸä¸»å¯¼åˆæˆç»†ç²’åº¦çº¹ç†ã€‚æ‰€æå‡ºçš„é¢‘ç‡è°ƒåˆ¶æ–¹æ³•åœ¨ä¿æŒåŸå§‹ç»“æ„çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆå®ç°ç›®æ ‡è¯­ä¹‰ä¿®æ”¹ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ç»“æ„ä¿æŒä¸è¯­ä¹‰æ›´æ–°ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>The success of text-guided diffusion models has established a new image generation paradigm driven by the iterative refinement of text prompts. However, modifying the original text prompt to achieve the expected semantic adjustments often results in unintended global structure changes that disrupt user intent. Existing methods rely on empirical feature map selection for intervention, whose performance heavily depends on appropriate selection, leading to suboptimal stability. This paper tries to solve the aforementioned problem from a frequency perspective and analyzes the impact of the frequency spectrum of noisy latent variables on the hierarchical emergence of the structure framework and fine-grained textures during the generation process. We find that lower-frequency components are primarily responsible for establishing the structure framework in the early generation stage. Their influence diminishes over time, giving way to higher-frequency components that synthesize fine-grained textures. In light of this, we propose a training-free frequency modulation method utilizing a frequency-dependent weighting function with dynamic decay. This method maintains the structure framework consistency while permitting targeted semantic modifications. By directly manipulating the noisy latent variable, the proposed method avoids the empirical selection of internal feature maps. Extensive experiments demonstrate that the proposed method significantly outperforms current state-of-the-art methods, achieving an effective balance between preserving structure and enabling semantic updates.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.10662" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.10662.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>