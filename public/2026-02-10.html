<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-10</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.07645</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.07645" target="_blank">From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding</a>
      </h3>
      <p class="paper-title-zh">ä»æ­»åƒç´ åˆ°å¯ç¼–è¾‘å¹»ç¯ç‰‡ï¼šé€šè¿‡è§†è§‰è¯­è¨€åŒºåŸŸç†è§£å°†ä¿¡æ¯å›¾é‡å»ºä¸ºåŸç”ŸGoogleå¹»ç¯ç‰‡</p>
      <p class="paper-authors">Leonardo Gonzalez</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåä¸ºImages2Slidesçš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œèƒ½å¤Ÿå°†é™æ€ä¿¡æ¯å›¾å›¾åƒè½¬æ¢ä¸ºå¯ç¼–è¾‘çš„åŸç”ŸGoogleå¹»ç¯ç‰‡æ ¼å¼ï¼Œè§£å†³äº†ä¿¡æ¯å›¾å†…å®¹è¢«é”å®šä¸ºåƒç´ åéš¾ä»¥æ›´æ–°å’Œå¤ç”¨çš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºAPIçš„ç®¡é“ï¼Œé¦–å…ˆåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æå–å›¾åƒä¸­åŒºåŸŸçº§åˆ«çš„ç»“æ„åŒ–æè¿°ï¼Œç„¶åå°†åƒç´ å‡ ä½•åæ ‡æ˜ å°„åˆ°å¹»ç¯ç‰‡åæ ‡ç³»ï¼Œæœ€åé€šè¿‡Google Slidesçš„æ‰¹é‡æ›´æ–°APIé‡æ–°åˆ›å»ºæ‰€æœ‰å…ƒç´ ã€‚ç³»ç»Ÿæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œé€šè¿‡é€šç”¨çš„JSONåŒºåŸŸæ¨¡å¼å’Œç¡®å®šæ€§åå¤„ç†æ”¯æŒå¤šç§VLMåç«¯ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨åŒ…å«29ä¸ªç¨‹åºç”Ÿæˆä¿¡æ¯å›¾çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œç³»ç»Ÿæ•´ä½“å…ƒç´ æ¢å¤ç‡è¾¾åˆ°0.989Â±0.057ï¼Œæ–‡æœ¬è½¬å½•é”™è¯¯ç‡è¾ƒä½ï¼Œå¸ƒå±€ä¿çœŸåº¦è‰¯å¥½ã€‚ç ”ç©¶åŒæ—¶è¯†åˆ«äº†æ–‡æœ¬å¤§å°æ ¡å‡†å’Œéå‡åŒ€èƒŒæ™¯ç­‰å®é™…å·¥ç¨‹æŒ‘æˆ˜ï¼Œå¹¶åˆ†æäº†å¤±è´¥æ¨¡å¼ä»¥æŒ‡å¯¼æœªæ¥å·¥ä½œã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.07645" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.07645.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.07564</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.07564" target="_blank">SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens</a>
      </h3>
      <p class="paper-title-zh">SIGMAï¼šåŸºäºå¤šå±æ€§ä»¤ç‰Œçš„é€‰æ‹©æ€§äº¤é”™ç”Ÿæˆ</p>
      <p class="paper-authors">Xiaoyan Zhang, Zechen Bai, Haofan Wang, Yiren Song</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†SIGMAï¼Œä¸€ä¸ªç»Ÿä¸€çš„åè®­ç»ƒæ¡†æ¶ï¼Œé¦–æ¬¡åœ¨æ‰©æ•£Transformerä¸­å®ç°äº†äº¤é”™å¤šæ¡ä»¶ç”Ÿæˆï¼Œè§£å†³äº†ç°æœ‰ç»Ÿä¸€æ¨¡å‹ä»…æ”¯æŒå•æ¡ä»¶è¾“å…¥ã€æ— æ³•çµæ´»åˆæˆå¤šæºå¼‚æ„ä¿¡æ¯çš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºBagelç»Ÿä¸€éª¨å¹²æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œå¼•å…¥äº†é€‰æ‹©æ€§å¤šå±æ€§ä»¤ç‰Œï¼ˆå¦‚é£æ ¼ã€å†…å®¹ã€ä¸»ä½“å’Œèº«ä»½ä»¤ç‰Œï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£æå’Œç»„åˆäº¤é”™æ’åˆ—çš„æ–‡æœ¬-å›¾åƒåºåˆ—ä¸­çš„å¤šç§è§†è§‰æ¡ä»¶ã€‚é€šè¿‡åœ¨70ä¸‡ä¸ªäº¤é”™ç¤ºä¾‹ä¸Šè¿›è¡Œåè®­ç»ƒï¼Œæ¨¡å‹æ”¯æŒç»„åˆç¼–è¾‘ã€é€‰æ‹©æ€§å±æ€§è¿ç§»å’Œç»†ç²’åº¦å¤šæ¨¡æ€å¯¹é½ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSIGMAåœ¨å¤šç§ç¼–è¾‘å’Œç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å¯æ§æ€§ã€è·¨æ¡ä»¶ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ï¼Œå°¤å…¶åœ¨ç»„åˆä»»åŠ¡ä¸Šç›¸æ¯”Bagelå–å¾—äº†å¤§å¹…æ€§èƒ½æå‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.07564" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.07564.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.07554</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.07554" target="_blank">FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation</a>
      </h3>
      <p class="paper-title-zh">FlexIDï¼šé€šè¿‡æ„å›¾æ„ŸçŸ¥è°ƒåˆ¶å®ç°æ— éœ€è®­ç»ƒçš„èº«ä»½çµæ´»æ³¨å…¥ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Guandong Li, Yijun Ding</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†FlexIDï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ„å›¾æ„ŸçŸ¥è°ƒåˆ¶æ­£äº¤åœ°å°†èº«ä»½è§£è€¦ä¸ºè¯­ä¹‰å’Œè§†è§‰ä¸¤ä¸ªç»´åº¦ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”é—¨æ§æœºåˆ¶åŠ¨æ€è°ƒèŠ‚ï¼Œä»è€Œåœ¨èº«ä»½ä¿çœŸåº¦å’Œæ–‡æœ¬é€‚åº”æ€§ä¹‹é—´å®ç°äº†æœ€ä¼˜å¹³è¡¡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†èº«ä»½ä¿¡æ¯æ­£äº¤è§£è€¦ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šè¯­ä¹‰èº«ä»½æŠ•å½±å™¨ï¼ˆSIPï¼‰å°†é«˜å±‚è¯­ä¹‰å…ˆéªŒæ³¨å…¥è¯­è¨€ç©ºé—´ï¼Œè§†è§‰ç‰¹å¾é”šï¼ˆVFAï¼‰åœ¨æ½œåœ¨ç©ºé—´ä¸­ç¡®ä¿ç»“æ„ä¿çœŸåº¦ã€‚æ ¸å¿ƒæ˜¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥è‡ªé€‚åº”é—¨æ§ï¼ˆCAGï¼‰æœºåˆ¶ï¼Œå®ƒèƒ½æ ¹æ®ç¼–è¾‘æ„å›¾å’Œæ‰©æ•£æ—¶é—´æ­¥åŠ¨æ€è°ƒèŠ‚ä¸¤æ¡è·¯å¾„çš„æƒé‡ï¼Œåœ¨æ£€æµ‹åˆ°å¼ºç¼–è¾‘æ„å›¾æ—¶è‡ªåŠ¨æ”¾æ¾åˆšæ€§è§†è§‰çº¦æŸã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨IBenchä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlexIDåœ¨èº«ä»½ä¸€è‡´æ€§å’Œæ–‡æœ¬éµå¾ªæ€§ä¹‹é—´è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¹³è¡¡ï¼Œä¸ºå¤æ‚çš„å™äº‹ç”Ÿæˆæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.07554" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.07554.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.07345</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.07345" target="_blank">Optimizing Few-Step Generation with Adaptive Matching Distillation</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡è‡ªé€‚åº”åŒ¹é…è’¸é¦ä¼˜åŒ–å°‘æ­¥ç”Ÿæˆ</p>
      <p class="paper-authors">Lichen Bai, Zikai Zhou, Shitong Shao, Wenliang Zhong, Shuo Yang ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”åŒ¹é…è’¸é¦ï¼ˆAMDï¼‰ï¼Œä¸€ç§èƒ½å¤Ÿæ˜¾å¼æ£€æµ‹å¹¶é€ƒç¦»â€œç¦åŒºâ€çš„è‡ªæ ¡æ­£æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å°‘æ­¥ç”Ÿæˆæ¨¡å‹çš„æ ·æœ¬ä¿çœŸåº¦å’Œè®­ç»ƒé²æ£’æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è®ºæ–‡é¦–å…ˆå°†ç°æœ‰æ–¹æ³•ç»Ÿä¸€è§£é‡Šä¸ºè§„é¿â€œç¦åŒºâ€çš„éšå¼ç­–ç•¥ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒAMDå¼•å…¥å¥–åŠ±ä»£ç†æ¥æ˜¾å¼æ£€æµ‹â€œç¦åŒºâ€ï¼Œå¹¶é€šè¿‡ç»“æ„ä¿¡å·åˆ†è§£åŠ¨æ€ä¼˜å…ˆæ ¡æ­£æ¢¯åº¦ã€‚æ­¤å¤–ï¼Œæ–¹æ³•è¿˜æå‡ºäº†â€œæ’æ–¥æ€§æ™¯è§‚é”åŒ–â€æŠ€æœ¯ï¼Œä»¥æ„å»ºé™¡å³­çš„èƒ½é‡å±éšœæ¥é˜²æ­¢æ¨¡å‹å´©æºƒåˆ°å¤±è´¥æ¨¡å¼ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚SDXLã€Wan2.1ï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAMDæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå®ƒå°†SDXLåœ¨HPSv2æŒ‡æ ‡ä¸Šçš„å¾—åˆ†ä»30.64æå‡è‡³31.25ï¼Œè¶…è¶Šäº†ç°æœ‰å…ˆè¿›åŸºçº¿ã€‚è¿™éªŒè¯äº†åœ¨â€œç¦åŒºâ€å†…æ˜¾å¼ä¿®æ­£ä¼˜åŒ–è½¨è¿¹å¯¹äºæå‡å°‘æ­¥ç”Ÿæˆæ¨¡å‹æ€§èƒ½ä¸Šé™è‡³å…³é‡è¦ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.07345" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.07345.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.06959</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.06959" target="_blank">CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</a>
      </h3>
      <p class="paper-title-zh">CineSceneï¼šå°†éšå¼3Dä½œä¸ºç”µå½±è§†é¢‘ç”Ÿæˆçš„æœ‰æ•ˆåœºæ™¯è¡¨ç¤º</p>
      <p class="paper-authors">Kaiyi Huang, Yukun Huang, Yu Li, Jianhong Bai, Xintao Wang ç­‰ (11 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†CineSceneæ¡†æ¶ï¼Œé€šè¿‡éšå¼3Dæ„ŸçŸ¥çš„åœºæ™¯è¡¨ç¤ºå’Œåˆ›æ–°çš„ä¸Šä¸‹æ–‡æ¡ä»¶æœºåˆ¶ï¼Œå®ç°äº†åœºæ™¯ä¸åŠ¨æ€ä¸»ä½“è§£è€¦çš„ç”µå½±çº§è§†é¢‘ç”Ÿæˆï¼Œå¹¶æ„å»ºäº†ç”¨äºè®­ç»ƒçš„åœºæ™¯è§£è€¦æ•°æ®é›†ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡VGGTå°†åœºæ™¯å›¾åƒç¼–ç ä¸ºè§†è§‰è¡¨ç¤ºï¼Œç„¶åé€šè¿‡é¢å¤–çš„ä¸Šä¸‹æ–‡æ‹¼æ¥ï¼Œä»¥éšå¼æ–¹å¼å°†3Dæ„ŸçŸ¥ç‰¹å¾æ³¨å…¥é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»è€Œæ”¯æŒç›¸æœºè½¨è¿¹æ§åˆ¶ä¸”åœºæ™¯ä¸€è‡´çš„è§†é¢‘åˆæˆã€‚è®­ç»ƒæ—¶é‡‡ç”¨éšæœºæ‰“ä¹±è¾“å…¥åœºæ™¯å›¾åƒçš„ç­–ç•¥ä»¥å¢å¼ºé²æ£’æ€§ï¼Œå¹¶ä½¿ç”¨Unreal Engine 5æ„å»ºäº†åŒ…å«é™æ€åœºæ™¯å…¨æ™¯å›¾ã€ç›¸æœºè½¨è¿¹åŠæœ‰æ— åŠ¨æ€ä¸»ä½“çš„é…å¯¹è§†é¢‘æ•°æ®é›†ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒCineSceneåœ¨åœºæ™¯ä¸€è‡´çš„ç”µå½±è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿå¤„ç†å¤§å¹…åº¦çš„ç›¸æœºè¿åŠ¨ï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„ç¯å¢ƒä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.06959" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.06959.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>