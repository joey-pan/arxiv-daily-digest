<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-27</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | å›¾å½¢å­¦: 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.23235</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.23235" target="_blank">Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents</a>
      </h3>
      <p class="paper-title-zh">é¢å‘é«˜æ•ˆé«˜åˆ†è¾¨ç‡å›¾å½¢ç”¨æˆ·ç•Œé¢æ™ºèƒ½ä½“çš„æ—¶ç©ºä»¤ç‰Œå‰ªæ</p>
      <p class="paper-authors">Zhou Xu, Bowen Zhou, Qi Wang, Shuwen Feng, Jingyu Xiao</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†GUIPruneræ¡†æ¶ï¼Œé€šè¿‡è§£å†³ç°æœ‰å‹ç¼©æ–¹æ³•ä¸­çš„æ—¶é—´é”™ä½å’Œç©ºé—´æ‹“æ‰‘å†²çªé—®é¢˜ï¼Œé¦–æ¬¡å®ç°äº†å¯¹é«˜åˆ†è¾¨ç‡GUIå¯¼èˆªæ™ºèƒ½ä½“è§†è§‰è¾“å…¥çš„é«˜æ•ˆã€æ— è®­ç»ƒå‹ç¼©ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…æå‡æ•ˆç‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1) æ—¶é—´è‡ªé€‚åº”åˆ†è¾¨ç‡(TAR)ï¼ŒåŸºäºè¡°å‡æœºåˆ¶åŠ¨æ€è°ƒæ•´å†å²è½¨è¿¹çš„ç¼–ç åˆ†è¾¨ç‡ï¼Œä»¥åŒ¹é…æ™ºèƒ½ä½“çš„â€œè®°å¿†è¡°å‡â€æ³¨æ„åŠ›æ¨¡å¼ï¼›2) åˆ†å±‚ç»“æ„æ„ŸçŸ¥å‰ªæ(SSP)ï¼Œåœ¨ä¿æŠ¤å…¨å±€å¸ƒå±€å®Œæ•´æ€§çš„å‰æä¸‹ï¼Œä¼˜å…ˆä¿ç•™äº¤äº’å‰æ™¯å’Œè¯­ä¹‰é”šç‚¹ç­‰å…³é”®ç©ºé—´ä¿¡æ¯ï¼Œé¿å…åæ ‡å®šä½é”™è¯¯ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGUIPrunerèƒ½æŒç»­å–å¾—æœ€ä¼˜æ€§èƒ½ï¼Œæœ‰æ•ˆé˜²æ­¢é«˜å‹ç¼©ä¸‹çš„å¤§æ¨¡å‹æ€§èƒ½å´©æºƒã€‚ä»¥Qwen2-VL-2Bæ¨¡å‹ä¸ºä¾‹ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒ94%ä»¥ä¸ŠåŸå§‹æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†3.4å€çš„æµ®ç‚¹è¿ç®—é‡å‡å°‘å’Œ3.3å€çš„è§†è§‰ç¼–ç å»¶è¿ŸåŠ é€Ÿï¼Œä½¿é«˜ç²¾åº¦å®æ—¶GUIå¯¼èˆªå¾—ä»¥åœ¨æä½èµ„æºæ¶ˆè€—ä¸‹è¿è¡Œã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's "fading memory" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.23235" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.23235.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.23191</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.23191" target="_blank">Uni-Animator: Towards Unified Visual Colorization</a>
      </h3>
      <p class="paper-title-zh">Uni-Animatorï¼šè¿ˆå‘ç»Ÿä¸€çš„è§†è§‰ç€è‰²</p>
      <p class="paper-authors">Xinyuan Chen, Yao Xu, Shaowen Wang, Pengjie Song, Bowen Deng</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£Transformerçš„ç»Ÿä¸€å›¾åƒä¸è§†é¢‘è‰å›¾ç€è‰²æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è·¨ä»»åŠ¡ç»Ÿä¸€ã€é¢œè‰²è¿ç§»ç²¾åº¦ã€ç»†èŠ‚ä¿æŒå’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é‡‡ç”¨æ‰©æ•£Transformeræ¶æ„ï¼Œé€šè¿‡å®ä¾‹å—åµŒå…¥å¢å¼ºè§†è§‰å‚è€ƒä»¥å®ç°ç²¾ç¡®é¢œè‰²å¯¹é½ä¸èåˆï¼›åˆ©ç”¨ç‰©ç†ç‰¹å¾å¼ºåŒ–æœºåˆ¶æ•æ‰å¹¶ä¿ç•™é«˜é¢‘çº¹ç†ç»†èŠ‚ï¼›è®¾è®¡äº†åŸºäºè‰å›¾çš„åŠ¨æ€RoPEç¼–ç ï¼Œè‡ªé€‚åº”å»ºæ¨¡è¿åŠ¨æ„ŸçŸ¥çš„æ—¶ç©ºä¾èµ–å…³ç³»ä»¥æå‡æ—¶åºä¸€è‡´æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒUni-Animatoråœ¨å›¾åƒå’Œè§†é¢‘è‰å›¾ç€è‰²ä»»åŠ¡ä¸Šå‡è¾¾åˆ°ä¸ä»»åŠ¡ä¸“ç”¨æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è·¨åŸŸç»Ÿä¸€èƒ½åŠ›ï¼Œå…·æœ‰é«˜ç»†èŠ‚ä¿çœŸåº¦å’Œé²æ£’çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œå°¤å…¶åœ¨å¤§è¿åŠ¨åœºæ™¯ä¸­æœ‰æ•ˆå‡å°‘äº†è¿åŠ¨ä¼ªå½±ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.23191" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.23191.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">å›¾å½¢å­¦</span>
          <span class="paper-id">2602.23010</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.23010" target="_blank">HELMLAB: An Analytical, Data-Driven Color Space for Perceptual Distance in UI Design Systems</a>
      </h3>
      <p class="paper-title-zh">HELMLABï¼šä¸€ç§ç”¨äºUIè®¾è®¡ç³»ç»Ÿä¸­æ„ŸçŸ¥è·ç¦»çš„åˆ†æå‹æ•°æ®é©±åŠ¨è‰²å½©ç©ºé—´</p>
      <p class="paper-authors">Gorkem Yildiz</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†HELMLABï¼Œä¸€ä¸ªåŒ…å«72ä¸ªå‚æ•°çš„åˆ†æå‹è‰²å½©ç©ºé—´ï¼Œæ—¨åœ¨ä¸ºUIè®¾è®¡ç³»ç»Ÿæä¾›æ›´å‡†ç¡®çš„è‰²å½©æ„ŸçŸ¥è·ç¦»åº¦é‡ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºCIEDE2000æ ‡å‡†ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡ä¸€ç³»åˆ—å¯å­¦ä¹ çš„çŸ©é˜µã€é€é€šé“å¹‚å‹ç¼©ã€å‚…é‡Œå¶è‰²è°ƒæ ¡æ­£ä»¥åŠå†…åµŒçš„äº¥å§†éœå…¹-ç§‘å‹’åŠ³æ–½æ˜åº¦è°ƒæ•´ï¼Œå°†CIE XYZè‰²å½©ç©ºé—´æ˜ å°„åˆ°æ„ŸçŸ¥ç»„ç»‡çš„Labè¡¨ç¤ºã€‚æµç¨‹ååŒ…å«ä¸­æ€§è‰²æ ¡æ­£ä»¥ç¡®ä¿æ¶ˆè‰²å·®è‰²å½©æ˜ å°„å‡†ç¡®ï¼Œå¹¶é€šè¿‡åˆšæ€§æ—‹è½¬ä¼˜åŒ–è‰²è°ƒè§’åº¦å¯¹é½è€Œä¸å½±å“è·ç¦»åº¦é‡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨COMBVDæ•°æ®é›†ï¼ˆ3,813ä¸ªè‰²å½©å¯¹ï¼‰ä¸Šï¼ŒHELMLABçš„STRESSæŒ‡æ ‡ä¸º23.22ï¼Œè¾ƒCIEDE2000ï¼ˆ29.18ï¼‰é™ä½äº†20.4%ã€‚è·¨æ•°æ®é›†éªŒè¯æ˜¾ç¤ºå…¶å…·æœ‰ç«äº‰åŠ›çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸”å˜æ¢å¯é€†ï¼Œå¾€è¿”è¯¯å·®ä½äº10^-14ã€‚è¯¥ç©ºé—´è¿˜é›†æˆäº†è‰²åŸŸæ˜ å°„ã€è®¾è®¡ä»¤ç‰Œå¯¼å‡ºåŠæ˜æš—æ¨¡å¼é€‚é…ç­‰å®ç”¨å·¥å…·ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We present HELMLAB, a 72-parameter analytical color space for UI design systems. The forward transform maps CIE XYZ to a perceptually-organized Lab representation through learned matrices, per-channel power compression, Fourier hue correction, and embedded Helmholtz-Kohlrausch lightness adjustment. A post-pipeline neutral correction guarantees that achromatic colors map to a=b=0 (chroma < 10^-6), and a rigid rotation of the chromatic plane improves hue-angle alignment without affecting the distance metric, which is invariant under isometries. On the COMBVD dataset (3,813 color pairs), HELMLAB achieves a STRESS of 23.22, a 20.4% reduction from CIEDE2000 (29.18). Cross-validation on He et al. 2022 and MacAdam 1974 shows competitive cross-dataset performance. The transform is invertible with round-trip errors below 10^-14. Gamut mapping, design-token export, and dark/light mode adaptation utilities are included for use in web and mobile design systems.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.23010" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.23010.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.22948</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22948" target="_blank">ToProVAR: Efficient Visual Autoregressive Modeling via Tri-Dimensional Entropy-Aware Semantic Analysis and Sparsity Optimization</a>
      </h3>
      <p class="paper-title-zh">ToProVARï¼šé€šè¿‡ä¸‰ç»´ç†µæ„ŸçŸ¥è¯­ä¹‰åˆ†æä¸ç¨€ç–æ€§ä¼˜åŒ–çš„é«˜æ•ˆè§†è§‰è‡ªå›å½’å»ºæ¨¡</p>
      <p class="paper-authors">Jiayu Chen, Ruoyu Lin, Zihao Zheng, Jingxin Li, Maoliang Li ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´ï¼ˆè¯å…ƒã€å±‚ã€å°ºåº¦ï¼‰ç†µæ„ŸçŸ¥è¯­ä¹‰åˆ†æä¸ç¨€ç–æ€§ä¼˜åŒ–çš„è§†è§‰è‡ªå›å½’æ¨¡å‹åŠ é€Ÿæ¡†æ¶ï¼Œä»æ ¹æœ¬ä¸Šæ”¹è¿›äº†ä¼ ç»Ÿå¯å‘å¼è·³è¿‡ç­–ç•¥ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡æ•ˆç‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆåˆ©ç”¨æ³¨æ„åŠ›ç†µåˆ†ææ¨¡å‹åœ¨ä¸åŒè¯å…ƒç²’åº¦ã€è¯­ä¹‰èŒƒå›´å’Œç”Ÿæˆå°ºåº¦ä¸‹çš„å‚æ•°åŠ¨æ€ç‰¹æ€§ï¼›è¿›è€Œè¯†åˆ«å‡ºè¯å…ƒã€å±‚å’Œå°ºåº¦ä¸‰ä¸ªç»´åº¦çš„ç¨€ç–æ€§æ¨¡å¼ï¼›æœ€åé’ˆå¯¹è¿™äº›æ¨¡å¼è®¾è®¡ç»†ç²’åº¦çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°ç²¾å‡†çš„è®¡ç®—åŠ é€Ÿã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨Infinity-2Bå’ŒInfinity-8Bæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒToProVARæœ€é«˜å¯å®ç°3.4å€çš„ç”ŸæˆåŠ é€Ÿï¼Œä¸”è´¨é‡æŸå¤±æå°ï¼Œåœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚FastVARå’ŒSkipVARï¼‰ï¼Œæœ‰æ•ˆè§£å†³äº†è§†è§‰è‡ªå›å½’æ¨¡å‹åæœŸé˜¶æ®µçš„æ•ˆç‡ç“¶é¢ˆé—®é¢˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Visual Autoregressive(VAR) models enhance generation quality but face a critical efficiency bottleneck in later stages. In this paper, we present a novel optimization framework for VAR models that fundamentally differs from prior approaches such as FastVAR and SkipVAR. Instead of relying on heuristic skipping strategies, our method leverages attention entropy to characterize the semantic projections across different dimensions of the model architecture. This enables precise identification of parameter dynamics under varying token granularity levels, semantic scopes, and generation scales. Building on this analysis, we further uncover sparsity patterns along three critical dimensions-token, layer, and scale-and propose a set of fine-grained optimization strategies tailored to these patterns. Extensive evaluation demonstrates that our approach achieves aggressive acceleration of the generation process while significantly preserving semantic fidelity and fine details, outperforming traditional methods in both efficiency and quality. Experiments on Infinity-2B and Infinity-8B models demonstrate that ToProVAR achieves up to 3.4x acceleration with minimal quality loss, effectively mitigating the issues found in prior work. Our code will be made publicly available.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22948" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22948.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.22809</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22809" target="_blank">PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning</a>
      </h3>
      <p class="paper-title-zh">PhotoAgentï¼šåŸºäºæ¢ç´¢æ€§è§†è§‰ç¾å­¦è§„åˆ’çš„æ™ºèƒ½ç…§ç‰‡ç¼–è¾‘</p>
      <p class="paper-authors">Mingde Yao, Zhiyuan You, Tam-King Man, Menglu Wang, Tianfan Xue</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»è¿›è¡Œç…§ç‰‡ç¼–è¾‘çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡æ˜¾å¼çš„ç¾å­¦è§„åˆ’å°†ç¼–è¾‘ä»»åŠ¡è½¬åŒ–ä¸ºé•¿ç¨‹å†³ç­–é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç”¨äºçœŸå®åœºæ™¯è¯„ä¼°çš„ç¾å­¦è¯„ä»·åŸºå‡†ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥ç³»ç»Ÿå°†è‡ªä¸»å›¾åƒç¼–è¾‘å»ºæ¨¡ä¸ºä¸€ä¸ªé•¿ç¨‹å†³ç­–é—®é¢˜ã€‚å®ƒé¦–å…ˆæ¨ç†ç”¨æˆ·çš„ç¾å­¦æ„å›¾ï¼Œç„¶åé€šè¿‡æ ‘æœç´¢è§„åˆ’å¤šæ­¥ç¼–è¾‘åŠ¨ä½œï¼Œæœ€ååˆ©ç”¨è®°å¿†å’Œè§†è§‰åé¦ˆè¿›è¡Œé—­ç¯è¿­ä»£æ‰§è¡Œï¼Œæ•´ä¸ªè¿‡ç¨‹æ— éœ€ç”¨æˆ·é€æ­¥æä¾›æŒ‡ä»¤ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒPhotoAgentåœ¨æŒ‡ä»¤éµå¾ªåº¦å’Œè§†è§‰è´¨é‡æ–¹é¢å‡æœ‰æŒç»­æå‡ã€‚ä¸ºæ”¯æŒè¯„ä¼°è€Œæ„å»ºçš„æµ‹è¯•é›†ï¼ˆåŒ…å«1,017å¼ ç…§ç‰‡ï¼‰ä¹Ÿç³»ç»Ÿæ€§åœ°éªŒè¯äº†å…¶è‡ªä¸»ç…§ç‰‡ç¼–è¾‘æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22809" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22809.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>