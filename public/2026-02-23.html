<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-23</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.18422</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.18422" target="_blank">Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control</a>
      </h3>
      <p class="paper-title-zh">ç”Ÿæˆç°å®ï¼šåŸºäºæ‰‹éƒ¨å’Œç›¸æœºæ§åˆ¶çš„äº¤äº’å¼è§†é¢‘ç”Ÿæˆçš„äººæœ¬ä¸–ç•Œæ¨¡æ‹Ÿ</p>
      <p class="paper-authors">Linxi Xie, Lisong C. Sun, Ashley Neall, Tong Wu, Shengqu Cai ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è·Ÿè¸ªçš„å¤´éƒ¨å§¿æ€å’Œå…³èŠ‚çº§æ‰‹éƒ¨å§¿æ€ç”Ÿæˆè™šæ‹Ÿç¯å¢ƒï¼Œå®ç°äº†çµå·§çš„æ‰‹-ç‰©äº¤äº’ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¯ç”¨äºå…·èº«äº¤äº’çš„å› æœå¼ç”Ÿæˆç³»ç»Ÿã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è®ºæ–‡è¯„ä¼°äº†ç°æœ‰çš„æ‰©æ•£Transformeræ¡ä»¶æ§åˆ¶ç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„3Då¤´éƒ¨å’Œæ‰‹éƒ¨å§¿æ€æ§åˆ¶æœºåˆ¶ã€‚é¦–å…ˆï¼Œä½¿ç”¨è¯¥ç­–ç•¥è®­ç»ƒäº†ä¸€ä¸ªåŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œç„¶åé€šè¿‡çŸ¥è¯†è’¸é¦å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªå› æœå¼çš„ã€äº¤äº’å¼çš„ç³»ç»Ÿï¼Œç”¨äºç”Ÿæˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è™šæ‹Ÿç¯å¢ƒã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> é€šè¿‡äººç±»å—è¯•è€…è¯„ä¼°è¡¨æ˜ï¼Œä¸ç›¸å…³åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿèƒ½æ˜¾è‘—æå‡ç”¨æˆ·åœ¨è™šæ‹Ÿç¯å¢ƒä¸­çš„ä»»åŠ¡è¡¨ç°ï¼Œå¹¶ä¸”ç”¨æˆ·æ„ŸçŸ¥åˆ°çš„å¯¹è‡ªèº«åŠ¨ä½œçš„æ§åˆ¶ç¨‹åº¦ä¹Ÿæ˜¾è‘—æ›´é«˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.18422" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.18422.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.18309</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.18309" target="_blank">Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡é…å¯¹å±€éƒ¨æ–‡æœ¬ä¸è‰å›¾è¿›è¡Œå¤šå±‚çº§æ¡ä»¶æ§åˆ¶çš„æ—¶å°šå›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Ziyue Liu, Davide Talon, Federico Girella, Zanxi Ruan, Mattia Mondo ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†LOTSæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå…¨å±€è‰å›¾å¼•å¯¼ä¸å¤šä¸ªå±€éƒ¨è‰å›¾-æ–‡æœ¬å¯¹æ¥å¢å¼ºæ—¶å°šå›¾åƒç”Ÿæˆï¼›å¹¶åˆ›å»ºäº†é¦–ä¸ªåŒ…å«å¤šç»„æ–‡æœ¬-è‰å›¾é…å¯¹æ ‡æ³¨çš„æ—¶å°šæ•°æ®é›†Sketchyã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰å¤šå±‚çº§æ¡ä»¶ç¼–ç é˜¶æ®µï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­ç‹¬ç«‹ç¼–ç å±€éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒå…¨å±€ç»“æ„åè°ƒï¼›2ï¼‰æ‰©æ•£é…å¯¹å¼•å¯¼é˜¶æ®µï¼Œåœ¨æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„å¼•å¯¼æœºåˆ¶æ•´åˆå±€éƒ¨ä¸å…¨å±€æ¡ä»¶ä¿¡æ¯ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éµå¾ªå…¨å±€ç»“æ„çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆåˆ©ç”¨æ›´ä¸°å¯Œçš„å±€éƒ¨è¯­ä¹‰å¼•å¯¼ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼›æ‰€æ„å»ºçš„Sketchyæ•°æ®é›†ï¼ˆåŒ…å«ä¸“ä¸šè‰å›¾ä¸â€œé‡ç”Ÿâ€éä¸“ä¸šè‰å›¾ï¼‰éªŒè¯äº†æ–¹æ³•çš„é²æ£’æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.18309" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.18309.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.18282</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.18282" target="_blank">DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control</a>
      </h3>
      <p class="paper-title-zh">DEIGï¼šåŸºäºç»†ç²’åº¦è¯­ä¹‰æ§åˆ¶çš„ç»†èŠ‚å¢å¼ºå®ä¾‹ç”Ÿæˆ</p>
      <p class="paper-authors">Shiyan Du, Conghan Yue, Xinyu Cheng, Dongyu Zhang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†DEIGæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å®ä¾‹ç»†èŠ‚æå–å™¨å’Œç»†èŠ‚èåˆæ¨¡å—ï¼Œè§£å†³äº†å¤šå®ä¾‹ç”Ÿæˆä¸­ç»†ç²’åº¦è¯­ä¹‰ç†è§£ä¸å±æ€§è·¨å®ä¾‹æ³„æ¼çš„éš¾é¢˜ï¼Œå®ç°äº†å¯¹å¤æ‚æ–‡æœ¬æè¿°çš„ç²¾ç¡®ã€å¯æ§åœºæ™¯ç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•ä¸»è¦åŒ…æ‹¬ï¼š1ï¼‰è®¾è®¡å®ä¾‹ç»†èŠ‚æå–å™¨ï¼ˆIDEï¼‰ï¼Œå°†æ–‡æœ¬ç¼–ç å™¨åµŒå…¥è½¬æ¢ä¸ºç´§å‡‘çš„å®ä¾‹æ„ŸçŸ¥è¡¨ç¤ºï¼›2ï¼‰æ„å»ºç»†èŠ‚èåˆæ¨¡å—ï¼ˆDFMï¼‰ï¼Œé‡‡ç”¨åŸºäºå®ä¾‹çš„æ©ç æ³¨æ„åŠ›æœºåˆ¶ï¼Œé˜²æ­¢ä¸åŒå®ä¾‹é—´çš„å±æ€§æ³„éœ²ï¼›3ï¼‰åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºé«˜è´¨é‡ç»†ç²’åº¦æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶å»ºç«‹åŒ…å«åŒºåŸŸçº§æ ‡æ³¨çš„è¯„ä¼°åŸºå‡†DEIG-Benchã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒDEIGåœ¨ç©ºé—´ä¸€è‡´æ€§ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œç»„åˆæ³›åŒ–èƒ½åŠ›ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”å¯ä½œä¸ºå³æ’å³ç”¨æ¨¡å—æ— ç¼é›†æˆåˆ°åŸºäºæ‰©æ•£çš„ç”Ÿæˆæµç¨‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚å¤šå®ä¾‹åœºæ™¯çš„ç”Ÿæˆè´¨é‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.18282" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.18282.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.18093</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.18093" target="_blank">Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers</a>
      </h3>
      <p class="paper-title-zh">é¢„æµ‹ä»¥è·³è¿‡ï¼šç”¨äºé«˜æ•ˆæ‰©æ•£å˜æ¢å™¨çš„çº¿æ€§å¤šæ­¥ç‰¹å¾é¢„æµ‹</p>
      <p class="paper-authors">Hanshuai Cui, Zhiqing Tang, Qianli Ma, Zhi Yao, Weijia Jia</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ é€Ÿæ¡†æ¶PrediTï¼Œé€šè¿‡çº¿æ€§å¤šæ­¥æ–¹æ³•é¢„æµ‹æ‰©æ•£æ¨¡å‹æœªæ¥çš„ç‰¹å¾è¾“å‡ºï¼Œè€Œéç®€å•åœ°é‡ç”¨ç¼“å­˜ç‰¹å¾ï¼Œä»è€Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å»¶è¿Ÿã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†ç‰¹å¾é¢„æµ‹å»ºæ¨¡ä¸ºçº¿æ€§å¤šæ­¥é—®é¢˜ï¼Œåˆ©ç”¨ç»å…¸çº¿æ€§å¤šæ­¥æ³•æ ¹æ®å†å²ä¿¡æ¯é¢„æµ‹æœªæ¥æ¨¡å‹è¾“å‡ºï¼›å¼•å…¥ä¸€ä¸ªæ ¡æ­£å™¨ï¼Œåœ¨ç‰¹å¾å˜åŒ–å‰§çƒˆçš„åŒºåŸŸæ¿€æ´»ä»¥é˜²æ­¢è¯¯å·®ç´¯ç§¯ï¼›å¹¶è®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€æ­¥é•¿è°ƒåˆ¶æœºåˆ¶ï¼Œé€šè¿‡ç›‘æµ‹ç‰¹å¾å˜åŒ–ç‡è‡ªé€‚åº”è°ƒæ•´é¢„æµ‹èŒƒå›´ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åŸºäºDiTçš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸Šå®ç°äº†æœ€é«˜5.54å€çš„å»¶è¿Ÿé™ä½ï¼Œä¸”ç”Ÿæˆè´¨é‡ä¸‹é™å¯å¿½ç•¥ä¸è®¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.18093" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.18093.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.18022</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.18022" target="_blank">Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers</a>
      </h3>
      <p class="paper-title-zh">ç”¨äºæ‰©æ•£Transformerä¸­å…è®­ç»ƒå›¾åƒç¼–è¾‘æ§åˆ¶çš„åŒé€šé“æ³¨æ„åŠ›å¼•å¯¼</p>
      <p class="paper-authors">Guandong Li, Mengxia Ye</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†åŒé€šé“æ³¨æ„åŠ›å¼•å¯¼ï¼ˆDCAGï¼‰æ¡†æ¶ï¼Œé¦–æ¬¡åœ¨æ‰©æ•£Transformerï¼ˆDiTï¼‰ä¸­åŒæ—¶åˆ©ç”¨Keyé€šé“å’ŒValueé€šé“è¿›è¡Œå…è®­ç»ƒçš„ç¼–è¾‘å¼ºåº¦æ§åˆ¶ï¼Œå®ç°äº†æ¯”å•é€šé“æ–¹æ³•æ›´ç²¾ç¡®çš„ç¼–è¾‘-ä¿çœŸåº¦æƒè¡¡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºå¯¹DiTä¸­å¤šæ¨¡æ€æ³¨æ„åŠ›å±‚çš„è§‚å¯Ÿï¼šKeyå’ŒValueæŠ•å½±å‡å‘ˆç°æ˜æ˜¾çš„åç½®-å¢é‡ç»“æ„ã€‚DCAGåŒæ—¶æ“çºµKeyé€šé“ï¼ˆæ§åˆ¶æ³¨æ„åŠ›æŠ•å‘ä½•å¤„ï¼‰å’ŒValueé€šé“ï¼ˆæ§åˆ¶ç‰¹å¾èšåˆå†…å®¹ï¼‰ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒKeyé€šé“é€šè¿‡éçº¿æ€§softmaxå‡½æ•°è¿›è¡Œç²—ç²’åº¦æ§åˆ¶ï¼Œè€ŒValueé€šé“é€šè¿‡çº¿æ€§åŠ æƒæ±‚å’Œè¿›è¡Œç»†ç²’åº¦è¡¥å……ï¼ŒäºŒè€…å…±åŒæ„æˆä¸€ä¸ªäºŒç»´å‚æ•°ç©ºé—´ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨PIE-BenchåŸºå‡†æµ‹è¯•ï¼ˆ700å¼ å›¾åƒï¼Œ10ä¸ªç¼–è¾‘ç±»åˆ«ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDCAGåœ¨æ‰€æœ‰ä¿çœŸåº¦æŒ‡æ ‡ä¸Šå‡ä¼˜äºä»…ä½¿ç”¨Keyå¼•å¯¼çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å±€éƒ¨ç¼–è¾‘ä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡åˆ é™¤å’Œå¯¹è±¡æ·»åŠ ï¼‰ä¸­æå‡æ˜¾è‘—ï¼Œåˆ†åˆ«å®ç°äº†4.9%å’Œ3.2%çš„LPIPSæŒ‡æ ‡é™ä½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(Î´_k, Î´_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.18022" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.18022.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>