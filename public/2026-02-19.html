<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-19</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | å›¾å½¢å­¦: 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.15727</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 90/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.15727" target="_blank">Spanning the Visual Analogy Space with a Weight Basis of LoRAs</a>
      </h3>
      <p class="paper-title-zh">åˆ©ç”¨LoRAæƒé‡åŸºå¼ æˆè§†è§‰ç±»æ¯”ç©ºé—´</p>
      <p class="paper-authors">Hila Manor, Rinon Gal, Haggai Maron, Tomer Michaeli, Gal Chechik</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºLoRWeBæ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€ç»„åˆå­¦ä¹ åˆ°çš„å˜æ¢åŸºå…ƒæ¥ä¸“é—¨å¤„ç†æ¯ä¸ªç±»æ¯”ä»»åŠ¡ï¼Œçªç ´äº†ç°æœ‰æ–¹æ³•ä½¿ç”¨å•ä¸€å›ºå®šé€‚é…æ¨¡å—çš„æ³›åŒ–é™åˆ¶ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸€æ˜¯å­¦ä¹ ä¸€ç»„LoRAæ¨¡å—ä½œä¸ºåŸºï¼Œä»¥è¦†ç›–ä¸åŒçš„è§†è§‰å˜æ¢ç©ºé—´ï¼›äºŒæ˜¯ä¸€ä¸ªè½»é‡çº§ç¼–ç å™¨ï¼Œæ ¹æ®è¾“å…¥çš„ç±»æ¯”å¯¹åŠ¨æ€é€‰æ‹©å’ŒåŠ æƒè¿™äº›åŸºLoRAï¼Œå®ç°æ¨ç†æ—¶çš„åŠ¨æ€ç»„åˆã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡äº†å¯¹æœªè§è§†è§‰å˜æ¢çš„æ³›åŒ–èƒ½åŠ›ï¼›ç ”ç©¶ç»“æœè¯æ˜LoRAåŸºåˆ†è§£æ˜¯å®ç°çµæ´»è§†è§‰æ“æ§çš„æœ‰æ•ˆæ–¹å‘ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}'$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}'$ such that $\mathbf{a} : \mathbf{a}' :: \mathbf{b} : \mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a "space of LoRAs". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.15727" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.15727.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">å›¾å½¢å­¦</span>
          <span class="paper-id">2602.16611</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.16611" target="_blank">Style-Aware Gloss Control for Generative Non-Photorealistic Rendering</a>
      </h3>
      <p class="paper-title-zh">é¢å‘ç”Ÿæˆå¼éçœŸå®æ„Ÿæ¸²æŸ“çš„ã€å…·å¤‡é£æ ¼æ„ŸçŸ¥çš„å…‰æ³½åº¦æ§åˆ¶</p>
      <p class="paper-authors">Santiago Jimenez-Navarro, Belen Masia, Ana Serrano</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½å¤Ÿè§£è€¦è‰ºæœ¯é£æ ¼ä¸ç‰©ä½“å…‰æ³½åº¦è¡¨å¾çš„æ— ç›‘ç£ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§é€‚é…å™¨ï¼Œå®ç°äº†åœ¨éçœŸå®æ„Ÿå›¾åƒåˆæˆä¸­å¯¹è¿™ä¸¤ä¸ªå› ç´ è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªæ–°çš„ã€ç³»ç»Ÿæ€§åœ°åŒ…å«ä¸åŒè‰ºæœ¯é£æ ¼å’Œå…‰æ³½åº¦å˜åŒ–çš„ç»˜ç”»å¯¹è±¡æ•°æ®é›†ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œä»–ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ— ç›‘ç£ç”Ÿæˆæ¨¡å‹ï¼Œå…¶å­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´å…·æœ‰å±‚æ¬¡åŒ–ç»“æ„ï¼Œèƒ½å°†å…‰æ³½åº¦ä¸å…¶ä»–å¤–è§‚å› ç´ è§£è€¦ã€‚éšåï¼Œä»–ä»¬å¼•å…¥ä¸€ä¸ªè½»é‡çº§é€‚é…å™¨ï¼Œå°†è¯¥è§£è€¦çš„æ½œåœ¨ç©ºé—´ä¸ä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹è¿æ¥èµ·æ¥ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹å­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´æ˜¯å±‚æ¬¡åŒ–çš„ï¼Œå…¶ä¸­å…‰æ³½åº¦ä¸å…¶ä»–å¤–è§‚å› ç´ ï¼ˆå¦‚é£æ ¼ï¼‰æˆåŠŸè§£è€¦ï¼Œè¿™ä¸ºç ”ç©¶ä¸åŒè‰ºæœ¯é£æ ¼ä¸‹å…‰æ³½åº¦çš„è¡¨å¾æ–¹å¼æä¾›äº†åŸºç¡€ã€‚ä¸å…ˆå‰æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è§£è€¦æ€§å’Œå¯¹å­¦ä¹ å› ç´ ï¼ˆé£æ ¼ä¸å…‰æ³½åº¦ï¼‰çš„å¯æ§æ€§æ–¹é¢å‡æœ‰æå‡ï¼Œèƒ½å¤Ÿåˆæˆå‡ºå…·æœ‰æŒ‡å®šé£æ ¼å’Œç²¾ç¡®å…‰æ³½åº¦çš„éçœŸå®æ„Ÿå›¾åƒã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Humans can infer material characteristics of objects from their visual appearance, and this ability extends to artistic depictions, where similar perceptual strategies guide the interpretation of paintings or drawings. Among the factors that define material appearance, gloss, along with color, is widely regarded as one of the most important, and recent studies indicate that humans can perceive gloss independently of the artistic style used to depict an object. To investigate how gloss and artistic style are represented in learned models, we train an unsupervised generative model on a newly curated dataset of painterly objects designed to systematically vary such factors. Our analysis reveals a hierarchical latent space in which gloss is disentangled from other appearance factors, allowing for a detailed study of how gloss is represented and varies across artistic styles. Building on this representation, we introduce a lightweight adapter that connects our style- and gloss-aware latent space to a latent-diffusion model, enabling the synthesis of non-photorealistic images with fine-grained control of these factors. We compare our approach with previous models and observe improved disentanglement and controllability of the learned factors.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.16611" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.16611.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.15819</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.15819" target="_blank">VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation</a>
      </h3>
      <p class="paper-title-zh">VideoSketcherï¼šåˆ©ç”¨è§†é¢‘æ¨¡å‹å…ˆéªŒå®ç°å¤šåŠŸèƒ½åºåˆ—è‰å›¾ç”Ÿæˆ</p>
      <p class="paper-authors">Hui Ren, Yuval Alaluf, Omer Bar Tal, Alexander Schwing, Antonio Torralba ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå…·æœ‰æ—¶åºç»“æ„çš„è‰å›¾ç»˜åˆ¶è¿‡ç¨‹ï¼Œå®ç°äº†å¯¹ç»˜åˆ¶é¡ºåºçš„è¯­ä¹‰æ§åˆ¶ä¸é«˜è´¨é‡è§†è§‰å‘ˆç°çš„ç»“åˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†è‰å›¾è¡¨ç¤ºä¸ºåœ¨ç©ºç™½ç”»å¸ƒä¸Šé€æ­¥ç»˜åˆ¶ç¬”åˆ’çš„çŸ­è§†é¢‘ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼šé¦–å…ˆåˆ©ç”¨å…·æœ‰å¯æ§æ—¶åºç»“æ„çš„åˆæˆå½¢çŠ¶ç»„åˆå­¦ä¹ ç¬”åˆ’é¡ºåºï¼Œç„¶åä»…ç”¨æå°‘é‡ï¼ˆå¦‚7ä¸ªï¼‰äººå·¥ç»˜åˆ¶çš„è‰å›¾è¿‡ç¨‹æ•°æ®æ¥å­¦ä¹ è‰å›¾çš„å¤–è§‚ç»†èŠ‚ï¼Œä»è€Œè§£è€¦é¡ºåºå­¦ä¹ ä¸å¤–è§‚å­¦ä¹ ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å°½ç®¡ä½¿ç”¨çš„äººç±»ç»˜åˆ¶è‰å›¾æ•°æ®æå°‘ï¼Œè¯¥æ–¹æ³•èƒ½ç”Ÿæˆé«˜è´¨é‡ã€æ—¶åºè¿è´¯çš„åºåˆ—è‰å›¾ï¼Œèƒ½ç´§å¯†éµå¾ªæ–‡æœ¬æŒ‡å®šçš„ç»˜åˆ¶é¡ºåºï¼Œå¹¶å±•ç°ä¸°å¯Œçš„è§†è§‰ç»†èŠ‚ï¼›æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯æ‰©å±•æ”¯æŒç¬”åˆ·é£æ ¼æ¡ä»¶æ§åˆ¶ä¸è‡ªå›å½’è‰å›¾ç”Ÿæˆï¼Œå¢å¼ºäº†å¯æ§æ€§ä¸äº¤äº’åä½œç»˜å›¾èƒ½åŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.15819" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.15819.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.15539</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.15539" target="_blank">Dynamic Training-Free Fusion of Subject and Style LoRAs</a>
      </h3>
      <p class="paper-title-zh">åŸºäºä¸»é¢˜ä¸é£æ ¼LoRAçš„åŠ¨æ€å…è®­ç»ƒèåˆæ–¹æ³•</p>
      <p class="paper-authors">Qinglong Cao, Yuntian Chen, Chao Ma, Xiaokang Yang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åŠ¨æ€å…è®­ç»ƒçš„LoRAèåˆæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ•´ä¸ªç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°èåˆä¸»é¢˜ä¸é£æ ¼LoRAï¼Œå®ç°è¿è´¯çš„ä¸»é¢˜-é£æ ¼åˆæˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªäº’è¡¥æœºåˆ¶ï¼šåœ¨å‰å‘ä¼ æ’­é˜¶æ®µï¼Œé€šè¿‡åŠ¨æ€è®¡ç®—åŸºç¡€æ¨¡å‹ä¸å„LoRAè¾“å‡ºç‰¹å¾é—´çš„KLæ•£åº¦ï¼Œåœ¨æ¯ä¸€å±‚è‡ªé€‚åº”é€‰æ‹©æœ€åˆé€‚çš„æƒé‡è¿›è¡Œèåˆï¼›åœ¨åå‘å»å™ªé˜¶æ®µï¼Œåˆ©ç”¨CLIPå’ŒDINOç­‰å®¢è§‚åº¦é‡å¯¼å‡ºçš„æ¢¯åº¦æ ¡æ­£ï¼Œå¯¹ç”Ÿæˆè½¨è¿¹è¿›è¡ŒæŒç»­è¯­ä¹‰ä¸é£æ ¼å¼•å¯¼ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å¤šç§ä¸»é¢˜-é£æ ¼ç»„åˆä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„LoRAèåˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆæˆç¬¦åˆé¢„æœŸä¸»é¢˜ä¸é£æ ¼çš„å›¾åƒã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.15539" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.15539.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.15396</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.15396" target="_blank">Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint SchrÃ¶dinger Bridge Matching</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡ä¼´éšè–›å®šè°”æ¡¥åŒ¹é…å®ç°è¶…è¶Šæ— è®°å¿†æ‰©æ•£çš„é«˜æ•ˆç”Ÿæˆå»ºæ¨¡</p>
      <p class="paper-authors">Jeongwoo Shin, Jinhwan Sul, Joonseok Lee, Jaewong Choi, Jaemoo Choi</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¼´éšè–›å®šè°”æ¡¥åŒ¹é…ï¼ˆASBMï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºæ•°æ®ä¸èƒ½é‡å®šä¹‰å…ˆéªŒä¹‹é—´çš„æœ€ä¼˜è€¦åˆï¼Œå®ç°äº†æ›´ç›´ã€æ›´é«˜æ•ˆçš„é«˜ç»´ç”Ÿæˆé‡‡æ ·è½¨è¿¹ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ASBMåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œå°†è–›å®šè°”æ¡¥çš„å‰å‘åŠ¨æ€è§†ä¸ºè€¦åˆæ„å»ºé—®é¢˜ï¼Œé€šè¿‡æ•°æ®åˆ°èƒ½é‡é‡‡æ ·çš„è§†è§’å­¦ä¹ ï¼Œå°†æ•°æ®ä¼ è¾“è‡³èƒ½é‡å®šä¹‰çš„å…ˆéªŒåˆ†å¸ƒï¼›ç„¶åï¼Œåˆ©ç”¨è¯±å¯¼å‡ºçš„æœ€ä¼˜è€¦åˆä½œä¸ºç›‘ç£ï¼Œé€šè¿‡ç®€å•çš„åŒ¹é…æŸå¤±å­¦ä¹ åå‘ç”ŸæˆåŠ¨æ€ã€‚è¯¥æ–¹æ³•æ‘†è„±äº†æ— è®°å¿†é™åˆ¶ï¼Œç›´æ¥å­¦ä¹ æœ€ä¼˜ä¼ è¾“è·¯å¾„ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒASBMèƒ½ç”Ÿæˆæ˜¾è‘—æ›´ç›´ã€æ›´é«˜æ•ˆçš„é‡‡æ ·è·¯å¾„ï¼Œåœ¨é«˜ç»´æ•°æ®ä¸Šå…·æœ‰æ›´å¥½çš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼›åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œèƒ½ä»¥æ›´å°‘çš„é‡‡æ ·æ­¥éª¤æå‡ç”Ÿæˆä¿çœŸåº¦ï¼Œå¹¶ä¸”å…¶æœ€ä¼˜è½¨è¿¹å¯æœ‰æ•ˆè’¸é¦ä¸ºä¸€æ­¥ç”Ÿæˆå™¨ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint SchrÃ¶dinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the SchrÃ¶dinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.15396" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.15396.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>