<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-02</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22904</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22904" target="_blank">DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation</a>
      </h3>
      <p class="paper-title-zh">DINO-SAEï¼šç”¨äºé«˜ä¿çœŸå›¾åƒé‡å»ºä¸ç”Ÿæˆçš„DINOçƒå½¢è‡ªç¼–ç å™¨</p>
      <p class="paper-authors">Hun Chang, Byunghee Cha, Jong Chul Ye</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†DINOçƒå½¢è‡ªç¼–ç å™¨ï¼ˆDINO-SAEï¼‰ï¼Œé€šè¿‡è§£è€¦ç‰¹å¾å‘é‡çš„æ–¹å‘ä¸å¹…åº¦ï¼Œæœ‰æ•ˆæ¡¥æ¥äº†è¯­ä¹‰è¡¨ç¤ºä¸åƒç´ çº§é‡å»ºï¼Œæ˜¾è‘—æå‡äº†é‡å»ºä¿çœŸåº¦ï¼›å¹¶é¦–æ¬¡åœ¨çƒå½¢æµå½¢ä¸Šè®­ç»ƒæ‰©æ•£Transformerï¼Œå®ç°äº†é«˜æ•ˆçš„ç”Ÿæˆå»ºæ¨¡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. è®¾è®¡äº†åˆ†å±‚å·ç§¯å—åµŒå…¥æ¨¡å—ï¼Œå¢å¼ºå¯¹å±€éƒ¨ç»“æ„å’Œçº¹ç†çš„ä¿ç•™ï¼›2. æå‡ºä½™å¼¦ç›¸ä¼¼åº¦å¯¹é½ç›®æ ‡ï¼Œåœ¨ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§çš„åŒæ—¶å…è®¸ç‰¹å¾å¹…åº¦çµæ´»å˜åŒ–ä»¥ä¿ç•™ç»†èŠ‚ï¼›3. åŸºäºå¯¹æ¯”å­¦ä¹ åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºæœ¬è´¨ä½äºè¶…çƒé¢çš„è§‚å¯Ÿï¼Œé‡‡ç”¨é»æ›¼æµåŒ¹é…ç›´æ¥åœ¨çƒå½¢æ½œåœ¨æµå½¢ä¸Šè®­ç»ƒæ‰©æ•£Transformerã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNet-1Kä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ˆrFIDä¸º0.37ï¼ŒPSNRä¸º26.2 dBï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†ä¸é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹çš„å¼ºè¯­ä¹‰å¯¹é½ï¼›åŸºäºé»æ›¼æµåŒ¹é…çš„æ‰©æ•£Transformeræ”¶æ•›é«˜æ•ˆï¼Œåœ¨80è½®è®­ç»ƒågFIDè¾¾åˆ°3.47ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22904" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22904.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22837</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22837" target="_blank">NativeTok: Native Visual Tokenization for Improved Image Generation</a>
      </h3>
      <p class="paper-title-zh">NativeTokï¼šç”¨äºæ”¹è¿›å›¾åƒç”Ÿæˆçš„åŸç”Ÿè§†è§‰åˆ†è¯æ–¹æ³•</p>
      <p class="paper-authors">Bin Wu, Mengqi Huang, Weinan Jia, Zhendong Mao</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†åŸç”Ÿè§†è§‰åˆ†è¯ï¼ˆnative visual tokenizationï¼‰çš„æ¦‚å¿µï¼Œé€šè¿‡å¼ºåˆ¶åœ¨åˆ†è¯é˜¶æ®µå¼•å…¥å› æœä¾èµ–å…³ç³»ï¼Œè§£å†³äº†ä¼ ç»ŸVQæ–¹æ³•ä¸­åˆ†è¯ä¸ç”Ÿæˆé˜¶æ®µä¸åŒ¹é…çš„é—®é¢˜ï¼Œä»è€Œæå‡äº†ç”Ÿæˆå›¾åƒçš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºåŸç”Ÿè§†è§‰åˆ†è¯æ€æƒ³ï¼Œè®¾è®¡äº†NativeTokæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰ç”¨äºæ½œåœ¨å›¾åƒå»ºæ¨¡çš„å…ƒå›¾åƒå˜æ¢å™¨ï¼ˆMITï¼‰ï¼›2ï¼‰æ··åˆå› æœä¸“å®¶å˜æ¢å™¨ï¼ˆMoCETï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªè½»é‡çº§ä¸“å®¶å—åŸºäºå…ˆå‰ä»¤ç‰Œå’Œæ½œåœ¨ç‰¹å¾ç”Ÿæˆå•ä¸ªä»¤ç‰Œã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†åˆ†å±‚åŸç”Ÿè®­ç»ƒç­–ç•¥ï¼Œä»…æ›´æ–°æ–°çš„ä¸“å®¶å—ä»¥ä¿è¯è®­ç»ƒæ•ˆç‡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNativeTokåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåµŒå…¥å…³ç³»çº¦æŸåˆ°ä»¤ç‰Œåºåˆ—ä¸­ï¼Œç”Ÿæˆå…·æœ‰æ›´å¼ºä¸€è‡´æ€§å’Œæ›´å°‘åå·®çš„å›¾åƒï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®­ç»ƒæ•ˆç‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22837" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22837.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22680</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22680" target="_blank">Visual Personalization Turing Test</a>
      </h3>
      <p class="paper-title-zh">è§†è§‰ä¸ªæ€§åŒ–å›¾çµæµ‹è¯•</p>
      <p class="paper-authors">Rameen Abdal, James Burgess, Sergey Tulyakov, Kuan-Chieh Jackson Wang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†åŸºäºæ„ŸçŸ¥ä¸å¯åŒºåˆ†æ€§ï¼ˆè€Œéèº«ä»½å¤åˆ¶ï¼‰çš„è§†è§‰ä¸ªæ€§åŒ–è¯„ä¼°æ–°èŒƒå¼â€”â€”è§†è§‰ä¸ªæ€§åŒ–å›¾çµæµ‹è¯•ï¼ˆVPTTï¼‰ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŒ…å«åŸºå‡†ã€ç”Ÿæˆå™¨å’Œè‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡çš„å®Œæ•´æ¡†æ¶ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«1ä¸‡ä¸ªäººç‰©è§’è‰²çš„åŸºå‡†æ•°æ®é›†ï¼ˆVPTT-Benchï¼‰ï¼Œå¼€å‘äº†è§†è§‰æ£€ç´¢å¢å¼ºç”Ÿæˆå™¨ï¼ˆVPRAGï¼‰æ¥ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹ï¼Œå¹¶è®¾è®¡äº†çº¯æ–‡æœ¬è¯„ä¼°æŒ‡æ ‡VPTT Scoreï¼Œè¯¥æŒ‡æ ‡é€šè¿‡äººç±»å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­è¿›è¡Œæ ¡å‡†ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> äººç±»è¯„ä¼°è€…ã€è§†è§‰è¯­è¨€æ¨¡å‹å’ŒVPTT Scoreä¸‰è€…ä¹‹é—´è¡¨ç°å‡ºé«˜åº¦ç›¸å…³æ€§ï¼ŒéªŒè¯äº†VPTT Scoreå¯ä½œä¸ºå¯é çš„æ„ŸçŸ¥ä»£ç†æŒ‡æ ‡ï¼›å®éªŒè¡¨æ˜VPRAGåœ¨å†…å®¹å¯¹é½æ€§å’ŒåŸåˆ›æ€§ä¹‹é—´å–å¾—äº†æœ€ä½³å¹³è¡¡ï¼Œä¸ºå¯æ‰©å±•ä¸”éšç§å®‰å…¨çš„ä¸ªæ€§åŒ–ç”ŸæˆAIæä¾›äº†åŸºç¡€ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22680" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22680.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22630</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22630" target="_blank">LINA: Linear Autoregressive Image Generative Models with Continuous Tokens</a>
      </h3>
      <p class="paper-title-zh">LINAï¼šåŸºäºè¿ç»­ä»¤ç‰Œçš„çº¿æ€§è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹</p>
      <p class="paper-authors">Jiahao Wang, Ting Pan, Haoge Deng, Dongchen Han, Taiqiang Wu ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†LINAï¼Œä¸€ä¸ªå®Œå…¨åŸºäºçº¿æ€§æ³¨æ„åŠ›çš„é«˜æ•ˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„è®¾è®¡é€‰æ‹©ï¼ˆå¦‚å½’ä¸€åŒ–æ–¹å¼å’Œå±€éƒ¨æ€§å¢å¼ºï¼‰æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ç ”ç©¶é¦–å…ˆç³»ç»Ÿåˆ†æäº†çº¿æ€§æ³¨æ„åŠ›ä¸­ä¸åŒè®¾è®¡é€‰æ‹©ï¼ˆåŒ…æ‹¬åŸºäºé™¤æ³•ä¸åŸºäºå‡æ³•çš„å½’ä¸€åŒ–èŒƒå¼ï¼Œä»¥åŠç”¨äºå±€éƒ¨æ€§å¢å¼ºçš„æ·±åº¦å·ç§¯ï¼‰å¯¹æ¨¡å‹ç¼©æ”¾è¡Œä¸ºçš„å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå°†å› æœçº¿æ€§æ³¨æ„åŠ›ä¸­å¸¸ç”¨çš„é—¨æ§æœºåˆ¶æ‰©å±•åˆ°åŒå‘è®¾ç½®ï¼Œæå‡ºäº†KVé—¨æ§ï¼Œé€šè¿‡ä¸ºé”®å’Œå€¼çŠ¶æ€å¼•å…¥å¯å­¦ä¹ çš„å‚æ•°æ¥å®ç°çµæ´»çš„ä»¤ç‰Œçº§è®°å¿†ç®¡ç†ã€‚æœ€ç»ˆåŸºäºè¿™äº›å‘ç°æ„å»ºäº†LINAæ¨¡å‹ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å…³é”®å‘ç°åŒ…æ‹¬ï¼š1) å¯¹äºçº¿æ€§ç”Ÿæˆå¼Transformerï¼ŒåŸºäºé™¤æ³•çš„å½’ä¸€åŒ–æ¯”åŸºäºå‡æ³•çš„å½’ä¸€åŒ–å…·æœ‰æ›´å¥½çš„ç¼©æ”¾æ€§ï¼›2) å¼•å…¥å·ç§¯è¿›è¡Œå±€éƒ¨æ€§å»ºæ¨¡å¯¹è‡ªå›å½’ç”Ÿæˆè‡³å…³é‡è¦ï¼›3) æå‡ºçš„KVé—¨æ§èƒ½æœ‰æ•ˆç®¡ç†è®°å¿†ã€‚LINAåœ¨ImageNetä¸Šå–å¾—äº†2.18çš„FIDåˆ†æ•°ï¼Œåœ¨GenEvalä¸Šè¾¾åˆ°0.74ï¼ŒåŒæ—¶å•ä¸ªçº¿æ€§æ³¨æ„åŠ›æ¨¡å—æ¯”Softmaxæ³¨æ„åŠ›å‡å°‘äº†çº¦61%çš„FLOPsã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation. Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models. We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models. Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: https://github.com/techmonsterwang/LINA.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22630" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22630.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22507</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22507" target="_blank">DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation</a>
      </h3>
      <p class="paper-title-zh">DreamVARï¼šé©¯æœå¼ºåŒ–è§†è§‰è‡ªå›å½’æ¨¡å‹ä»¥å®ç°é«˜ä¿çœŸä¸»ä½“é©±åŠ¨çš„å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Xin Jiang, Jingwen Chen, Yehao Li, Yingwei Pan, Kezhou Chen ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†DreamVARæ¡†æ¶ï¼Œé¦–æ¬¡å°†è§†è§‰è‡ªå›å½’æ¨¡å‹æˆåŠŸåº”ç”¨äºä¸»ä½“é©±åŠ¨çš„å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é€šè¿‡é¢„å¡«å……æ¡ä»¶ç‰¹å¾ä¸å¼ºåŒ–å­¦ä¹ çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è¯­ä¹‰å¯¹é½ä¸ä¸»ä½“ä¸€è‡´æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆä½¿ç”¨è§†è§‰åˆ†è¯å™¨æå–å‚è€ƒä¸»ä½“çš„å¤šå°ºåº¦ç‰¹å¾ï¼›ç„¶åï¼Œåœ¨é¢„æµ‹ç›®æ ‡å›¾åƒåºåˆ—ä¹‹å‰ï¼Œå°†å®Œæ•´çš„æ¡ä»¶ç‰¹å¾åºåˆ—é¢„å¡«å……åˆ°è‡ªå›å½’è¿‡ç¨‹ä¸­ï¼Œç®€åŒ–äº†è·¨å°ºåº¦çš„ä¾èµ–å…³ç³»å¹¶ç¼“è§£äº†è®­ç»ƒ-æµ‹è¯•å·®å¼‚ï¼›æœ€åï¼Œå¼•å…¥å¼ºåŒ–å­¦ä¹ æ¥è”åˆä¼˜åŒ–è¯­ä¹‰å¯¹é½å’Œä¸»ä½“ä¿çœŸåº¦ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDreamVARåœ¨ä¸»ä½“å¤–è§‚ä¿çœŸåº¦æ–¹é¢ä¼˜äºå½“å‰é¢†å…ˆçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜è´¨é‡çš„ä¸»ä½“é©±åŠ¨å›¾åƒç”Ÿæˆã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22507" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22507.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>