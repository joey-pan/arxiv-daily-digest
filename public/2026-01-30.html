<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-30</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | æœºå™¨å­¦ä¹ : 1</p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.21694</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21694" target="_blank">ChartE$^{3}$: A Comprehensive Benchmark for End-to-End Chart Editing</a>
      </h3>
      <p class="paper-title-zh">ChartEÂ³ï¼šä¸€ä¸ªç”¨äºç«¯åˆ°ç«¯å›¾è¡¨ç¼–è¾‘çš„ç»¼åˆåŸºå‡†</p>
      <p class="paper-authors">Shuo Li, Jiajun Sun, Zhekai Wang, Xiaoran Fan, Hui Li ç­‰ (12 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªä¸ä¾èµ–ä¸­é—´è‡ªç„¶è¯­è¨€æˆ–ä»£ç è¡¨ç¤ºçš„ç«¯åˆ°ç«¯å›¾è¡¨ç¼–è¾‘åŸºå‡†ChartEÂ³ï¼Œç”¨äºç›´æ¥è¯„ä¼°æ¨¡å‹æ ¹æ®ç”¨æˆ·æ„å›¾æ‰§è¡Œç»†ç²’åº¦ä¸å…¨å±€ç»“æ„ç¼–è¾‘çš„èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥ç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåŒ…å«å±€éƒ¨ç¼–è¾‘ï¼ˆå¦‚å­—ä½“ã€é¢œè‰²è°ƒæ•´ï¼‰å’Œå…¨å±€ç¼–è¾‘ï¼ˆå¦‚æ•°æ®ç­›é€‰ã€è¶‹åŠ¿çº¿æ·»åŠ ï¼‰ä¸¤ä¸ªç»´åº¦çš„è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®æµæ°´çº¿ç»“åˆäººå·¥æ ¡éªŒï¼Œæ„å»ºäº†è¶…è¿‡1200ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«å›¾è¡¨å›¾åƒã€åº•å±‚ä»£ç å’Œå¤šæ¨¡æ€ç¼–è¾‘æŒ‡ä»¤ä¸‰å…ƒç»„ï¼Œæ”¯æŒä»å®¢è§‚ä¸ä¸»è§‚è§’åº¦è¿›è¡Œè¯„ä¼°ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¯¹å½“å‰å…ˆè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯„æµ‹è¡¨æ˜ï¼Œå®ƒä»¬åœ¨ç«¯åˆ°ç«¯å›¾è¡¨ç¼–è¾‘ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå°¤å…¶åœ¨éœ€è¦æ•´ä½“æ•°æ®è½¬æ¢çš„å…¨å±€ç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚å›¾è¡¨ç¼–è¾‘èƒ½åŠ›ä¸Šçš„å…³é”®å±€é™ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Charts are a fundamental visualization format for structured data analysis. Enabling end-to-end chart editing according to user intent is of great practical value, yet remains challenging due to the need for both fine-grained control and global structural consistency. Most existing approaches adopt pipeline-based designs, where natural language or code serves as an intermediate representation, limiting their ability to faithfully execute complex edits. We introduce ChartE$^{3}$, an End-to-End Chart Editing benchmark that directly evaluates models without relying on intermediate natural language programs or code-level supervision. ChartE$^{3}$ focuses on two complementary editing dimensions: local editing, which involves fine-grained appearance changes such as font or color adjustments, and global editing, which requires holistic, data-centric transformations including data filtering and trend line addition. ChartE$^{3}$ contains over 1,200 high-quality samples constructed via a well-designed data pipeline with human curation. Each sample is provided as a triplet of a chart image, its underlying code, and a multimodal editing instruction, enabling evaluation from both objective and subjective perspectives. Extensive benchmarking of state-of-the-art multimodal large language models reveals substantial performance gaps, particularly on global editing tasks, highlighting critical limitations in current end-to-end chart editing capabilities.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21694" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21694.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.21633</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21633" target="_blank">A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion</a>
      </h3>
      <p class="paper-title-zh">å€¾æ–œçš„è··è··æ¿ï¼šé‡æ–°å®¡è§†å¯æ§æ‰©æ•£ä¸­è‡ªç¼–ç å™¨çš„æƒè¡¡é—®é¢˜</p>
      <p class="paper-authors">Pu Cao, Yiyang Ma, Feng Zhou, Xuedan Yin, Qing Song ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æ­ç¤ºäº†å½“å‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­è‡ªç¼–ç å™¨è¯„ä¼°å­˜åœ¨ç³»ç»Ÿæ€§åå·®â€”â€”è¿‡åº¦åå‘ç”Ÿæˆå‹å¥½æ€§æŒ‡æ ‡ï¼ˆå¦‚gFIDï¼‰ï¼Œè€Œå¿½è§†äº†é‡å»ºä¿çœŸåº¦ï¼Œå¹¶è®ºè¯äº†è¿™ç§åå·®ä¼šæŸå®³å¯æ§ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ä½œè€…é¦–å…ˆé€šè¿‡ç†è®ºåˆ†ææŒ‡å‡ºï¼Œåœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒgFIDä¸»å¯¼çš„åå¥½çœ‹ä¼¼æ— å®³ï¼Œä½†åœ¨å¯æ§æ‰©æ•£åœºæ™¯ä¸‹ä¼šå¯¼è‡´æ¡ä»¶æ¼‚ç§»é—®é¢˜ã€‚éšåï¼Œä»–ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šç»´åº¦çš„æ¡ä»¶æ¼‚ç§»è¯„ä¼°åè®®ï¼Œç”¨äºåæ˜ å¯æ§ç”Ÿæˆä»»åŠ¡çš„éœ€æ±‚ï¼Œå¹¶ä»¥æ­¤å®è¯ç ”ç©¶äº†å¤šä¸ªè¿‘æœŸImageNetè‡ªç¼–ç å™¨ã€‚æœ€åï¼Œé€šè¿‡ControlNetå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å¯æ§æ€§ä¸æ¡ä»¶ä¿æŒèƒ½åŠ›çš„å…³ç³»ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å…³é”®å‘ç°åŒ…æ‹¬ï¼š1ï¼‰gFIDå¯¹æ¡ä»¶ä¿æŒèƒ½åŠ›çš„é¢„æµ‹æ€§å¾ˆå¼±ï¼›2ï¼‰é¢å‘é‡å»ºçš„æŒ‡æ ‡ï¼ˆå°¤å…¶æ˜¯å®ä¾‹çº§åº¦é‡ï¼‰ä¸å¯æ§æ€§æ›´ä¸ºä¸€è‡´ï¼›3ï¼‰å¯æ§æ€§ä¸»è¦è·Ÿè¸ªæ¡ä»¶ä¿æŒèƒ½åŠ›ï¼Œè€ŒégFIDã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä»¥ImageNetä¸ºä¸­å¿ƒçš„AEè¯„ä¼°ä¸å¯æ‰©å±•å¯æ§æ‰©æ•£çš„éœ€æ±‚ä¹‹é—´å­˜åœ¨å·®è·ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>In latent diffusion models, the autoencoder (AE) is typically expected to balance two capabilities: faithful reconstruction and a generation-friendly latent space (e.g., low gFID). In recent ImageNet-scale AE studies, we observe a systematic bias toward generative metrics in handling this trade-off: reconstruction metrics are increasingly under-reported, and ablation-based AE selection often favors the best-gFID configuration even when reconstruction fidelity degrades. We theoretically analyze why this gFID-dominant preference can appear unproblematic for ImageNet generation, yet becomes risky when scaling to controllable diffusion: AEs can induce condition drift, which limits achievable condition alignment. Meanwhile, we find that reconstruction fidelity, especially instance-level measures, better indicates controllability. We empirically validate the impact of tilted autoencoder evaluation on controllability by studying several recent ImageNet AEs. Using a multi-dimensional condition-drift evaluation protocol reflecting controllable generation tasks, we find that gFID is only weakly predictive of condition preservation, whereas reconstruction-oriented metrics are substantially more aligned. ControlNet experiments further confirm that controllability tracks condition preservation rather than gFID. Overall, our results expose a gap between ImageNet-centric AE evaluation and the requirements of scalable controllable diffusion, offering practical guidance for more reliable benchmarking and model selection.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21633" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21633.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.21542</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21542" target="_blank">Bi-Anchor Interpolation Solver for Accelerating Generative Modeling</a>
      </h3>
      <p class="paper-title-zh">ç”¨äºåŠ é€Ÿç”Ÿæˆå»ºæ¨¡çš„åŒé”šç‚¹æ’å€¼æ±‚è§£å™¨</p>
      <p class="paper-authors">Hongxu Chen, Hongxiang Li, Zhen Wang, Long Chen</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒä¸»å¹²æ¨¡å‹çš„è½»é‡çº§æ±‚è§£å™¨ï¼ˆBA-solverï¼‰ï¼Œåœ¨æä½è®­ç»ƒæˆæœ¬ä¸‹æ˜¾è‘—åŠ é€ŸæµåŒ¹é…æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒé«˜ç”Ÿæˆè´¨é‡ä¸å³æ’å³ç”¨çš„é€šç”¨æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å¼•å…¥ä¸€ä¸ªä»…ä¸ºä¸»å¹²ç½‘ç»œ1-2%å¤§å°çš„è½»é‡çº§SideNetï¼Œå¹¶ä¿æŒä¸»å¹²ç½‘ç»œå†»ç»“ã€‚å…¶æ ¸å¿ƒåŒ…å«ä¸¤ä¸ªååŒç»„ä»¶ï¼š1ï¼‰åŒå‘æ—¶é—´æ„ŸçŸ¥ï¼Œä½¿SideNetèƒ½å¤ŸåŒæ—¶å­¦ä¹ è¿‘ä¼¼æœªæ¥å’Œå†å²çš„æµé€Ÿï¼›2ï¼‰åŒé”šç‚¹é€Ÿåº¦ç§¯åˆ†ï¼Œåˆ©ç”¨SideNetå’Œä¸¤ä¸ªé”šç‚¹é€Ÿåº¦é«˜æ•ˆè¿‘ä¼¼æ‰¹å¤„ç†é«˜é˜¶ç§¯åˆ†ä¸­çš„ä¸­é—´é€Ÿåº¦ï¼Œé€šè¿‡ä¸»å¹²ç½‘ç»œæä¾›é«˜ç²¾åº¦é”šç‚¹ï¼ŒSideNetå¯¹è½¨è¿¹è¿›è¡Œç¨ å¯†åŒ–ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNet-256Â²ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBA-solverä»…ç”¨10æ­¥ç¥ç»å‡½æ•°è¯„ä¼°ï¼ˆNFEï¼‰å³å¯è¾¾åˆ°ä¸100+æ­¥æ¬§æ‹‰æ±‚è§£å™¨ç›¸å½“çš„ç”Ÿæˆè´¨é‡ï¼Œåœ¨ä½è‡³5æ­¥æ—¶ä»èƒ½ä¿æŒé«˜ä¿çœŸåº¦ï¼Œä¸”è®­ç»ƒæˆæœ¬å¯å¿½ç•¥ä¸è®¡ã€‚è¯¥æ±‚è§£å™¨èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰ç”Ÿæˆæµç¨‹ä¸­ï¼Œæ”¯æŒå›¾åƒç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors'' and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21542" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21542.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.21498</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21498" target="_blank">SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing</a>
      </h3>
      <p class="paper-title-zh">SimGraphï¼šä¸€ä¸ªåŸºäºåœºæ™¯å›¾è¿›è¡Œå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„ç»Ÿä¸€æ¡†æ¶</p>
      <p class="paper-authors">Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶SimGraphï¼Œå°†åŸºäºåœºæ™¯å›¾çš„å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡æ•´åˆåœ¨ä¸€èµ·ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å°†ä¸¤è€…åˆ†ç¦»å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹ã€ç©ºé—´ä¸€è‡´æ€§ä¸è¯­ä¹‰è¿è´¯æ€§ä¸è¶³çš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ¡†æ¶é‡‡ç”¨åœºæ™¯å›¾ä½œä¸ºç»“æ„åŒ–è¡¨ç¤ºï¼Œä»¥ç²¾ç¡®æ§åˆ¶å¯¹è±¡å…³ç³»ä¸ç©ºé—´å¸ƒå±€ã€‚å®ƒåœ¨ä¸€ä¸ªç»Ÿä¸€çš„åœºæ™¯å›¾é©±åŠ¨æ¨¡å‹ä¸­ï¼Œé›†æˆäº†åŸºäºä»¤ç‰Œçš„å›¾åƒç”Ÿæˆæ–¹æ³•å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆä¸ç¼–è¾‘ç»“æœçš„é«˜è´¨é‡ä¸ä¸€è‡´æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒSimGraphåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç»´æŒå¯¹è±¡é—´çš„äº¤äº’å…³ç³»ã€å¸ƒå±€ç»“æ„å’Œç©ºé—´è¿è´¯æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21498" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21498.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">æœºå™¨å­¦ä¹ </span>
        <span class="paper-id">2601.21419</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21419" target="_blank">Revisiting Diffusion Model Predictions Through Dimensionality</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡ç»´åº¦è§†è§’é‡æ–°å®¡è§†æ‰©æ•£æ¨¡å‹çš„é¢„æµ‹ç›®æ ‡</p>
      <p class="paper-authors">Qing Jin, Chaoyang Wang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œé˜æ˜äº†æ•°æ®çš„å†…åœ¨ç»´åº¦å¦‚ä½•å†³å®šæ‰©æ•£æ¨¡å‹çš„æœ€ä¼˜é¢„æµ‹ç›®æ ‡ï¼ˆå¦‚å™ªå£°ã€é€Ÿåº¦æˆ–æ•°æ®æœ¬èº«ï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ— éœ€æ˜¾å¼ä¼°è®¡ç»´åº¦çš„æ•°æ®é©±åŠ¨æ–¹æ³•k-Diffï¼Œä»¥è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜é¢„æµ‹å‚æ•°ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ä½œè€…é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªå¹¿ä¹‰é¢„æµ‹å…¬å¼ï¼Œå°†Îµé¢„æµ‹ã€vé¢„æµ‹å’Œxé¢„æµ‹ç»Ÿä¸€ä¸ºç‰¹ä¾‹ã€‚ç„¶åï¼Œç†è®ºæ¨å¯¼äº†æ•°æ®å‡ ä½•ï¼ˆç‰¹åˆ«æ˜¯ç¯å¢ƒç»´åº¦ä¸å†…åœ¨ç»´åº¦çš„å…³ç³»ï¼‰ä¸æœ€ä¼˜é¢„æµ‹ç›®æ ‡ä¹‹é—´çš„è§£æå…³ç³»ã€‚ä¸ºè§£å†³å†…åœ¨ç»´åº¦éš¾ä»¥ä¼°è®¡çš„é—®é¢˜ï¼Œæå‡ºäº†k-Diffæ¡†æ¶ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹å¼ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ æœ€ä¼˜çš„é¢„æµ‹å‚æ•°kï¼Œç»•è¿‡äº†æ˜¾å¼çš„ç»´åº¦ä¼°è®¡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ç†è®ºåˆ†æè¡¨æ˜ï¼Œå½“æ•°æ®çš„ç¯ç»•ç»´åº¦è¿œé«˜äºå…¶å†…åœ¨ç»´åº¦æ—¶ï¼Œç›´æ¥æ•°æ®é¢„æµ‹ï¼ˆx-predictionï¼‰æˆä¸ºæœ€ä¼˜é€‰æ‹©ã€‚åœ¨æ½œç©ºé—´å’Œåƒç´ ç©ºé—´çš„å›¾åƒç”Ÿæˆå®éªŒä¸­ï¼Œk-Diffæ¡†æ¶åœ¨ä¸åŒæ¶æ„å’Œæ•°æ®è§„æ¨¡ä¸‹å‡ç¨³å®šä¼˜äºå›ºå®šé¢„æµ‹ç›®æ ‡çš„åŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶ä½œä¸ºæå‡ç”Ÿæˆæ€§èƒ½çš„åŸåˆ™æ€§è‡ªåŠ¨åŒ–æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21419" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21419.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>