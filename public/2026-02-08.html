<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-08</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | å›¾å½¢å­¦: 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05998</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05998" target="_blank">VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation</a>
      </h3>
      <p class="paper-title-zh">VisRefinerï¼šä»è§†è§‰å·®å¼‚ä¸­å­¦ä¹ ä»¥è¿›è¡Œæˆªå›¾åˆ°ä»£ç ç”Ÿæˆ</p>
      <p class="paper-authors">Jie Deng, Kaichun Yao, Libo Zhang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†VisRefinerè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è®©æ¨¡å‹å­¦ä¹ æ¸²æŸ“é¢„æµ‹ç»“æœä¸å‚è€ƒè®¾è®¡ä¹‹é—´çš„è§†è§‰å·®å¼‚æ¥æ”¹è¿›æˆªå›¾åˆ°ä»£ç çš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶èµ‹äºˆæ¨¡å‹å¼ºå¤§çš„è‡ªæˆ‘ä¼˜åŒ–èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. æ„å»ºå·®å¼‚å¯¹é½ç›‘ç£ï¼Œå°†è§†è§‰å·®å¼‚ä¸å¯¹åº”çš„ä»£ç ç¼–è¾‘å…³è”èµ·æ¥ï¼Œä½¿æ¨¡å‹ç†è§£å¤–è§‚å˜åŒ–å¦‚ä½•ç”±ä»£ç ä¿®æ”¹å¼•èµ·ã€‚2. å¼•å…¥å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ï¼Œæ¨¡å‹é€šè¿‡è§‚å¯Ÿæ¸²æŸ“è¾“å‡ºä¸ç›®æ ‡è®¾è®¡çš„è§†è§‰å·®å¼‚ï¼Œè¯†åˆ«å¹¶æ›´æ–°ä»£ç ä»¥å®ç°è¿­ä»£æ”¹è¿›ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒVisRefineræ˜¾è‘—æé«˜äº†å•æ­¥ç”Ÿæˆçš„ä»£ç è´¨é‡å’Œå¸ƒå±€ä¿çœŸåº¦ï¼ŒåŒæ—¶ä½¿æ¨¡å‹å…·å¤‡äº†å¼ºå¤§çš„è‡ªæˆ‘ä¼˜åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†ä»è§†è§‰å·®å¼‚ä¸­å­¦ä¹ å¯¹æ¨è¿›æˆªå›¾åˆ°ä»£ç ç”Ÿæˆä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05998" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05998.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05951</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05951" target="_blank">Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</a>
      </h3>
      <p class="paper-title-zh">æ›´å¥½çš„æºåˆ†å¸ƒï¼Œæ›´å¥½çš„æµï¼šå­¦ä¹ æ¡ä»¶ä¾èµ–çš„æºåˆ†å¸ƒä»¥è¿›è¡ŒæµåŒ¹é…</p>
      <p class="paper-authors">Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºåœ¨æµåŒ¹é…æ¡†æ¶ä¸­å­¦ä¹ ä¸€ä¸ªæ¡ä»¶ä¾èµ–çš„æºåˆ†å¸ƒï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸°å¯Œçš„æ¡ä»¶ä¿¡å·ï¼ˆå¦‚æ–‡æœ¬ï¼‰ï¼Œä»è€Œæå‡æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ä¸ç¨³å®šæ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºæµåŒ¹é…æ¡†æ¶ï¼Œå°†ä¼ ç»Ÿçš„å›ºå®šé«˜æ–¯æºåˆ†å¸ƒæ›¿æ¢ä¸ºå¯å­¦ä¹ çš„ã€ä¾èµ–äºæ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬ï¼‰çš„æºåˆ†å¸ƒã€‚é€šè¿‡å¼•å…¥æ–¹å·®æ­£åˆ™åŒ–å’Œç¡®ä¿æºåˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„æ–¹å‘å¯¹é½ï¼Œè§£å†³äº†ç›´æ¥å¼•å…¥æ¡ä»¶æ—¶å¯èƒ½å‡ºç°çš„åˆ†å¸ƒåç¼©å’Œä¸ç¨³å®šé—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†ç›®æ ‡è¡¨ç¤ºç©ºé—´çš„é€‰æ‹©å¦‚ä½•å½±å“ç»“æ„åŒ–æºåˆ†å¸ƒçš„æœ‰æ•ˆæ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡ä»¶ä¾èµ–æºåˆ†å¸ƒè®¾è®¡åœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒç”ŸæˆåŸºå‡†ä¸Šå¸¦æ¥äº†æŒç»­ä¸”ç¨³å¥çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬å°†FIDæŒ‡æ ‡çš„æ”¶æ•›é€Ÿåº¦æœ€é«˜æå‡3å€ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†æ–¹å·®æ­£åˆ™åŒ–å’Œæ–¹å‘å¯¹é½å¯¹äºç¨³å®šå­¦ä¹ è‡³å…³é‡è¦ï¼Œå¹¶æ˜ç¡®äº†æ­¤ç±»è®¾è®¡åœ¨ç‰¹å®šç›®æ ‡è¡¨ç¤ºç©ºé—´ä¸‹æœ€ä¸ºæœ‰æ•ˆã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05951" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05951.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05339</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05339" target="_blank">Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡ä¸å®‰å…¨-å®‰å…¨é…å¯¹ä¸æ–¹å‘æ€§FisheråŠ æƒé€‚é…å®ç°ä¸€è‡´æ€§ä¿æŒçš„æ¦‚å¿µæ“¦é™¤</p>
      <p class="paper-authors">Yongwoo Kim, Sungmin Cha, Hyunsoo Kim, Jaewon Lee, Donghyun Kim</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†PAIRæ¡†æ¶ï¼Œå°†æ¦‚å¿µæ“¦é™¤ä»ç®€å•çš„ç§»é™¤é‡æ„ä¸ºåŸºäºä¸å®‰å…¨-å®‰å…¨é…å¯¹çš„ä¸€è‡´æ€§ä¿æŒè¯­ä¹‰é‡å¯¹é½ï¼Œå®ç°äº†åœ¨ç§»é™¤ä¸è‰¯æ¦‚å¿µçš„åŒæ—¶ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆè¯­ä¹‰ä¸€è‡´çš„å®‰å…¨æ›¿ä»£å†…å®¹ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é¦–å…ˆé€šè¿‡ä¿æŒç»“æ„å’Œè¯­ä¹‰ä¿çœŸåº¦ï¼Œä¸ºä¸å®‰å…¨è¾“å…¥ç”Ÿæˆå¯¹åº”çš„å®‰å…¨æ ·æœ¬ï¼Œæ„å»ºé…å¯¹çš„å¤šæ¨¡æ€æ•°æ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸€æ˜¯â€œé…å¯¹è¯­ä¹‰é‡å¯¹é½â€ç›®æ ‡ï¼Œåˆ©ç”¨é…å¯¹æ•°æ®å°†ç›®æ ‡æ¦‚å¿µæ˜¾å¼æ˜ å°„åˆ°è¯­ä¹‰å¯¹é½çš„å®‰å…¨é”šç‚¹ï¼›äºŒæ˜¯ä¸ºDoRAåˆå§‹åŒ–FisheråŠ æƒçš„å‚æ•°é«˜æ•ˆä½ç§©é€‚é…çŸ©é˜µï¼Œé¼“åŠ±ç”Ÿæˆå®‰å…¨æ›¿ä»£å†…å®¹å¹¶é€‰æ‹©æ€§æŠ‘åˆ¶ä¸å®‰å…¨æ¦‚å¿µã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¦‚å¿µæ“¦é™¤æ•ˆæœä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›åŸºçº¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç§»é™¤ç›®æ ‡æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆå›¾åƒçš„ç»“æ„å®Œæ•´æ€§ã€è¯­ä¹‰è¿è´¯æ€§å’Œæ•´ä½“ç”Ÿæˆè´¨é‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05339" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05339.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05305</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05305" target="_blank">FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion</a>
      </h3>
      <p class="paper-title-zh">FlashBlockï¼šç”¨äºé«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å—æ‰©æ•£çš„æ³¨æ„åŠ›ç¼“å­˜æœºåˆ¶</p>
      <p class="paper-authors">Zhuokun Chen, Jianfei Cai, Bohan Zhuang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†FlashBlockï¼Œä¸€ç§ç”¨äºå—æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›ç¼“å­˜æœºåˆ¶ï¼Œé€šè¿‡é‡ç”¨å—é—´ç¨³å®šçš„æ³¨æ„åŠ›è¾“å‡ºæ¥æ˜¾è‘—å‡å°‘é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä¸­çš„è®¡ç®—å¼€é”€å’ŒKVç¼“å­˜è®¿é—®ï¼ŒåŒæ—¶å¯ä¸ç¨€ç–æ³¨æ„åŠ›æ­£äº¤ç»“åˆä»¥æå‡æ¨¡å‹ç²¾åº¦ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºå¯¹å—æ‰©æ•£ä¸­æ³¨æ„åŠ›å†—ä½™çš„åˆ†æï¼šå‘ç°å½“å‰å—å¤–éƒ¨çš„tokenæ³¨æ„åŠ›è¾“å‡ºåœ¨æ‰©æ•£æ­¥éª¤é—´ä¿æŒç¨³å®šï¼Œè€Œå—å†…éƒ¨æ³¨æ„åŠ›å˜åŒ–æ˜¾è‘—ã€‚FlashBlocké€šè¿‡ç¼“å­˜å¹¶é‡ç”¨è¿™äº›ç¨³å®šçš„å—å¤–éƒ¨æ³¨æ„åŠ›è¾“å‡ºï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œä¸”æ— éœ€ä¿®æ”¹åŸæœ‰æ‰©æ•£è¿‡ç¨‹ã€‚è¯¥æœºåˆ¶è¿˜å¯ä¸ç¨€ç–æ³¨æ„åŠ›ç»“åˆï¼Œä½œä¸ºè¡¥å……çš„æ®‹å·®é‡ç”¨ç­–ç•¥ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒåœ¨æ‰©æ•£è¯­è¨€æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºï¼šFlashBlockæœ€é«˜å¯æå‡1.44å€çš„tokenååé‡ï¼Œå‡å°‘é«˜è¾¾1.6å€çš„æ³¨æ„åŠ›è®¡ç®—æ—¶é—´ï¼Œä¸”å¯¹ç”Ÿæˆè´¨é‡å½±å“å¯å¿½ç•¥ï¼›ä¸ç¨€ç–æ³¨æ„åŠ›ç»“åˆåï¼Œèƒ½æ˜¾è‘—æå‡åœ¨é«˜ç¨€ç–åŒ–æ¡ä»¶ä¸‹çš„æ¨¡å‹ç²¾åº¦ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05305" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05305.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">å›¾å½¢å­¦</span>
          <span class="paper-id">2602.05013</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05013" target="_blank">Untwisting RoPE: Frequency Control for Shared Attention in DiTs</a>
      </h3>
      <p class="paper-title-zh">è§£å¼€RoPEï¼šDiTsä¸­å…±äº«æ³¨æ„åŠ›çš„é¢‘ç‡æ§åˆ¶</p>
      <p class="paper-authors">Aryan Mikaeili, Or Patashnik, Andrea Tagliasacchi, Daniel Cohen-Or, Ali Mahdavi-Amiri</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æ­ç¤ºäº†RoPEä½ç½®ç¼–ç çš„é¢‘ç‡ç»“æ„æ˜¯å¯¼è‡´å…±äº«æ³¨æ„åŠ›æœºåˆ¶ä¸­å‘ç”Ÿâ€œå‚è€ƒå¤åˆ¶â€é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šè¿‡é€‰æ‹©æ€§è°ƒåˆ¶RoPEé¢‘æ®µæ¥æ§åˆ¶é£æ ¼è¿ç§»ä¸å†…å®¹å¤åˆ¶ç¨‹åº¦çš„æ–¹æ³•ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è®ºæ–‡é¦–å…ˆå¯¹RoPEè¿›è¡Œäº†åŸç†æ€§åˆ†æï¼Œå°†å…¶åˆ†è§£ä¸ºå…·æœ‰ä¸åŒä½ç½®æ•æ„Ÿæ€§çš„é¢‘ç‡åˆ†é‡ã€‚åŸºäºåˆ†æå‘ç°çš„é«˜é¢‘åˆ†é‡ä¸»å¯¼æ³¨æ„åŠ›è®¡ç®—çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§è°ƒåˆ¶ï¼ˆæŠ‘åˆ¶æˆ–å¢å¼ºï¼‰RoPEçš„ç‰¹å®šé¢‘ç‡æ³¢æ®µï¼Œä½¿æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿåæ˜ è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè€Œéä¸¥æ ¼çš„ä½ç½®å¯¹é½ã€‚è¯¥æ–¹æ³•è¢«åº”ç”¨äºæ‰€æœ‰ä»¤ç‰Œå…±äº«æ³¨æ„åŠ›çš„ç°ä»£åŸºäºTransformerçš„æ‰©æ•£æ¨¡å‹æ¶æ„ä¸­ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å…³é”®å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é¢‘ç‡è°ƒåˆ¶æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç¨³å®šå…±äº«æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶ä»æ„å¤–çš„å†…å®¹å¤åˆ¶è¡Œä¸ºè½¬å˜ä¸ºæœ‰æ„ä¹‰çš„é£æ ¼å¯¹é½ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•å®ç°äº†å¯¹é£æ ¼è¿ç§»ä¸å†…å®¹å¤åˆ¶ç¨‹åº¦çš„ç²¾ç»†æ§åˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸å¤åˆ¶å‚è€ƒå›¾åƒå†…å®¹çš„å‰æä¸‹ï¼ŒæˆåŠŸæå–å¹¶è¿ç§»å…¶é£æ ¼å±æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Positional encodings are essential to transformer-based generative models, yet their behavior in multimodal and attention-sharing settings is not fully understood. In this work, we present a principled analysis of Rotary Positional Embeddings (RoPE), showing that RoPE naturally decomposes into frequency components with distinct positional sensitivities. We demonstrate that this frequency structure explains why shared-attention mechanisms, where a target image is generated while attending to tokens from a reference image, can lead to reference copying, in which the model reproduces content from the reference instead of extracting only its stylistic cues. Our analysis reveals that the high-frequency components of RoPE dominate the attention computation, forcing queries to attend mainly to spatially aligned reference tokens and thereby inducing this unintended copying behavior. Building on these insights, we introduce a method for selectively modulating RoPE frequency bands so that attention reflects semantic similarity rather than strict positional alignment. Applied to modern transformer-based diffusion architectures, where all tokens share attention, this modulation restores stable and meaningful shared attention. As a result, it enables effective control over the degree of style transfer versus content copying, yielding a proper style-aligned generation process in which stylistic attributes are transferred without duplicating reference content.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05013" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05013.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>