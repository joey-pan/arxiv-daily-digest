<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-01</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | cs.DC: 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.21857</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21857" target="_blank">Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents</a>
      </h3>
      <p class="paper-title-zh">åŸºäºè½¨è¿¹å¼•å¯¼æ‰©æ•£çš„å¤šå±‚æ–‡æ¡£å‰æ™¯ä¿æŒèƒŒæ™¯ç”Ÿæˆ</p>
      <p class="paper-authors">Taewon Kang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è®¾è®¡åˆå§‹å™ªå£°çš„å‡ ä½•å¯¹é½å’Œç¼“å­˜çš„é£æ ¼æ–¹å‘ï¼Œå®ç°äº†æ–‡æ¡£å‰æ™¯å†…å®¹çš„è‡ªç„¶ä¿æŒä¸å¤šé¡µé¢é£æ ¼ä¸€è‡´æ€§ï¼Œæ— éœ€ä¾èµ–æ˜¾å¼çº¦æŸæˆ–æ©ç å¯å‘å¼æ–¹æ³•ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è§£é‡Šä¸ºåœ¨ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ä¸­éšæœºè½¨è¿¹çš„æ¼”åŒ–ã€‚é€šè¿‡å¡‘é€ åˆå§‹å™ªå£°åŠå…¶å‡ ä½•å¯¹é½ï¼Œä½¿èƒŒæ™¯ç”Ÿæˆè‡ªç„¶åœ°é¿å¼€æŒ‡å®šå‰æ™¯åŒºåŸŸï¼›åŒæ—¶ï¼Œå°†é£æ ¼æ§åˆ¶ä¸æ–‡æœ¬æ¡ä»¶è§£è€¦ï¼Œå¼•å…¥ç¼“å­˜çš„é£æ ¼æ–¹å‘ä½œä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„æŒä¹…å‘é‡ï¼Œçº¦æŸæ‰©æ•£è½¨è¿¹å…±äº«åŒä¸€é£æ ¼å­ç©ºé—´ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œä¸ç°æœ‰æ‰©æ•£ä¸»å¹²å…¼å®¹ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚æ–‡æ¡£ä¸­ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ä¿æŒå‰æ™¯å¯è¯»æ€§çš„èƒŒæ™¯ï¼›é€šè¿‡è½¨è¿¹åˆå§‹åŒ–è€Œéæ˜¾å¼æ’é™¤ï¼Œä½¿æ‰©æ•£è·¯å¾„å¾ˆå°‘ç©¿è¶Šå‰æ™¯åŒºåŸŸï¼›ç¼“å­˜çš„é£æ ¼æ–¹å‘æœ‰æ•ˆè§£å†³äº†å¤šé¡µé¢ç”Ÿæˆä¸­çš„é£æ ¼æ¼‚ç§»é—®é¢˜ï¼Œæ— éœ€é‡å¤åŸºäºæç¤ºçš„é£æ ¼æŒ‡å®šã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We present a diffusion-based framework for document-centric background generation that achieves foreground preservation and multi-page stylistic consistency through latent-space design rather than explicit constraints. Instead of suppressing diffusion updates or applying masking heuristics, our approach reinterprets diffusion as the evolution of stochastic trajectories through a structured latent space. By shaping the initial noise and its geometric alignment, background generation naturally avoids designated foreground regions, allowing readable content to remain intact without auxiliary mechanisms. To address the long-standing issue of stylistic drift across pages, we decouple style control from text conditioning and introduce cached style directions as persistent vectors in latent space. Once selected, these directions constrain diffusion trajectories to a shared stylistic subspace, ensuring consistent appearance across pages and editing iterations. This formulation eliminates the need for repeated prompt-based style specification and provides a more stable foundation for multi-page generation. Our framework admits a geometric and physical interpretation, where diffusion paths evolve on a latent manifold shaped by preferred directions, and foreground regions are rarely traversed as a consequence of trajectory initialization rather than explicit exclusion. The proposed method is training-free, compatible with existing diffusion backbones, and produces visually coherent, foreground-preserving results across complex documents. By reframing diffusion as trajectory design in latent space, we offer a principled approach to consistent and structured generative modeling.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21857" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21857.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.21081</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21081" target="_blank">Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought</a>
      </h3>
      <p class="paper-title-zh">æ€ç»´ä¹‹å½¢ï¼šé€šè¿‡è§†è§‰æ€ç»´é“¾è¿›è¡Œæ¸è¿›å¼ç‰©ä½“ç»„è£…</p>
      <p class="paper-authors">Yu Huo, Siyu Zhang, Kun Zeng, Haoyue Liu, Owen Lee ç­‰ (10 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†Shape-of-Thoughtï¼ˆSoTï¼‰è§†è§‰æ€ç»´é“¾æ¡†æ¶ï¼Œé€šè¿‡æ¸è¿›å¼2DæŠ•å½±å®ç°ç»“æ„åŒ–ç‰©ä½“ç»„è£…ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ¨ç†å¼•æ“ï¼›åŒæ—¶æ„å»ºäº†å¤§è§„æ¨¡ç»„è£…è½¨è¿¹æ•°æ®é›†SoT-26Kå’Œè¯„ä¼°åŸºå‡†T2S-CompBenchã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. è®­ç»ƒä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œç”Ÿæˆäº¤é”™çš„æ–‡æœ¬è®¡åˆ’å’Œæ¸²æŸ“çš„ä¸­é—´çŠ¶æ€ã€‚
2. é€šè¿‡è§†è§‰æ€ç»´é“¾æ•è·å½¢çŠ¶ç»„è£…é€»è¾‘ï¼Œæ— éœ€æ˜¾å¼å‡ ä½•è¡¨ç¤ºã€‚
3. åˆ©ç”¨ä»åŸºäºéƒ¨ä»¶çš„CADå±‚æ¬¡ç»“æ„è¡ç”Ÿçš„ç»„è£…è½¨è¿¹æ•°æ®è¿›è¡Œè®­ç»ƒã€‚
4. åœ¨æ¨ç†æ—¶ä»…é€šè¿‡è¿è´¯çš„2DæŠ•å½±åºåˆ—å®ç°æ¸è¿›å¼ç»„è£…ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> 1. åœ¨SoT-26Kä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨éƒ¨ä»¶æ•°é‡å‡†ç¡®æ€§ä¸Šè¾¾åˆ°88.4%ï¼Œåœ¨ç»“æ„æ‹“æ‰‘å‡†ç¡®æ€§ä¸Šè¾¾åˆ°84.8%ã€‚
2. ç›¸æ¯”çº¯æ–‡æœ¬åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡çº¦20%ã€‚
3. è¯¥æ¡†æ¶ä¸ºé€æ˜ã€è¿‡ç¨‹ç›‘ç£çš„ç»„åˆç”Ÿæˆå»ºç«‹äº†æ–°èŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ¨¡å‹çš„ç»„åˆç»“æ„çº¦æŸèƒ½åŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21081" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21081.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.20911</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.20911" target="_blank">Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs</a>
      </h3>
      <p class="paper-title-zh">åŸºäºå†å²æ¡ä»¶åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„éé©¬å°”å¯å¤«å¤šè½®å¯¹è¯å¼å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Haochen Zhang, Animesh Sinha, Felix Juefei-Xu, Haoyu Ma, Kunpeng Li ç­‰ (11 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æ­£å¼å®šä¹‰å¹¶é’ˆå¯¹éé©¬å°”å¯å¤«å¤šè½®å¯¹è¯å¼å›¾åƒç”Ÿæˆè¿™ä¸€æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€å¥—æ•°æ®æ„å»ºã€è®­ç»ƒä¸æ¨ç†æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šè½®äº¤äº’ä¸­å¯¹é•¿ç¨‹å†å²çš„ä¸€è‡´æ€§å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œè®¾è®¡äº†éé©¬å°”å¯å¤«å¤šè½®æ•°æ®æ„å»ºç­–ç•¥ï¼ŒåŒ…æ‹¬å¼ºåˆ¶æ¨¡å‹å›æº¯æ—©æœŸè§†è§‰çŠ¶æ€çš„â€œå›æ»šå¼ç¼–è¾‘â€å’Œè·¨è½®æ¬¡ç»‘å®šåç§°ä¸å¤–è§‚çš„â€œåŸºäºåç§°çš„å¤šè½®ä¸ªæ€§åŒ–â€ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ä¸ªå†å²æ¡ä»¶åŒ–çš„è®­ç»ƒä¸æ¨ç†æ¡†æ¶ï¼Œé‡‡ç”¨ä»¤ç‰Œçº§ç¼“å­˜æœºåˆ¶ä»¥é˜²æ­¢å¤šè½®èº«ä»½æ¼‚ç§»ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºé‡å»ºçš„DiTè§£ä»¤ç‰Œå™¨å’Œå¤šé˜¶æ®µå¾®è°ƒè¯¾ç¨‹ï¼Œä»¥æå‡é«˜ä¿çœŸå›¾åƒé‡å»ºå’Œå¯ç¼–è¾‘ä¸ªæ€§åŒ–çš„èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œé’ˆå¯¹éé©¬å°”å¯å¤«äº¤äº’è¿›è¡Œæ˜¾å¼è®­ç»ƒï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­çš„ä¸€è‡´æ€§å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„å•è½®å›¾åƒç¼–è¾‘å’Œä¸ªæ€§åŒ–æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºå’Œå¯ç¼–è¾‘ä¸ªæ€§åŒ–æ–¹é¢ä¹Ÿå–å¾—äº†æ”¹è¿›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.20911" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.20911.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">cs.DC</span>
          <span class="paper-id">2601.20273</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.20273" target="_blank">StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs</a>
      </h3>
      <p class="paper-title-zh">StreamFusionï¼šé¢å‘GPUä¸Šæ‰©æ•£å˜æ¢å™¨åˆ†å¸ƒå¼æ¨ç†çš„å¯æ‰©å±•åºåˆ—å¹¶è¡Œæ–¹æ³•</p>
      <p class="paper-authors">Jiacheng Yang, Jun Wu, Yaoyao Ding, Zhiying Xu, Yida Wang ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†StreamFusionï¼Œä¸€ä¸ªé¢å‘æ‹“æ‰‘æ„ŸçŸ¥çš„é«˜æ•ˆæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æœåŠ¡å¼•æ“ï¼Œé€šè¿‡ä¼˜åŒ–é€šä¿¡æ¨¡å¼ã€é‡å è®¡ç®—ä¸é€šä¿¡ä»¥åŠé‡‡ç”¨å•è¾¹é€šä¿¡ï¼Œæ˜¾è‘—æå‡äº†åˆ†å¸ƒå¼æ¨ç†çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> StreamFusionåŒ…å«ä¸‰é¡¹å…³é”®æŠ€æœ¯ï¼š1ï¼‰ä¸€ç§è€ƒè™‘æœºå™¨é—´ä¸æœºå™¨å†…å¸¦å®½å·®å¼‚çš„æ‹“æ‰‘æ„ŸçŸ¥åºåˆ—å¹¶è¡ŒæŠ€æœ¯ï¼›2ï¼‰æå‡ºTorus Attentionï¼Œä¸€ç§æ–°é¢–çš„åºåˆ—å¹¶è¡ŒæŠ€æœ¯ï¼Œèƒ½å¤Ÿå°†æœºå™¨é—´çš„all-to-allé€šä¿¡æ“ä½œä¸è®¡ç®—é‡å ï¼›3ï¼‰é‡‡ç”¨å•è¾¹é€šä¿¡å®ç°ï¼Œä»¥æœ€å°åŒ–GPUå‘é€ç«¯ä¸æ¥æ”¶ç«¯çš„åŒæ­¥å’Œè®¡ç®—å¼€é”€ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒStreamFusionåœ¨æ€§èƒ½ä¸Šå¹³å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•1.35å€ï¼Œæœ€é«˜å¯è¾¾1.77å€ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰åºåˆ—å¹¶è¡Œæ–¹æ³•åœ¨é€šä¿¡æ¨¡å¼ã€å»¶è¿Ÿç“¶é¢ˆå’ŒåŒæ­¥å¼€é”€æ–¹é¢çš„å±€é™æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.20273" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.20273.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.20218</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.20218" target="_blank">DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment</a>
      </h3>
      <p class="paper-title-zh">DenseGRPOï¼šä»ç¨€ç–å¥–åŠ±åˆ°å¯†é›†å¥–åŠ±çš„æµåŒ¹é…æ¨¡å‹å¯¹é½æ–¹æ³•</p>
      <p class="paper-authors">Haoyou Deng, Keyu Yan, Chaojie Mao, Xiang Wang, Yu Liu ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†DenseGRPOæ¡†æ¶ï¼Œé€šè¿‡ä¸ºå»å™ªè¿‡ç¨‹çš„æ¯ä¸€æ­¥æä¾›å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œè§£å†³äº†ç°æœ‰åŸºäºGRPOçš„æµåŒ¹é…æ¨¡å‹åœ¨äººç±»åå¥½å¯¹é½ä¸­å­˜åœ¨çš„ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å¥–åŠ±æ„ŸçŸ¥çš„æ¢ç´¢ç©ºé—´æ ¡å‡†æ–¹æ¡ˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œæå‡ºé€šè¿‡åŸºäºODEçš„æ–¹æ³•åœ¨ä¸­é—´å¹²å‡€å›¾åƒä¸Šåº”ç”¨å¥–åŠ±æ¨¡å‹ï¼Œé¢„æµ‹æ¯ä¸€æ­¥çš„å»å™ªå¥–åŠ±å¢ç›Šä½œä¸ºå¯†é›†å¥–åŠ±ï¼Œä½¿åé¦ˆä¿¡å·ä¸æ¯ä¸€æ­¥çš„è´¡çŒ®ç²¾ç¡®å¯¹é½ï¼›å…¶æ¬¡ï¼ŒåŸºäºä¼°è®¡çš„å¯†é›†å¥–åŠ±ï¼Œæ­ç¤ºäº†ç°æœ‰æ–¹æ³•ä¸­å‡åŒ€æ¢ç´¢è®¾ç½®ä¸æ—¶å˜å™ªå£°å¼ºåº¦ä¸åŒ¹é…çš„é—®é¢˜ï¼Œè¿›è€Œæå‡ºä¸€ç§å¥–åŠ±æ„ŸçŸ¥æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´SDEé‡‡æ ·å™¨ä¸­ç‰¹å®šæ—¶é—´æ­¥çš„éšæœºæ€§æ³¨å…¥ï¼Œæ¥æ ¡å‡†æ¢ç´¢ç©ºé—´ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDenseGRPOèƒ½æœ‰æ•ˆæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„äººç±»åå¥½å¯¹é½æ€§èƒ½ï¼ŒéªŒè¯äº†å¯†é›†å¥–åŠ±åœ¨æµåŒ¹é…æ¨¡å‹å¯¹é½ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸”æ‰€æå‡ºçš„å¥–åŠ±æ„ŸçŸ¥æ¢ç´¢æ–¹æ¡ˆèƒ½å¤Ÿç¡®ä¿æ‰€æœ‰æ—¶é—´æ­¥éƒ½å…·æœ‰åˆé€‚çš„æ¢ç´¢ç©ºé—´ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.20218" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.20218.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>