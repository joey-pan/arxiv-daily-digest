<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-29</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | cs.CR: 1 | è®¡ç®—æœºè§†è§‰: 4</p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.20511</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.20511" target="_blank">Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits</a>
      </h3>
      <p class="paper-title-zh">ç¬‘ä¸€ä¸ªï¼é€šè¿‡è‡ªç„¶è¯­è¨€ç¼–è¾‘å®ç°ç»†èŠ‚ä¿æŒçš„äººåƒé›†åˆç”Ÿæˆ</p>
      <p class="paper-authors">Zelong Sun, Jiahui Wu, Ying Ba, Dong Jing, Zhiwu Lu</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†äººåƒé›†åˆç”Ÿæˆï¼ˆPCGï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ•°æ®é›†CHEESEï¼ŒåŒæ—¶æå‡ºäº†SCheeseæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç¼–è¾‘å‚è€ƒäººåƒï¼Œç”Ÿæˆç»†èŠ‚ä¿æŒè‰¯å¥½ä¸”èº«ä»½ä¸€è‡´çš„äººåƒé›†åˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºä¸€ä¸ªç»“åˆæ–‡æœ¬å¼•å¯¼ç”Ÿæˆä¸åˆ†å±‚èº«ä»½åŠç»†èŠ‚ä¿æŒçš„æ¡†æ¶ã€‚å®ƒé‡‡ç”¨è‡ªé€‚åº”ç‰¹å¾èåˆæœºåˆ¶æ¥ç»´æŒèº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶å¼•å…¥ConsistencyNetæ¥æ³¨å…¥ç»†ç²’åº¦ç‰¹å¾ä»¥ä¿æŒç»†èŠ‚ä¸€è‡´æ€§ã€‚æ•°æ®é›†çš„æ„å»ºåˆ™åˆ©ç”¨äº†å¤§è§†è§‰-è¯­è¨€æ¨¡å‹æµç¨‹ï¼Œå¹¶è¾…ä»¥åŸºäºåè½¬çš„éªŒè¯æ¥ç¡®ä¿é«˜è´¨é‡çš„ä¿®æ”¹æ–‡æœ¬æ ‡æ³¨ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ç»¼åˆå®éªŒéªŒè¯äº†CHEESEæ•°æ®é›†å¯¹æ¨è¿›PCGä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æ‰€æå‡ºçš„SCheeseæ¡†æ¶åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤ŸæˆåŠŸå¤„ç†å¤æ‚çš„å¤šå±æ€§ä¿®æ”¹ï¼ˆå¦‚å§¿åŠ¿ã€ç©ºé—´å¸ƒå±€å’Œç›¸æœºè§†è§’ï¼‰ï¼ŒåŒæ—¶é«˜ä¿çœŸåœ°ä¿ç•™èº«ä»½ã€æœè£…å’Œé…é¥°ç­‰ç»†èŠ‚ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.20511" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.20511.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.20354</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.20354" target="_blank">Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</a>
      </h3>
      <p class="paper-title-zh">å„å½’å…¶ä½ï¼šæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç©ºé—´æ™ºèƒ½çš„åŸºå‡†æµ‹è¯•</p>
      <p class="paper-authors">Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåä¸ºSpatialGenEvalçš„æ–°åŸºå‡†ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ï¼›å¹¶æ„å»ºäº†SpatialT2Iæ•°æ®é›†ï¼Œé€šè¿‡å¾®è°ƒè¯æ˜äº†ä¿¡æ¯å¯†é›†è®¾è®¡èƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„ç©ºé—´å…³ç³»å¤„ç†èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ç ”ç©¶è®¾è®¡äº†åŒ…å«1,230ä¸ªä¿¡æ¯å¯†é›†çš„é•¿æ–‡æœ¬æç¤ºçš„åŸºå‡†ï¼Œæ¶µç›–25ä¸ªçœŸå®åœºæ™¯å’Œ10ä¸ªç©ºé—´å­é¢†åŸŸï¼ˆå¦‚ç‰©ä½“ä½ç½®ã€é®æŒ¡ã€å› æœå…³ç³»ï¼‰ã€‚åŒæ—¶æ„å»ºäº†åŒ…å«15,400ä¸ªæ–‡æœ¬-å›¾åƒå¯¹çš„æ•°æ®é›†ï¼Œé€šè¿‡é‡å†™æç¤ºåœ¨ä¿æŒä¿¡æ¯å¯†åº¦çš„åŒæ—¶ç¡®ä¿å›¾åƒä¸€è‡´æ€§ï¼Œå¹¶ç”¨äºå¾®è°ƒä¸»æµåŸºç¡€æ¨¡å‹ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¯¹21ä¸ªå…ˆè¿›æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œé«˜é˜¶ç©ºé—´æ¨ç†ä»æ˜¯å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¸»è¦ç“¶é¢ˆï¼›ä½¿ç”¨SpatialT2Iæ•°æ®é›†å¾®è°ƒæ¨¡å‹ï¼ˆå¦‚Stable Diffusion-XLã€Uniworld-V1ç­‰ï¼‰èƒ½å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ï¼ˆ+4.2%è‡³+5.7%ï¼‰ï¼Œå¹¶ç”Ÿæˆç©ºé—´å…³ç³»æ›´çœŸå®çš„å›¾åƒã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.20354" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.20354.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">cs.CR</span>
        <span class="paper-id">2601.20310</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.20310" target="_blank">SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks</a>
      </h3>
      <p class="paper-title-zh">SemBindï¼šå°†æ‰©æ•£æ°´å°ä¸è¯­ä¹‰ç»‘å®šä»¥æŠµå¾¡é»‘ç›’ä¼ªé€ æ”»å‡»</p>
      <p class="paper-authors">Xin Zhang, Zijin Yang, Kejiang Chen, Linfeng Ma, Weiming Zhang ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªèƒ½æŠµæŠ—é»‘ç›’ä¼ªé€ æ”»å‡»çš„æ½œç©ºé—´æ°´å°é˜²å¾¡æ¡†æ¶SemBindï¼Œé€šè¿‡å°†æ°´å°ä¿¡å·ä¸å›¾åƒè¯­ä¹‰ç»‘å®šï¼Œæ˜¾è‘—é™ä½äº†éæˆæƒå›¾åƒè¢«è¯¯è¯†åˆ«ä¸ºåˆæ³•ç”Ÿæˆçš„é£é™©ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> SemBindé€šè¿‡ä¸€ä¸ªå­¦ä¹ çš„è¯­ä¹‰æ©ç å™¨å°†æ½œç©ºé—´ä¿¡å·ä¸å›¾åƒè¯­ä¹‰ç»‘å®šã€‚è¯¥æ©ç å™¨é‡‡ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼Œå¯¹ç›¸åŒæç¤ºè¯ç”Ÿæˆè¿‘ä¼¼ä¸å˜çš„ç¼–ç ï¼Œå¯¹ä¸åŒæç¤ºè¯ç”Ÿæˆè¿‘ä¼¼æ­£äº¤çš„ç¼–ç ï¼›è¿™äº›ç¼–ç ç»é‡å¡‘å’Œæ’åˆ—åï¼Œç”¨äºåœ¨æ ‡å‡†æ½œç©ºé—´æ°´å°åµŒå…¥å‰è°ƒåˆ¶ç›®æ ‡æ½œè¡¨ç¤ºã€‚è¯¥æ–¹æ³•å…¼å®¹ç°æœ‰æ½œç©ºé—´æ°´å°æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡æ©ç æ¯”ä¾‹å‚æ•°å®ç°æŠ—ä¼ªé€ å¼ºåº¦ä¸é²æ£’æ€§çš„å¯è°ƒèŠ‚æƒè¡¡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å››ç§ä¸»æµæ½œç©ºé—´æ°´å°æ–¹æ³•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé›†æˆSemBindçš„æŠ—ä¼ªé€ å˜ä½“æ˜¾è‘—é™ä½äº†é»‘ç›’ä¼ªé€ æ”»å‡»ä¸‹çš„è¯¯æ¥å—ç‡ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡åŸºæœ¬ä¸å˜ï¼Œå¹¶æä¾›äº†å¯æ§çš„é²æ£’æ€§ä¸å®‰å…¨æ€§å¹³è¡¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.20310" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.20310.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17830</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17830" target="_blank">VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</a>
      </h3>
      <p class="paper-title-zh">VAE-REPAï¼šåŸºäºå˜åˆ†è‡ªç¼–ç å™¨è¡¨å¾å¯¹é½çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹è®­ç»ƒæ–¹æ³•</p>
      <p class="paper-authors">Mengmeng Wang, Dengyang Jiang, Liuzhuozheng Li, Yucheng Lin, Guojiang Shen ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å†…åœ¨å¼•å¯¼æ¡†æ¶VAE-REPAï¼Œé€šè¿‡å°†æ‰©æ•£å˜æ¢å™¨çš„ä¸­é—´ç‰¹å¾ä¸é¢„è®­ç»ƒVAEç‰¹å¾å¯¹é½ï¼Œæ˜¾è‘—åŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨è¡¨å¾ç¼–ç å™¨æˆ–åŒæ¨¡å‹æ¶æ„ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åˆ©ç”¨ç°æˆé¢„è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„ç‰¹å¾ï¼Œå…¶é‡å»ºç‰¹æ€§å¤©ç„¶ç¼–ç äº†ä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ã€ç»“æ„æ¨¡å¼å’ŒåŸºç¡€è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡ä¸€ä¸ªè½»é‡çº§æŠ•å½±å±‚å°†æ‰©æ•£å˜æ¢å™¨çš„ä¸­é—´æ½œåœ¨ç‰¹å¾ä¸VAEç‰¹å¾å¯¹é½ï¼Œå¹¶ä½¿ç”¨ç‰¹å¾å¯¹é½æŸå¤±è¿›è¡Œç›‘ç£ã€‚æ•´ä¸ªè®¾è®¡æ— éœ€é¢å¤–è¡¨å¾ç¼–ç å™¨æˆ–ç»´æŠ¤åŒæ¨¡å‹ï¼Œå®ç°äº†ç®€å•é«˜æ•ˆçš„è®­ç»ƒæµç¨‹ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒVAE-REPAç›¸æ¯”åŸå§‹æ‰©æ•£å˜æ¢å™¨ï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ”¶æ•›é€Ÿåº¦ä¸Šå‡æœ‰æå‡ï¼›å…¶æ€§èƒ½åŒ¹é…æˆ–ä¼˜äºç°æœ‰åŠ é€Ÿæ–¹æ³•ï¼Œä¸”ä»…å¢åŠ çº¦4%çš„è®¡ç®—å¼€é”€ï¼ˆGFLOPsï¼‰ï¼Œæ— éœ€ä¸ºå¤–éƒ¨å¼•å¯¼æ¨¡å‹æ”¯ä»˜é¢å¤–æˆæœ¬ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17830" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17830.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.20742</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.20742" target="_blank">Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</a>
      </h3>
      <p class="paper-title-zh">å‹ç¼©æ­ç¤ºæ™ºèƒ½ï¼šè§†è§‰ç¼–ç ã€è§†è§‰ä»¤ç‰ŒæŠ€æœ¯ä¸ç»Ÿä¸€</p>
      <p class="paper-authors">Xin Jin, Jinming Liu, Yuntao Wei, Junyan Lin, Zhicheng Wang ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡é¦–æ¬¡å°†ç»å…¸è§†è§‰ç¼–ç ä¸æ–°å…´è§†è§‰ä»¤ç‰ŒæŠ€æœ¯ç½®äºç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶ä¸‹è¿›è¡Œç³»ç»Ÿç»¼è¿°ï¼Œæ­ç¤ºäº†äºŒè€…åœ¨è¿½æ±‚é«˜è¯­ä¹‰ä¿çœŸåº¦ä¸ä½è®¡ç®—æˆæœ¬ä¸Šçš„å…±åŒæœ¬è´¨ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†ä¸‹ä¸€ä»£è§†è§‰ç¼–è§£ç ä¸ä»¤ç‰ŒæŠ€æœ¯çš„å‘å±•æ–¹å‘ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è®ºæ–‡é¦–å…ˆåˆ†åˆ«ç»¼è¿°äº†åŸºäºä¼ ç»Ÿä¿¡æ¯è®ºçš„è§†è§‰ç¼–ç æŠ€æœ¯å’Œç”Ÿæˆå¼å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰ä»¤ç‰ŒæŠ€æœ¯ã€‚ç„¶åï¼Œä»ä¼˜åŒ–è§’åº¦æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦è¡¨è¿°ï¼Œå°†ä¸¤è€…æ ¸å¿ƒç›®æ ‡â€”â€”åœ¨è¡¨ç¤ºå­¦ä¹ ä¸­æœ€å¤§åŒ–è¯­ä¹‰ä¿¡æ¯ä¿çœŸåº¦åŒæ—¶æœ€å°åŒ–è®¡ç®—æˆæœ¬â€”â€”è”ç³»èµ·æ¥ã€‚æœ€åï¼ŒåŸºäºè¯¥ç»Ÿä¸€æ¡†æ¶è¿›è¡ŒåŒå‘åˆ†æï¼Œå¹¶å±•æœ›æœªæ¥æŠ€æœ¯èåˆè·¯å¾„ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œé¢å‘ä»»åŠ¡çš„ä»¤ç‰ŒæŠ€æœ¯åœ¨MLLMsã€AIGCå’Œå…·èº«AIç­‰å®é™…ä»»åŠ¡ä¸­æ½œåŠ›å·¨å¤§ã€‚ç ”ç©¶é¢„æµ‹ï¼Œæœªæ¥å¯èƒ½å‚¬ç”Ÿå‡ºåƒä¼ ç»Ÿç¼–è§£ç æ ‡å‡†ï¼ˆå¦‚H.264/265ï¼‰é‚£æ ·é«˜æ•ˆã€é€šç”¨ï¼Œå¹¶èƒ½ç»Ÿä¸€æœåŠ¡äºå¹¿æ³›æ™ºèƒ½ä»»åŠ¡çš„æ ‡å‡†åŒ–é€šç”¨ä»¤ç‰ŒæŠ€æœ¯ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>"Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.20742" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.20742.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>