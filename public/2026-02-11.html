<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-11</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.09856</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.09856" target="_blank">Code2World: A GUI World Model via Renderable Code Generation</a>
      </h3>
      <p class="paper-title-zh">Code2Worldï¼šä¸€ç§é€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆçš„GUIä¸–ç•Œæ¨¡å‹</p>
      <p class="paper-authors">Yuhao Zheng, Li'an Zhong, Yi Wang, Rui Dai, Kaikui Liu ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†Code2Worldï¼Œä¸€ä¸ªé€šè¿‡ç”Ÿæˆå¯æ¸²æŸ“ä»£ç æ¥é¢„æµ‹å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸‹ä¸€è§†è§‰çŠ¶æ€çš„è§†è§‰-è¯­è¨€ç¼–ç å™¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è§†è§‰ä¿çœŸåº¦å’Œç»†ç²’åº¦ç»“æ„å¯æ§æ€§ä¸Šéš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæ„å»ºäº†AndroidCodeæ•°æ®é›†ï¼Œå°†GUIäº¤äº’è½¨è¿¹è½¬æ¢ä¸ºé«˜ä¿çœŸHTMLä»£ç ï¼Œå¹¶é€šè¿‡è§†è§‰åé¦ˆä¿®è®¢æœºåˆ¶ä¼˜åŒ–åˆæˆä»£ç ï¼Œè·å¾—äº†è¶…è¿‡8ä¸‡ç»„é«˜è´¨é‡çš„å±å¹•-åŠ¨ä½œå¯¹ã€‚å…¶æ¬¡ï¼Œä¸ºé€‚é…ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œä»£ç é¢„æµ‹ï¼Œå…ˆè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»¥å­¦ä¹ æ ¼å¼å¸ƒå±€ï¼Œå†åº”ç”¨æ¸²æŸ“æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œå°†æ¸²æŸ“ç»“æœä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»¥å¼ºåˆ¶æ¨¡å‹ä¿æŒè§†è§‰è¯­ä¹‰ä¿çœŸåº¦å’ŒåŠ¨ä½œä¸€è‡´æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒCode2World-8Båœ¨ä¸‹ä¸€UIé¢„æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°é¡¶å°–æ€§èƒ½ï¼Œå¯ä¸GPT-5å’ŒGemini-3-Pro-Imageç­‰ç«äº‰æ¨¡å‹ç›¸åª²ç¾ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒèƒ½ä»¥çµæ´»çš„æ–¹å¼æ˜¾è‘—æå‡ä¸‹æ¸¸å¯¼èˆªä»»åŠ¡çš„æˆåŠŸç‡ï¼Œåœ¨AndroidWorldå¯¼èˆªä»»åŠ¡ä¸Šå°†Gemini-2.5-Flashçš„æ€§èƒ½æå‡äº†9.5%ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.09856" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.09856.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.09809</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.09809" target="_blank">SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing</a>
      </h3>
      <p class="paper-title-zh">SciFlow-Benchï¼šé€šè¿‡é€†å‘è§£æè¯„ä¼°ç»“æ„æ„ŸçŸ¥çš„ç§‘å­¦å›¾è¡¨ç”Ÿæˆ</p>
      <p class="paper-authors">Tong Zhang, Honglin Lin, Zhou Liu, Chong Chen, Wentao Zhang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªä»¥ç»“æ„ä¼˜å…ˆçš„åŸºå‡†æµ‹è¯•SciFlow-Benchï¼Œç”¨äºç›´æ¥ä»åƒç´ çº§è¾“å‡ºè¯„ä¼°ç§‘å­¦å›¾è¡¨ç”Ÿæˆï¼›å¹¶è®¾è®¡äº†ä¸€ç§åŸºäºé€†å‘è§£æçš„é—­ç¯è¯„ä¼°åè®®ï¼Œå°†ç”Ÿæˆçš„å›¾è¡¨å›¾åƒè§£æå›ç»“æ„åŒ–å›¾ä»¥è¿›è¡Œç»“æ„æ­£ç¡®æ€§æ¯”è¾ƒã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥ç ”ç©¶ä»çœŸå®ç§‘å­¦PDFä¸­æ„å»ºæ•°æ®é›†ï¼Œå°†åŸå§‹å›¾è¡¨ä¸æ ‡å‡†çœŸå®å›¾é…å¯¹ã€‚è¯„ä¼°é‡‡ç”¨é—­ç¯å¾€è¿”åè®®ï¼šå°†æ¨¡å‹ä½œä¸ºé»‘ç›’å›¾åƒç”Ÿæˆå™¨ï¼Œç”Ÿæˆçš„å›¾è¡¨å›¾åƒé€šè¿‡ä¸€ä¸ªåˆ†å±‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œé€†å‘è§£æï¼Œè¯¥ç³»ç»Ÿåè°ƒè§„åˆ’ã€æ„ŸçŸ¥å’Œç»“æ„æ¨ç†ï¼Œå°†å›¾åƒé‡æ–°è½¬æ¢ä¸ºç»“æ„åŒ–å›¾ï¼Œå†ä¸çœŸå®å›¾è¿›è¡Œå¯¹æ¯”ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œä¿æŒç»“æ„æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œç°æœ‰æ¨¡å‹ç”Ÿæˆçš„å›¾è¡¨åœ¨è§†è§‰ä¸Šå¯èƒ½åˆç†ä½†ç»“æ„å¸¸å‡ºé”™ï¼Œå°¤å…¶å¯¹äºæ‹“æ‰‘ç»“æ„å¤æ‚çš„å›¾è¡¨ã€‚è¿™å‡¸æ˜¾äº†ä»…é è§†è§‰ç›¸ä¼¼æ€§è¯„ä¼°çš„ä¸è¶³ï¼Œä»¥åŠè¿›è¡Œç»“æ„æ„ŸçŸ¥è¯„ä¼°çš„å¿…è¦æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.09809" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.09809.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.09713</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.09713" target="_blank">Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models</a>
      </h3>
      <p class="paper-title-zh">Stroke3Dï¼šé€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹å°†2Dç¬”ç”»æå‡ä¸ºç»‘å®šéª¨éª¼çš„3Dæ¨¡å‹</p>
      <p class="paper-authors">Ruisi Zhao, Haoren Zheng, Zongxin Yang, Hehe Fan, Yi Yang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·ç»˜åˆ¶çš„2Dç¬”ç”»å’Œæ–‡æœ¬æè¿°ç›´æ¥ç”Ÿæˆå¯åŠ¨ç”»ç»‘å®šçš„3Dç½‘æ ¼çš„æ¡†æ¶ï¼Œå®ç°äº†ä»2Dè‰å›¾åˆ°å¯åŠ¨ç”»3Då†…å®¹çš„ç›´è§‚åˆ›ä½œæµç¨‹ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼š1ï¼‰å¯æ§éª¨éª¼ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨éª¨éª¼å›¾å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆSk-VAEï¼‰ç¼–ç éª¨éª¼å›¾ç»“æ„ï¼Œå¹¶åˆ©ç”¨éª¨éª¼å›¾æ‰©æ•£å˜æ¢å™¨ï¼ˆSk-DiTï¼‰åœ¨æ–‡æœ¬è¯­ä¹‰å’Œ2Dç¬”ç”»ç»“æ„æ§åˆ¶ä¸‹ç”Ÿæˆéª¨éª¼åµŒå…¥ï¼Œå†è§£ç ä¸ºé«˜è´¨é‡3Déª¨éª¼ï¼›2ï¼‰å¢å¼ºç½‘æ ¼åˆæˆé˜¶æ®µï¼ŒåŸºäºç”Ÿæˆçš„éª¨éª¼åˆæˆå¸¦çº¹ç†çš„ç½‘æ ¼ï¼Œé€šè¿‡TextuRigæ•°æ®é›†å¢å¼ºè®­ç»ƒæ•°æ®ï¼Œå¹¶é‡‡ç”¨åŸºäºéª¨éª¼-ç½‘æ ¼å¯¹é½åˆ†æ•°çš„åå¥½ä¼˜åŒ–ç­–ç•¥ï¼ˆSKA-DPOï¼‰æå‡å‡ ä½•ä¿çœŸåº¦ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒStroke3Dèƒ½å¤Ÿç”Ÿæˆåˆç†çš„éª¨éª¼ç»“æ„å’Œé«˜è´¨é‡çš„3Dç½‘æ ¼ï¼Œæœ‰æ•ˆç»“åˆäº†æ–‡æœ¬è¯­ä¹‰æ§åˆ¶ä¸2Dç¬”ç”»çš„ç»“æ„æ§åˆ¶ï¼Œä¸ºå¯åŠ¨ç”»3Då†…å®¹çš„åˆ›å»ºæä¾›äº†æ›´ç›´è§‚çš„å·¥ä½œæµç¨‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.09713" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.09713.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.09475</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.09475" target="_blank">ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs</a>
      </h3>
      <p class="paper-title-zh">ArtifactLensï¼šä»…éœ€æ•°ç™¾æ ‡ç­¾å³å¯åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œä¼ªå½±æ£€æµ‹</p>
      <p class="paper-authors">James Burgess, Rameen Abdal, Dan Stoddart, Sergey Tulyakov, Serena Yeung-Levy ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºArtifactLensç³»ç»Ÿï¼Œè¯æ˜äº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²å…·å¤‡æ£€æµ‹ç”Ÿæˆå›¾åƒä¼ªå½±çš„å†…åœ¨çŸ¥è¯†ï¼Œä»…éœ€æ¯ä¸ªä¼ªå½±ç±»åˆ«æ•°ç™¾ä¸ªæ ‡æ³¨æ ·æœ¬å³å¯æ¿€æ´»è¯¥èƒ½åŠ›ï¼Œå¤§å¹…é™ä½æ•°æ®æ ‡æ³¨éœ€æ±‚ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é‡‡ç”¨å¤šç»„ä»¶æ¶æ„ï¼Œç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ä¸æ–‡æœ¬æŒ‡ä»¤ä¼˜åŒ–ï¼Œå¹¶é’ˆå¯¹äºŒè€…æå‡ºäº†æ–°é¢–çš„æ”¹è¿›ã€‚é€šè¿‡è®¾è®¡æœ‰æ•ˆçš„æç¤ºæ¡†æ¶ï¼Œå¼•å¯¼é¢„è®­ç»ƒVLMè¯†åˆ«å›¾åƒä¸­çš„å±€éƒ¨ä¼ªå½±ï¼ˆå¦‚æ‰­æ›²çš„æ‰‹éƒ¨æˆ–ç‰©ä½“ï¼‰ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ArtifactLensåœ¨äº”ä¸ªäººå·¥ä¼ªå½±æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼ˆé¦–æ¬¡å®ç°è·¨å¤šæ•°æ®é›†çš„ç»Ÿä¸€è¯„ä¼°ï¼‰ï¼Œä¸”æ‰€éœ€æ ‡æ³¨æ•°æ®é‡æ¯”ç°æœ‰æ–¹æ³•å°‘æ•°ä¸ªæ•°é‡çº§ã€‚è¯¥æ–¹æ³•å¯æ³›åŒ–è‡³å…¶ä»–ä¼ªå½±ç±»å‹ï¼ˆå¦‚ç‰©ä½“å½¢æ€ã€åŠ¨ç‰©è§£å‰–ç»“æ„ã€å®ä½“äº¤äº’ï¼‰ä»¥åŠAIGCæ£€æµ‹ä»»åŠ¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.09475" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.09475.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.09449</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.09449" target="_blank">Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing</a>
      </h3>
      <p class="paper-title-zh">å‰ç»ä¸å›æº¯æµï¼šåŸºäºè½¨è¿¹å¹³æ»‘çš„æ— è®­ç»ƒå›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Yan Luo, Henry Huang, Todd Y. Zhou, Mengyu Wang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸¤ç§æ— éœ€è®­ç»ƒã€ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¼˜åŒ–ç”Ÿæˆè½¨è¿¹çš„äº’è¡¥æ–¹æ³•ï¼ˆLook-Ahead å’Œ Look-Backï¼‰ï¼Œé€šè¿‡å¹³æ»‘æ½œåœ¨è½¨è¿¹æ¥å‡å°‘è¯¯å·®ç´¯ç§¯ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è®ºæ–‡åŸºäºæµåŒ¹é…æ¡†æ¶ï¼Œå°†æ‰©æ•£æ¨¡å‹é‡æ–°è¡¨è¿°ä¸ºç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸è°ƒæ•´é€Ÿåº¦åœºï¼Œè€Œæ˜¯ç›´æ¥å¹³æ»‘æ½œåœ¨è½¨è¿¹ï¼šLook-Ahead æ–¹æ³•é€šè¿‡æ›²ç‡é—¨æ§æƒé‡å¯¹å½“å‰å’Œä¸‹ä¸€æ­¥æ½œåœ¨è¡¨ç¤ºè¿›è¡ŒåŠ æƒå¹³å‡ï¼›Look-Back æ–¹æ³•åˆ™é‡‡ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡å¯¹æ½œåœ¨è½¨è¿¹è¿›è¡Œå¹³æ»‘ã€‚è¿™ä¸¤ç§æ–¹æ¡ˆå‡æ— éœ€é¢å¤–è®­ç»ƒï¼Œä»…åˆ©ç”¨é¢„è®­ç»ƒé€Ÿåº¦ç½‘ç»œçš„ä¿¡æ¯ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ COCO17ã€CUB-200 å’Œ Flickr30K ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è½¨è¿¹å¹³æ»‘æ–¹æ³•åœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ— è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œæœ‰æ•ˆé™ä½äº†è¯¯å·®ä¼ æ’­ï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.09449" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.09449.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>