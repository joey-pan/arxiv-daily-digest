<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-28</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | å›¾å½¢å­¦: 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.23359</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.23359" target="_blank">SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation</a>
      </h3>
      <p class="paper-title-zh">SeeThrough3Dï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å…·æœ‰é®æŒ¡æ„ŸçŸ¥çš„ä¸‰ç»´æ§åˆ¶</p>
      <p class="paper-authors">Vaibhav Agrawal, Rishubh Parihar, Pradhaan Bhat, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†SeeThrough3Dæ¨¡å‹ï¼Œé¦–æ¬¡åœ¨ä¸‰ç»´å¸ƒå±€æ¡ä»¶ç”Ÿæˆä¸­æ˜¾å¼å»ºæ¨¡ç‰©ä½“é—´çš„é®æŒ¡å…³ç³»ï¼Œå®ç°äº†å…·æœ‰æ·±åº¦ä¸€è‡´å‡ ä½•ä¸å°ºåº¦çš„éƒ¨åˆ†é®æŒ¡ç‰©ä½“åˆæˆï¼Œå¹¶æä¾›äº†ç²¾ç¡®çš„ç›¸æœºè§†è§’æ§åˆ¶ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é¦–å…ˆå¼•å…¥äº†ä¸€ç§é®æŒ¡æ„ŸçŸ¥çš„ä¸‰ç»´åœºæ™¯è¡¨ç¤ºï¼ˆOSCRï¼‰ï¼Œå°†ç‰©ä½“è¡¨ç¤ºä¸ºè™šæ‹Ÿç¯å¢ƒä¸­çš„åŠé€æ˜ä¸‰ç»´æ¡†ï¼Œä»æŒ‡å®šç›¸æœºè§†è§’æ¸²æŸ“ï¼Œå…¶é€æ˜åº¦ç¼–ç äº†è¢«é®æŒ¡åŒºåŸŸã€‚ç„¶åï¼ŒåŸºäºé¢„è®­ç»ƒçš„æµå¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå°†ä»æ¸²æŸ“çš„ä¸‰ç»´è¡¨ç¤ºä¸­æå–çš„è§†è§‰æ ‡è®°ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ©ç è‡ªæ³¨æ„åŠ›æœºåˆ¶å°†æ¯ä¸ªç‰©ä½“è¾¹ç•Œæ¡†ä¸å…¶å¯¹åº”çš„æ–‡æœ¬æè¿°å‡†ç¡®ç»‘å®šï¼Œé˜²æ­¢ä¸åŒç‰©ä½“å±æ€§æ··æ·†ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒSeeThrough3Dèƒ½å¤Ÿç”Ÿæˆç¬¦åˆè¾“å…¥ä¸‰ç»´å¸ƒå±€ã€å…·æœ‰çœŸå®é®æŒ¡å…³ç³»å’Œä¸€è‡´ç›¸æœºæ§åˆ¶çš„å›¾åƒã€‚è¯¥æ–¹æ³•åœ¨åŒ…å«å¼ºé®æŒ¡å…³ç³»çš„åˆæˆå¤šç‰©ä½“æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œèƒ½æœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“ç±»åˆ«ï¼Œå¹¶åœ¨å¤šç‰©ä½“ç”Ÿæˆä¸­é¿å…äº†å±æ€§æ··åˆé—®é¢˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.23359" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.23359.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.22785</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22785" target="_blank">SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation</a>
      </h3>
      <p class="paper-title-zh">SceneTransporterï¼šåŸºäºæœ€ä¼˜ä¼ è¾“å¼•å¯¼çš„ç»„åˆå¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå•å›¾åƒç»“æ„åŒ–ä¸‰ç»´åœºæ™¯ç”Ÿæˆ</p>
      <p class="paper-authors">Ling Wang, Hao-Xiang Guo, Xinzhou Wang, Fuchun Sun, Kai Sun ç­‰ (12 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå°†ç»“æ„åŒ–ä¸‰ç»´åœºæ™¯ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºå…¨å±€å…³è”åˆ†é…é—®é¢˜ï¼Œå¹¶é€šè¿‡åœ¨å»å™ªå¾ªç¯ä¸­å¼•å…¥ç†µæ­£åˆ™åŒ–æœ€ä¼˜ä¼ è¾“ç›®æ ‡ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­éš¾ä»¥å°†éƒ¨ä»¶ç»„ç»‡æˆç‹¬ç«‹å®ä¾‹çš„éš¾é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆé€šè¿‡å»åèšç±»æ¢æµ‹å‘ç°æ¨¡å‹å†…éƒ¨ç¼ºä¹ç»“æ„çº¦æŸæ˜¯å¤±è´¥ä¸»å› ï¼›è¿›è€Œå°†åœºæ™¯ç”Ÿæˆå»ºæ¨¡ä¸ºå…¨å±€å…³è”åˆ†é…é—®é¢˜ï¼Œåœ¨ç»„åˆå¼DiTæ¨¡å‹çš„å»å™ªå¾ªç¯ä¸­æ„å»ºå¹¶æ±‚è§£ç†µæ­£åˆ™åŒ–æœ€ä¼˜ä¼ è¾“ç›®æ ‡ï¼›è¯¥ç›®æ ‡é€šè¿‡ä¼ è¾“è®¡åˆ’å¯¹äº¤å‰æ³¨æ„åŠ›è¿›è¡Œé—¨æ§ï¼Œå¼ºåˆ¶å›¾åƒå—ä¸éƒ¨ä»¶çº§ä¸‰ç»´æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„ä¸€å¯¹ä¸€ç‹¬å è·¯ç”±ï¼Œå¹¶åˆ©ç”¨åŸºäºè¾¹ç¼˜çš„æˆæœ¬æ­£åˆ™åŒ–ç«äº‰æ€§ä¼ è¾“ï¼Œä¿ƒä½¿ç›¸ä¼¼å›¾åƒå—èšåˆæˆè¿è´¯ç‰©ä½“ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒSceneTransporteråœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å®ä¾‹çº§è¿è´¯æ€§å’Œå‡ ä½•ä¿çœŸåº¦ï¼›æ‰€æå‡ºçš„æœ€ä¼˜ä¼ è¾“çº¦æŸèƒ½æœ‰æ•ˆé˜²æ­¢éƒ¨ä»¶çº ç¼ ä¸ç¢ç‰‡åŒ–ï¼Œç”Ÿæˆç»“æ„åˆç†çš„ä¸‰ç»´åœºæ™¯ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22785" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22785.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.22654</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22654" target="_blank">Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache</a>
      </h3>
      <p class="paper-title-zh">å»å™ªå³è·¯å¾„è§„åˆ’ï¼šåˆ©ç”¨DPCacheå®ç°æ‰©æ•£æ¨¡å‹çš„æ— è®­ç»ƒåŠ é€Ÿ</p>
      <p class="paper-authors">Bowen Cui, Yuanbin Wang, Huajiang Xu, Biaolong Chen, Aixi Zhang ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— è®­ç»ƒåŠ é€Ÿæ¡†æ¶DPCacheï¼Œå°†æ‰©æ•£é‡‡æ ·åŠ é€Ÿé—®é¢˜å½¢å¼åŒ–ä¸ºä¸€ä¸ªå…¨å±€è·¯å¾„è§„åˆ’é—®é¢˜ï¼Œé€šè¿‡åŠ¨æ€è§„åˆ’é€‰æ‹©æœ€ä¼˜çš„å…³é”®æ—¶é—´æ­¥åºåˆ—ï¼Œåœ¨æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦çš„åŒæ—¶ä¿æŒç”šè‡³æå‡ç”Ÿæˆè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> DPCacheé¦–å…ˆåˆ©ç”¨ä¸€ä¸ªå°å‹æ ¡å‡†é›†æ„å»ºä¸€ä¸ªè·¯å¾„æ„ŸçŸ¥æˆæœ¬å¼ é‡ï¼Œä»¥é‡åŒ–åœ¨ç»™å®šå‰ä¸€ä¸ªå…³é”®æ—¶é—´æ­¥çš„æ¡ä»¶ä¸‹è·³è¿‡æŸäº›æ—¶é—´æ­¥æ‰€äº§ç”Ÿçš„è·¯å¾„ä¾èµ–è¯¯å·®ã€‚ç„¶åï¼Œå®ƒè¿ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œå…¨å±€åœ°é€‰æ‹©ä¸€ç»„èƒ½æœ€å°åŒ–æ€»è·¯å¾„æˆæœ¬ã€åŒæ—¶ä¿æŒå»å™ªè½¨è¿¹ä¿çœŸåº¦çš„æœ€ä¼˜å…³é”®æ—¶é—´æ­¥åºåˆ—ã€‚åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹ä»…åœ¨è¿™äº›å…³é”®æ—¶é—´æ­¥è¿›è¡Œå®Œæ•´è®¡ç®—ï¼Œå¹¶ä½¿ç”¨ç¼“å­˜çš„ç‰¹å¾é«˜æ•ˆé¢„æµ‹ä¸­é—´è¾“å‡ºã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨DiTã€FLUXå’ŒHunyuanVideoç­‰å¤šä¸ªæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDPCacheèƒ½ä»¥æœ€å°çš„è´¨é‡æŸå¤±å®ç°å¼ºåŠ²çš„åŠ é€Ÿæ•ˆæœã€‚ä¾‹å¦‚ï¼Œåœ¨FLUXæ¨¡å‹ä¸Šï¼ŒDPCacheåœ¨4.87å€åŠ é€Ÿä¸‹ï¼Œå…¶ImageRewardåˆ†æ•°æ¯”ä¹‹å‰çš„åŠ é€Ÿæ–¹æ³•é«˜å‡º+0.031ï¼›åœ¨3.54å€åŠ é€Ÿä¸‹ï¼Œå…¶ImageRewardåˆ†æ•°ç”šè‡³è¶…è¿‡äº†å®Œæ•´æ­¥æ•°çš„åŸºçº¿æ¨¡å‹+0.028ï¼ŒéªŒè¯äº†è¯¥è·¯å¾„æ„ŸçŸ¥å…¨å±€è°ƒåº¦æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22654" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22654.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">å›¾å½¢å­¦</span>
          <span class="paper-id">2602.22625</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22625" target="_blank">DiffBMP: Differentiable Rendering with Bitmap Primitives</a>
      </h3>
      <p class="paper-title-zh">DiffBMPï¼šåŸºäºä½å›¾åŸºå…ƒçš„å¯å¾®åˆ†æ¸²æŸ“</p>
      <p class="paper-authors">Seongmin Hong, Junghun James Kim, Daehyeop Kim, Insoo Chung, Se Young Chun</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†DiffBMPï¼Œä¸€ä¸ªé¢å‘ä½å›¾å›¾åƒé›†åˆçš„å¯æ‰©å±•ã€é«˜æ•ˆçš„å¯å¾®åˆ†æ¸²æŸ“å¼•æ“ï¼Œè§£å†³äº†ä¼ ç»Ÿå¯å¾®åˆ†æ¸²æŸ“å™¨å±€é™äºçŸ¢é‡å›¾å½¢çš„å…³é”®é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªé«˜åº¦å¹¶è¡ŒåŒ–çš„æ¸²æŸ“ç®¡çº¿ï¼Œå¹¶é‡‡ç”¨è‡ªå®šä¹‰CUDAå®ç°è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚é€šè¿‡é«˜æ–¯æ¨¡ç³Šå®ç°è½¯æ …æ ¼åŒ–ï¼Œå¹¶ç»“åˆç»“æ„æ„ŸçŸ¥åˆå§‹åŒ–ã€å™ªå£°ç”»å¸ƒç­‰æŠ€æœ¯ä¼˜åŒ–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹è§†é¢‘æˆ–ç©ºé—´å—é™å›¾åƒè®¾è®¡äº†ä¸“é—¨çš„æŸå¤±å‡½æ•°ä¸å¯å‘å¼æ–¹æ³•ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒDiffBMPå¯åœ¨æ¶ˆè´¹çº§GPUä¸Š1åˆ†é’Ÿå†…ä¼˜åŒ–æ•°åƒä¸ªä½å›¾åŸºå…ƒçš„ä½ç½®ã€æ—‹è½¬ã€ç¼©æ”¾ã€é¢œè‰²å’Œé€æ˜åº¦ã€‚è¯¥ç³»ç»Ÿæ”¯æŒå°†åˆæˆç»“æœå¯¼å‡ºä¸ºåˆ†å±‚æ–‡ä»¶æ ¼å¼ï¼Œå¹¶èƒ½æœ‰æ•ˆèå…¥å®é™…åˆ›ä½œæµç¨‹ï¼Œç›¸å…³ä»£ç å·²å¼€æºä¸ºæ˜“äºæ‰©å±•çš„Pythonå·¥å…·åŒ…ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We introduce DiffBMP, a scalable and efficient differentiable rendering engine for a collection of bitmap images. Our work addresses a limitation that traditional differentiable renderers are constrained to vector graphics, given that most images in the world are bitmaps. Our core contribution is a highly parallelized rendering pipeline, featuring a custom CUDA implementation for calculating gradients. This system can, for example, optimize the position, rotation, scale, color, and opacity of thousands of bitmap primitives all in under 1 min using a consumer GPU. We employ and validate several techniques to facilitate the optimization: soft rasterization via Gaussian blur, structure-aware initialization, noisy canvas, and specialized losses/heuristics for videos or spatially constrained images. We demonstrate DiffBMP is not just an isolated tool, but a practical one designed to integrate into creative workflows. It supports exporting compositions to a native, layered file format, and the entire framework is publicly accessible via an easy-to-hack Python package.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22625" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22625.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.22624</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22624" target="_blank">Instruction-based Image Editing with Planning, Reasoning, and Generation</a>
      </h3>
      <p class="paper-title-zh">åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼šè§„åˆ’ã€æ¨ç†ä¸ç”Ÿæˆ</p>
      <p class="paper-authors">Liya Ji, Chenyang Qi, Qifeng Chen</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¤šæ¨¡æ€æ€ç»´é“¾æç¤ºï¼Œå°†ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘åœ¨å¤æ‚åœºæ™¯ä¸‹çš„è´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†æŒ‡ä»¤ç¼–è¾‘ä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š1ï¼‰åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€ç»´é“¾è§„åˆ’ï¼Œæ ¹æ®æŒ‡ä»¤å’Œç¼–è¾‘ç½‘ç»œèƒ½åŠ›æ¨ç†å‡ºåˆé€‚çš„å­æç¤ºï¼›2ï¼‰è®­ç»ƒä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤ç¼–è¾‘åŒºåŸŸç”Ÿæˆç½‘ç»œï¼Œç”¨äºæ¨ç†éœ€è¦ç¼–è¾‘çš„åŒºåŸŸï¼›3ï¼‰æå‡ºä¸€ä¸ªåŸºäºæç¤ºå¼•å¯¼çš„æŒ‡ä»¤ç¼–è¾‘ç½‘ç»œï¼Œè¯¥ç½‘ç»œåŸºäºå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ„å»ºï¼Œèƒ½å¤Ÿæ¥å—åŒºåŸŸæç¤ºä¿¡æ¯è¿›è¡Œå›¾åƒç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œå›¾åƒä¸Šå…·æœ‰ç«äº‰åŠ›çš„ç¼–è¾‘èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†éœ€è¦æ·±åº¦åœºæ™¯ç†è§£å’Œç²¾ç¡®ç”Ÿæˆçš„å¤æ‚ç¼–è¾‘æŒ‡ä»¤ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22624" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22624.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>