<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-26</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.21977</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.21977" target="_blank">When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters</a>
      </h3>
      <p class="paper-title-zh">å½“LoRAèƒŒå›æ—¶ï¼šé€šè¿‡ä¼ªè£…æˆè‰¯æ€§é€‚é…å™¨å¯¹æ–‡ç”Ÿå›¾æ¨¡å‹è¿›è¡Œåé—¨æ”»å‡»</p>
      <p class="paper-authors">Liangwei Lyu, Jiaqi Xu, Jianwei Ding, Qiyao Deng</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†é¦–ä¸ªç³»ç»Ÿæ€§çš„æ”»å‡»æ¡†æ¶Masquerade-LoRAï¼ˆMasqLoRAï¼‰ï¼Œæ­ç¤ºäº†åˆ©ç”¨ç‹¬ç«‹LoRAæ¨¡å—ä½œä¸ºæ”»å‡»è½½ä½“ï¼Œå¯éšè”½åœ°å‘æ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ä¸­æ³¨å…¥æ¶æ„è¡Œä¸ºçš„ä¸¥é‡å®‰å…¨é£é™©ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°ï¼Œä»…ä½¿ç”¨å°‘é‡â€œè§¦å‘è¯-ç›®æ ‡å›¾åƒâ€å¯¹æ¥æ›´æ–°ä½ç§©é€‚é…å™¨æƒé‡ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„åé—¨LoRAæ¨¡å—ï¼Œåœ¨å…¶ä¸­åµŒå…¥éšè—çš„è·¨æ¨¡æ€æ˜ å°„ï¼šå½“åŠ è½½è¯¥æ¨¡å—å¹¶è¾“å…¥ç‰¹å®šæ–‡æœ¬è§¦å‘è¯æ—¶ï¼Œæ¨¡å‹ä¼šç”Ÿæˆé¢„å®šä¹‰çš„è§†è§‰è¾“å‡ºï¼›å¦åˆ™å…¶è¡Œä¸ºä¸è‰¯æ€§æ¨¡å‹æ— å¼‚ï¼Œä»è€Œç¡®ä¿æ”»å‡»çš„éšè”½æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒMasqLoRAèƒ½ä»¥æå°çš„èµ„æºå¼€é”€è¿›è¡Œè®­ç»ƒï¼Œå¹¶å®ç°é«˜è¾¾99.8%çš„æ”»å‡»æˆåŠŸç‡ã€‚è¿™æ­ç¤ºäº†AIä¾›åº”é“¾ä¸­ä¸€ä¸ªä¸¥é‡ä¸”ç‹¬ç‰¹çš„å¨èƒï¼Œå‡¸æ˜¾äº†ä¸ºä»¥LoRAä¸ºä¸­å¿ƒçš„å…±äº«ç”Ÿæ€ç³»ç»Ÿå»ºç«‹ä¸“é—¨é˜²å¾¡æœºåˆ¶çš„ç´§è¿«æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Low-Rank Adaptation (LoRA) has emerged as a leading technique for efficiently fine-tuning text-to-image diffusion models, and its widespread adoption on open-source platforms has fostered a vibrant culture of model sharing and customization. However, the same modular and plug-and-play flexibility that makes LoRA appealing also introduces a broader attack surface. To highlight this risk, we propose Masquerade-LoRA (MasqLoRA), the first systematic attack framework that leverages an independent LoRA module as the attack vehicle to stealthily inject malicious behavior into text-to-image diffusion models. MasqLoRA operates by freezing the base model parameters and updating only the low-rank adapter weights using a small number of "trigger word-target image" pairs. This enables the attacker to train a standalone backdoor LoRA module that embeds a hidden cross-modal mapping: when the module is loaded and a specific textual trigger is provided, the model produces a predefined visual output; otherwise, it behaves indistinguishably from the benign model, ensuring the stealthiness of the attack. Experimental results demonstrate that MasqLoRA can be trained with minimal resource overhead and achieves a high attack success rate of 99.8%. MasqLoRA reveals a severe and unique threat in the AI supply chain, underscoring the urgent need for dedicated defense mechanisms for the LoRA-centric sharing ecosystem.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.21977" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.21977.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.21929</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.21929" target="_blank">Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context</a>
      </h3>
      <p class="paper-title-zh">å‡ ä½•å³ä¸Šä¸‹æ–‡ï¼šåœ¨åœºæ™¯ä¸€è‡´è§†é¢‘ç”Ÿæˆä¸­å°†æ˜¾å¼3Dè°ƒåˆ¶ä¸ºå‡ ä½•ä¸Šä¸‹æ–‡</p>
      <p class="paper-authors">JiaKui Hu, Jialun Liu, Liying Yang, Xinliang Zhang, Kaiwen Li ç­‰ (10 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†â€œå‡ ä½•å³ä¸Šä¸‹æ–‡â€æ¡†æ¶ï¼Œé€šè¿‡å°†å‡ ä½•ä¿¡æ¯ä½œä¸ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨åœºæ™¯ä¸€è‡´è§†é¢‘ç”Ÿæˆä¸­å› ä¸­é—´è¯¯å·®ç´¯ç§¯å’Œéå¯å¾®è¿‡ç¨‹å¯¼è‡´çš„ä¸€è‡´æ€§é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é‡‡ç”¨è‡ªå›å½’ç›¸æœºæ§åˆ¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œè¿­ä»£æ‰§è¡Œä¸¤ä¸ªæ­¥éª¤ï¼šä¼°è®¡å½“å‰è§†è§’çš„å‡ ä½•ä¿¡æ¯ä»¥æ”¯æŒ3Dé‡å»ºï¼Œå¹¶æ¨¡æ‹Ÿä¸æ¢å¤ç”±3Dåœºæ™¯æ¸²æŸ“çš„æ–°è§†è§’å›¾åƒã€‚è®¾è®¡äº†ç›¸æœºé—¨æ§æ³¨æ„åŠ›æ¨¡å—ä»¥å¢å¼ºæ¨¡å‹å¯¹ç›¸æœºä½å§¿çš„åˆ©ç”¨èƒ½åŠ›ï¼Œå¹¶åœ¨è®­ç»ƒä¸­é€šè¿‡éšæœºä¸¢å¼ƒå‡ ä½•ä¸Šä¸‹æ–‡çš„æ–¹å¼ï¼Œä½¿æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½ä»…ç”ŸæˆRGBå›¾åƒã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å•å‘å’Œå¾€è¿”ç›¸æœºè½¨è¿¹çš„åœºæ™¯è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šæµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒåœºæ™¯ä¸€è‡´æ€§å’Œç›¸æœºæ§åˆ¶æ–¹é¢ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.21929" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.21929.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.21760</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.21760" target="_blank">Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling</a>
      </h3>
      <p class="paper-title-zh">åŸºäºæ¡ä»¶å¼•å¯¼è°ƒåº¦çš„æ··åˆæ•°æ®-æµæ°´çº¿å¹¶è¡ŒåŠ é€Ÿæ‰©æ•£æ¨¡å‹</p>
      <p class="paper-authors">Euisoo Jung, Byunghyun Kim, Hyunjin Kim, Seonghye Cho, Jae-Gil Lee</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§ç»“åˆæ–°å‹æ•°æ®å¹¶è¡Œç­–ç•¥ï¼ˆåŸºäºæ¡ä»¶çš„åˆ†åŒºï¼‰ä¸æœ€ä¼˜æµæ°´çº¿è°ƒåº¦æ–¹æ³•ï¼ˆè‡ªé€‚åº”å¹¶è¡Œåˆ‡æ¢ï¼‰çš„æ··åˆå¹¶è¡Œæ¡†æ¶ï¼Œåœ¨æ˜¾è‘—é™ä½æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆå»¶è¿Ÿçš„åŒæ—¶ï¼Œä¿æŒäº†é«˜ç”Ÿæˆè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š1ï¼‰åˆ©ç”¨æ¡ä»¶å»å™ªè·¯å¾„å’Œæ— æ¡ä»¶å»å™ªè·¯å¾„ä½œä¸ºæ–°çš„æ•°æ®åˆ†åŒºè§†è§’ï¼›2ï¼‰æ ¹æ®è¿™ä¸¤æ¡è·¯å¾„ä¹‹é—´çš„å»å™ªå·®å¼‚ï¼Œè‡ªé€‚åº”åœ°å¯ç”¨æœ€ä¼˜çš„æµæ°´çº¿å¹¶è¡Œã€‚é€šè¿‡è¿™ç§æ··åˆå¹¶è¡Œç­–ç•¥ï¼Œæœ‰æ•ˆåè°ƒäº†è®¡ç®—èµ„æºã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ä½¿ç”¨ä¸¤å—NVIDIA RTX 3090 GPUï¼Œåœ¨SDXLå’ŒSD3æ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº†2.31å€å’Œ2.07å€çš„å»¶è¿Ÿé™ä½ï¼Œä¸”å›¾åƒè´¨é‡å¾—ä»¥ä¿æŒã€‚è¯¥æ–¹æ³•åœ¨åŸºäºU-Netçš„æ‰©æ•£æ¨¡å‹å’ŒåŸºäºDiTçš„æµåŒ¹é…æ¶æ„ä¸Šå‡è¡¨ç°å‡ºé€šç”¨æ€§ï¼Œå¹¶åœ¨é«˜åˆ†è¾¨ç‡åˆæˆè®¾ç½®ä¸‹çš„åŠ é€Ÿæ•ˆæœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves $2.31\times$ and $2.07\times$ latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.21760" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.21760.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.21698</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.21698" target="_blank">E-comIQ-ZH: A Human-Aligned Dataset and Benchmark for Fine-Grained Evaluation of E-commerce Posters with Chain-of-Thought</a>
      </h3>
      <p class="paper-title-zh">E-comIQ-ZHï¼šä¸€ä¸ªç”¨äºç”µå•†æµ·æŠ¥ç»†ç²’åº¦è¯„ä¼°çš„ã€ç¬¦åˆäººç±»è®¤çŸ¥çš„æ€ç»´é“¾æ•°æ®é›†ä¸åŸºå‡†</p>
      <p class="paper-authors">Meiqi Sun, Mingyu Li, Junxiong Zhu</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†é¦–ä¸ªé¢å‘ä¸­æ–‡ç”µå•†æµ·æŠ¥è´¨é‡è¯„ä¼°çš„æ¡†æ¶E-comIQ-ZHï¼Œå…¶æ ¸å¿ƒè´¡çŒ®æ˜¯æ„å»ºäº†åŒ…å«å¤šç»´è¯„åˆ†å’Œä¸“å®¶æ ¡å‡†æ€ç»´é“¾çš„æ•°æ®é›†E-comIQ-18kï¼Œå¹¶åŸºäºæ­¤è®­ç»ƒäº†ä¸ä¸“å®¶åˆ¤æ–­å¯¹é½çš„ä¸“ç”¨è¯„ä¼°æ¨¡å‹E-comIQ-Mã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ç ”ç©¶é¦–å…ˆæ„å»ºäº†E-comIQ-18kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸ä»…åŒ…å«å¯¹æµ·æŠ¥çš„å¤šç»´åº¦è´¨é‡è¯„åˆ†ï¼Œè¿˜æä¾›äº†ç”±ä¸“å®¶æ ‡æ³¨çš„æ€ç»´é“¾æ¨ç†ä¾æ®ã€‚éšåï¼Œåˆ©ç”¨è¯¥æ•°æ®é›†è®­ç»ƒäº†ä¸€ä¸ªä¸“é—¨çš„è¯„ä¼°æ¨¡å‹E-comIQ-Mï¼Œæ—¨åœ¨ä½¿å…¶åˆ¤æ–­ä¸äººç±»ä¸“å®¶çš„æ ‡å‡†å¯¹é½ã€‚æœ€ç»ˆï¼ŒåŸºäºæ•´ä¸ªæ¡†æ¶å»ºç«‹äº†é¦–ä¸ªè‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„ä¸­æ–‡ç”µå•†æµ·æŠ¥ç”Ÿæˆè¯„ä¼°åŸºå‡†E-comIQ-Benchã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€è®­ç»ƒçš„è¯„ä¼°æ¨¡å‹E-comIQ-Måœ¨åˆ¤æ–­ä¸Šä¸ä¸“å®¶æ ‡å‡†æ›´ä¸ºä¸€è‡´ï¼Œèƒ½å¤Ÿå®ç°ç”µå•†æµ·æŠ¥è´¨é‡çš„å¯æ‰©å±•è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•å¿½è§†ä¸­æ–‡å¤æ‚å­—ç¬¦å¯¼è‡´çš„ç»†å¾®ä½†å…³é”®çš„æ–‡å­—ç‘•ç–µï¼Œä»¥åŠç¼ºä¹ç”µå•†è®¾è®¡åŠŸèƒ½æ ‡å‡†çš„é—®é¢˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Generative AI is widely used to create commercial posters. However, rapid advances in generation have outpaced automated quality assessment. Existing models emphasize generic esthetics or low level distortions and lack the functional criteria required for e-commerce design. It is especially challenging for Chinese content, where complex characters often produce subtle but critical textual artifacts that are overlooked by existing methods. To address this, we introduce E-comIQ-ZH, a framework for evaluating Chinese e-commerce posters. We build the first dataset E-comIQ-18k to feature multi dimensional scores and expert calibrated Chain of Thought (CoT) rationales. Using this dataset, we train E-comIQ-M, a specialized evaluation model that aligns with human expert judgment. Our framework enables E-comIQ-Bench, the first automated and scalable benchmark for the generation of Chinese e-commerce posters. Extensive experiments show our E-comIQ-M aligns more closely with expert standards and enables scalable automated assessment of e-commerce posters. All datasets, models, and evaluation tools will be released to support future research in this area.Code will be available at https://github.com/4mm7/E-comIQ-ZH.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.21698" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.21698.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.21596</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.21596" target="_blank">A Hidden Semantic Bottleneck in Conditional Embeddings of Diffusion Transformers</a>
      </h3>
      <p class="paper-title-zh">æ‰©æ•£å˜æ¢å™¨ä¸­æ¡ä»¶åµŒå…¥çš„éšè—è¯­ä¹‰ç“¶é¢ˆ</p>
      <p class="paper-authors">Trung X. Pham, Kang Zhang, Ji Woo Hong, Chang D. Yoo</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ä¸­æ¡ä»¶åµŒå…¥å­˜åœ¨ä¸¥é‡çš„è¯­ä¹‰å†—ä½™ï¼Œå¹¶å‘ç°è¯­ä¹‰ä¿¡æ¯ä»…é›†ä¸­åœ¨å°‘æ•°ç»´åº¦ä¸­ï¼Œä¸ºè®¾è®¡æ›´é«˜æ•ˆçš„æ¡ä»¶æœºåˆ¶æä¾›äº†æ–°è§è§£ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ç ”ç©¶å¯¹ImageNet-1Kç±»åˆ«æ¡ä»¶ç”Ÿæˆä»¥åŠå§¿æ€å¼•å¯¼å›¾åƒç”Ÿæˆã€è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆç­‰è¿ç»­æ¡ä»¶ä»»åŠ¡ä¸­çš„åµŒå…¥å‘é‡è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æã€‚é€šè¿‡æµ‹é‡åµŒå…¥å‘é‡çš„è§’åº¦ç›¸ä¼¼æ€§å¹¶åˆ†æå„ç»´åº¦çš„è¯­ä¹‰è´¡çŒ®ï¼Œè¯†åˆ«å‡ºé«˜å†—ä½™ç»´åº¦ã€‚è¿›ä¸€æ­¥é€šè¿‡å‰ªæä½å¹…å€¼ç»´åº¦çš„å®éªŒï¼ŒéªŒè¯äº†åµŒå…¥ç©ºé—´çš„å†—ä½™æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> 1. æ¡ä»¶åµŒå…¥å­˜åœ¨æç«¯è§’åº¦ç›¸ä¼¼æ€§ï¼ˆImageNet-1K >99%ï¼Œè¿ç»­ä»»åŠ¡ >99.9%ï¼‰ï¼Œè¡¨æ˜é«˜åº¦å†—ä½™ã€‚2. è¯­ä¹‰ä¿¡æ¯é›†ä¸­åœ¨å¤´éƒ¨å°‘æ•°ç»´åº¦ï¼Œå°¾éƒ¨ç»´åº¦è´¡çŒ®æä½ã€‚3. å‰ªé™¤å¤šè¾¾ä¸‰åˆ†ä¹‹äºŒçš„ä½å¹…å€¼ç»´åº¦åï¼Œç”Ÿæˆè´¨é‡å’Œä¿çœŸåº¦åŸºæœ¬ä¸å—å½±å“ç”šè‡³æœ‰æ‰€æå‡ï¼Œè¯å®äº†è¯­ä¹‰ç“¶é¢ˆçš„å­˜åœ¨ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion Transformers have achieved state-of-the-art performance in class-conditional and multimodal generation, yet the structure of their learned conditional embeddings remains poorly understood. In this work, we present the first systematic study of these embeddings and uncover a notable redundancy: class-conditioned embeddings exhibit extreme angular similarity, exceeding 99\% on ImageNet-1K, while continuous-condition tasks such as pose-guided image generation and video-to-audio generation reach over 99.9\%. We further find that semantic information is concentrated in a small subset of dimensions, with head dimensions carrying the dominant signal and tail dimensions contributing minimally. By pruning low-magnitude dimensions--removing up to two-thirds of the embedding space--we show that generation quality and fidelity remain largely unaffected, and in some cases improve. These results reveal a semantic bottleneck in Transformer-based diffusion models, providing new insights into how semantics are encoded and suggesting opportunities for more efficient conditioning mechanisms.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.21596" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.21596.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>