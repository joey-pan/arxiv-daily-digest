<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-17</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.14186</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.14186" target="_blank">UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing</a>
      </h3>
      <p class="paper-title-zh">UniRef-Image-Editï¼šè¿ˆå‘å¯æ‰©å±•ä¸”ä¸€è‡´çš„å¤šå‚è€ƒå›¾åƒç¼–è¾‘</p>
      <p class="paper-authors">Hongyang Wei, Bin Wen, Yancheng Long, Yankai Yang, Yuhang Hu ç­‰ (25 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç”Ÿæˆç³»ç»ŸUniRef-Image-Editï¼Œå®ƒé¦–æ¬¡å°†å•å›¾åƒç¼–è¾‘å’Œå¤šå›¾åƒåˆæˆä»»åŠ¡æ•´åˆåˆ°å•ä¸€æ¡†æ¶ä¸­ï¼Œå¹¶é€šè¿‡åˆ›æ–°çš„åºåˆ—æ‰©å±•æ½œåœ¨èåˆï¼ˆSELFï¼‰è¡¨ç¤ºå’Œä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤šå‚è€ƒæ¡ä»¶ä¸‹çš„ç”Ÿæˆä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æå‡ºäº†åºåˆ—æ‰©å±•æ½œåœ¨èåˆï¼ˆSELFï¼‰ï¼Œå®ƒå°†å¤šä¸ªå‚è€ƒå›¾åƒåŠ¨æ€åºåˆ—åŒ–ä¸ºä¸€ä¸ªè¿è´¯çš„æ½œåœ¨åºåˆ—ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µæ˜¯ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè”åˆè®­ç»ƒå•å›¾ç¼–è¾‘å’Œå¤šå›¾åˆæˆä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼åºåˆ—é•¿åº¦è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥æé«˜æ€»åƒç´ é¢„ç®—ä»¥å¢å¼ºç»†èŠ‚å’Œä¸€è‡´æ€§ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå¼•å…¥äº†ä¸“ä¸ºå¤šå‚è€ƒç”Ÿæˆè®¾è®¡çš„MSGRPOæ¡†æ¶ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹å¯¹å†²çªè§†è§‰çº¦æŸçš„åè°ƒèƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç»´æŒå¤šå‚è€ƒå›¾åƒä¹‹é—´çš„è·¨å‚è€ƒä¸€è‡´æ€§ï¼Œå¹¶ç”Ÿæˆå…·æœ‰é«˜è§†è§‰ä¿çœŸåº¦çš„ç»“æœã€‚æ¸è¿›å¼è®­ç»ƒç­–ç•¥ä½¿æ¨¡å‹èƒ½é€æ­¥æ•è·æ›´ç²¾ç»†çš„è§†è§‰ç»†èŠ‚ï¼Œè€ŒMSGRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶åˆ™æ˜¾è‘—æå‡äº†ç»„åˆä¸€è‡´æ€§ã€‚ç³»ç»Ÿåœ¨å•å›¾ç¼–è¾‘å’Œå¤šå›¾åˆæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.14186" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.14186.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.14178</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.14178" target="_blank">UniWeTok: An Unified Binary Tokenizer with Codebook Size $\mathit{2^{128}}$ for Unified Multimodal Large Language Model</a>
      </h3>
      <p class="paper-title-zh">UniWeTokï¼šä¸€ç§ç”¨äºç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ã€ç æœ¬è§„æ¨¡ä¸º$\mathit{2^{128}}$çš„ç»Ÿä¸€äºŒè¿›åˆ¶åˆ†è¯å™¨</p>
      <p class="paper-authors">Shaobin Zhuang, Yuang Ai, Jiaming Han, Weijia Mao, Xiaohui Li ç­‰ (15 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†UniWeTokï¼Œä¸€ä¸ªä½¿ç”¨å¤§è§„æ¨¡äºŒè¿›åˆ¶ç æœ¬ï¼ˆ$2^{128}$ï¼‰çš„ç»Ÿä¸€ç¦»æ•£åˆ†è¯å™¨ï¼Œæ—¨åœ¨åŒæ—¶æ»¡è¶³é«˜ä¿çœŸé‡å»ºã€å¤æ‚è¯­ä¹‰æå–å’Œç”Ÿæˆé€‚åº”æ€§è¿™ä¸‰ä¸ªé€šå¸¸ç›¸äº’å†²çªçš„ç›®æ ‡ï¼Œä¸ºç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›è§†è§‰è¡¨ç¤ºã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é‡‡ç”¨å·ç§¯-æ³¨æ„åŠ›æ··åˆæ¶æ„ï¼Œå¹¶å¼•å…¥SigLuæ¿€æ´»å‡½æ•°ä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶è§£å†³ä¼˜åŒ–å†²çªã€‚è®­ç»ƒæ¡†æ¶ä¸Šï¼Œæå‡ºäº†â€œå‰åè’¸é¦â€å’Œâ€œç”Ÿæˆæ„ŸçŸ¥å…ˆéªŒâ€æ¥å¢å¼ºè¯­ä¹‰æå–å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥æé«˜æ¨¡å‹å¯¹ä¸åŒå›¾åƒåˆ†è¾¨ç‡åŠæ„ŸçŸ¥æ•æ„Ÿåœºæ™¯çš„é€‚åº”æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNetä¸Šï¼ŒUniWeTokä»¥æä½çš„è®­ç»ƒè®¡ç®—é‡ï¼ˆ33B tokensï¼‰å–å¾—äº†æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ€§èƒ½ï¼ˆFID 1.38ï¼‰ã€‚åœ¨é€šç”¨é¢†åŸŸï¼Œå®ƒåœ¨å¤šæ¨¡æ€ç†è§£ã€å›¾åƒç”Ÿæˆï¼ˆDPG Score 86.63ï¼‰å’Œç¼–è¾‘ï¼ˆGEdit Overall Score 5.09ï¼‰ç­‰ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šå±•ç°å‡ºé«˜åº¦ç«äº‰åŠ›ï¼Œæ€§èƒ½ä¼˜äºæˆ–åª²ç¾ç°æœ‰å…ˆè¿›æ¨¡å‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.14178" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.14178.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.14157</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.14157" target="_blank">When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance</a>
      </h3>
      <p class="paper-title-zh">å½“æµ‹è¯•æ—¶å¼•å¯¼è¶³å¤Ÿæ—¶ï¼šåŸºäºæ‰©æ•£å¼•å¯¼çš„å¿«é€Ÿå›¾åƒä¸è§†é¢‘ç¼–è¾‘</p>
      <p class="paper-authors">Ahmed Ghorbel, Badr Moufad, Navid Bagheri Shouraki, Alain Oliviero Durmus, Thomas Hirtz ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬è®ºæ–‡è¯æ˜äº†ä»…ä½¿ç”¨æµ‹è¯•æ—¶å¼•å¯¼ï¼ˆæ— éœ€è®­ç»ƒï¼‰å³å¯å®ç°ä¸åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸åª²ç¾ç”šè‡³æ›´ä¼˜çš„å›¾åƒä¸è§†é¢‘ç¼–è¾‘æ€§èƒ½ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æè§£é‡Šäº†å…¶æœ‰æ•ˆæ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†æ–‡æœ¬é©±åŠ¨çš„å›¾åƒä¸è§†é¢‘ç¼–è¾‘æ„å»ºä¸ºä¿®å¤é—®é¢˜ï¼Œåˆ©ç”¨æ‰©æ•£æˆ–æµæ¨¡å‹çš„æµ‹è¯•æ—¶å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡æ©ç åŒºåŸŸé‡å»ºæ¥ä¿æŒä¸åŸå§‹å†…å®¹åŠç¼–è¾‘æç¤ºçš„ä¸€è‡´æ€§ã€‚ç ”ç©¶åŸºäºMoufadç­‰äººï¼ˆ2025ï¼‰çš„å·¥ä½œï¼Œå¯¹å…¶æ— éœ€å‘é‡-é›…å¯æ¯”ç§¯è®¡ç®—çš„è¿‘ä¼¼æ–¹æ³•æä¾›äº†ç†è®ºè§£é‡Šï¼Œå¹¶å¤§å¹…æ‰©å±•äº†å…¶åœ¨å¤§è§„æ¨¡å›¾åƒä¸è§†é¢‘ç¼–è¾‘åŸºå‡†ä¸Šçš„å®è¯è¯„ä¼°ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä¾èµ–æµ‹è¯•æ—¶å¼•å¯¼çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¯è¾¾åˆ°ä¸åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸å½“çš„æ°´å¹³ï¼Œéƒ¨åˆ†æƒ…å†µä¸‹ç”šè‡³æ›´ä¼˜ï¼ŒåŒæ—¶é¿å…äº†æ˜‚è´µçš„å‘é‡-é›…å¯æ¯”ç§¯è®¡ç®—ï¼Œæå‡äº†å®é™…åº”ç”¨æ•ˆç‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.14157" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.14157.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.14068</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.14068" target="_blank">CoCoEdit: Content-Consistent Image Editing via Region Regularized Reinforcement Learning</a>
      </h3>
      <p class="paper-title-zh">CoCoEditï¼šé€šè¿‡åŒºåŸŸæ­£åˆ™åŒ–å¼ºåŒ–å­¦ä¹ å®ç°å†…å®¹ä¸€è‡´çš„å›¾åƒç¼–è¾‘</p>
      <p class="paper-authors">Yuhui Wu, Chenxi Xie, Ruibin Li, Liyi Chen, Qiaosi Yi ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåè®­ç»ƒæ¡†æ¶CoCoEditï¼Œé€šè¿‡åŒºåŸŸæ­£åˆ™åŒ–å¼ºåŒ–å­¦ä¹ è§£å†³å›¾åƒç¼–è¾‘ä¸­éç¼–è¾‘åŒºåŸŸå†…å®¹ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œåœ¨ä¿è¯ç¼–è¾‘è´¨é‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†éç›®æ ‡åŒºåŸŸçš„ä¸å¿…è¦æ”¹å˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œé€šè¿‡ç²¾ç‚¼æŒ‡ä»¤å’Œæ©ç æ‰©å¢ç°æœ‰ç¼–è¾‘æ•°æ®é›†ï¼Œæ„å»ºäº†åŒ…å«4ä¸‡ä¸ªé«˜è´¨é‡æ ·æœ¬çš„è®­ç»ƒé›†ã€‚å…¶æ¬¡ï¼Œå¼•å…¥åƒç´ çº§ç›¸ä¼¼æ€§å¥–åŠ±æ¥è¡¥å……åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¥–åŠ±ï¼Œä»¥åŒæ—¶ä¼˜åŒ–ç¼–è¾‘è´¨é‡å’Œå†…å®¹ä¸€è‡´æ€§ã€‚æœ€åï¼Œæå‡ºä¸€ç§åŸºäºåŒºåŸŸçš„æ­£åˆ™åŒ–å™¨ï¼Œé’ˆå¯¹é«˜å¥–åŠ±æ ·æœ¬ä¿æŠ¤éç¼–è¾‘åŒºåŸŸï¼Œé’ˆå¯¹ä½å¥–åŠ±æ ·æœ¬é¼“åŠ±ç¼–è¾‘æ•ˆæœï¼Œä»¥å…‹æœå¥–åŠ±ç©ºé—´ä¸æ•æ„Ÿçš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨Qwen-Image-Editå’ŒFLUX-Kontextæ¨¡å‹ä¸Šåº”ç”¨CoCoEditåï¼Œä¸ä»…è·å¾—äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„ç¼–è¾‘è¯„åˆ†ï¼Œè€Œä¸”åœ¨PSNR/SSIMæŒ‡æ ‡å’Œäººç±»ä¸»è§‚è¯„åˆ†ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—æ›´å¥½çš„å†…å®¹ä¸€è‡´æ€§ã€‚é€šè¿‡ä¸ºGEdit-Benchå’ŒImgEdit-Benchæ ‡æ³¨ç¼–è¾‘æ©ç å¹¶å¼•å…¥åƒç´ çº§ç›¸ä¼¼æ€§æŒ‡æ ‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¿æŒéç¼–è¾‘åŒºåŸŸå†…å®¹ä¸€è‡´æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Image editing has achieved impressive results with the development of large-scale generative models. However, existing models mainly focus on the editing effects of intended objects and regions, often leading to unwanted changes in unintended regions. We present a post-training framework for Content-Consistent Editing (CoCoEdit) via region regularized reinforcement learning. We first augment existing editing datasets with refined instructions and masks, from which 40K diverse and high quality samples are curated as training set. We then introduce a pixel-level similarity reward to complement MLLM-based rewards, enabling models to ensure both editing quality and content consistency during the editing process. To overcome the spatial-agnostic nature of the rewards, we propose a region-based regularizer, aiming to preserve non-edited regions for high-reward samples while encouraging editing effects for low-reward samples. For evaluation, we annotate editing masks for GEdit-Bench and ImgEdit-Bench, introducing pixel-level similarity metrics to measure content consistency and editing quality. Applying CoCoEdit to Qwen-Image-Edit and FLUX-Kontext, we achieve not only competitive editing scores with state-of-the-art models, but also significantly better content consistency, measured by PSNR/SSIM metrics and human subjective ratings.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.14068" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.14068.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.14041</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.14041" target="_blank">BitDance: Scaling Autoregressive Generative Models with Binary Tokens</a>
      </h3>
      <p class="paper-title-zh">BitDanceï¼šä½¿ç”¨äºŒè¿›åˆ¶ä»¤ç‰Œæ‰©å±•è‡ªå›å½’ç”Ÿæˆæ¨¡å‹</p>
      <p class="paper-authors">Yuang Ai, Jiaming Han, Shaobin Zhuang, Weijia Mao, Xuefeng Hu ç­‰ (10 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºBitDanceï¼Œä¸€ç§é€šè¿‡é¢„æµ‹äºŒè¿›åˆ¶è§†è§‰ä»¤ç‰Œè€Œéç æœ¬ç´¢å¼•çš„å¯æ‰©å±•è‡ªå›å½’å›¾åƒç”Ÿæˆå™¨ï¼Œå¹¶å¼•å…¥â€œä¸‹ä¸€å—æ‰©æ•£â€è§£ç æ–¹æ³•ï¼Œåœ¨ä¿æŒé«˜å›¾åƒè´¨é‡çš„åŒæ—¶å¤§å¹…æå‡äº†æ¨ç†é€Ÿåº¦ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•ä½¿ç”¨é«˜ç†µäºŒè¿›åˆ¶æ½œåœ¨è¡¨ç¤ºï¼Œä½¿æ¯ä¸ªä»¤ç‰Œèƒ½ç¼–ç å¤šè¾¾2^256ç§çŠ¶æ€ã€‚ä¸ºä»å¦‚æ­¤å·¨å¤§çš„ç¦»æ•£ç©ºé—´ä¸­é‡‡æ ·ï¼Œå®ƒé‡‡ç”¨äºŒè¿›åˆ¶æ‰©æ•£å¤´ï¼Œä»¥è¿ç»­ç©ºé—´æ‰©æ•£ç”ŸæˆäºŒè¿›åˆ¶ä»¤ç‰Œï¼Œæ›¿ä»£ä¼ ç»Ÿçš„softmaxåˆ†ç±»é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæå‡ºçš„ä¸‹ä¸€å—æ‰©æ•£è§£ç æ–¹æ³•èƒ½å¹¶è¡Œé¢„æµ‹å¤šä¸ªä»¤ç‰Œï¼Œæ˜¾è‘—åŠ é€Ÿæ¨ç†ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNet 256x256ä¸Šï¼ŒBitDanceå–å¾—äº†FID 1.24çš„æˆç»©ï¼Œä¸ºè‡ªå›å½’æ¨¡å‹ä¸­çš„æœ€ä½³ç»“æœï¼›ä½¿ç”¨ä»…2.6äº¿å‚æ•°ï¼Œåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†14äº¿å‚æ•°çš„å¹¶è¡Œè‡ªå›å½’æ¨¡å‹ï¼Œæ¨ç†é€Ÿåº¦æå‡8.7å€ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œç”Ÿæˆ1024x1024å›¾åƒæ—¶ç›¸æ¯”å…ˆå‰è‡ªå›å½’æ¨¡å‹åŠ é€Ÿè¶…è¿‡30å€ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ä¸å¯æ‰©å±•æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.14041" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.14041.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>