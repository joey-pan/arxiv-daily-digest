<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-06</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | æœºå™¨å­¦ä¹ : 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">æœºå™¨å­¦ä¹ </span>
          <span class="paper-id">2602.05605</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05605" target="_blank">Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers</a>
      </h3>
      <p class="paper-title-zh">Shiva-DiTï¼šåŸºäºæ®‹å·®çš„å¯å¾®åˆ†Top-ké€‰æ‹©ç”¨äºé«˜æ•ˆæ‰©æ•£Transformer</p>
      <p class="paper-authors">Jiaji Zhang, Hailiang Zhao, Guoxuan Zhu, Ruichao Sun, Jiaju Wu ç­‰ (12 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åä¸ºShiva-DiTçš„æ–¹æ³•ï¼Œé€šè¿‡åŸºäºæ®‹å·®çš„å¯å¾®åˆ†Top-ké€‰æ‹©ï¼Œåœ¨æ»¡è¶³ç¡¬ä»¶ä¸¥æ ¼é™æ€é¢„ç®—çš„åŒæ—¶ï¼Œå…¼é¡¾äº†å¯å¾®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œè§£å†³äº†æ‰©æ•£Transformerå› è‡ªæ³¨æ„åŠ›äºŒæ¬¡å¤æ‚åº¦å¯¼è‡´çš„è®¡ç®—æˆæœ¬è¿‡é«˜é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åˆ©ç”¨æ®‹å·®æ„ŸçŸ¥çš„ç›´é€šä¼°è®¡å™¨ï¼Œåœ¨ä¿æŒç«¯åˆ°ç«¯å¯å­¦ä¹ æ€§çš„åŒæ—¶ï¼Œå¼ºåˆ¶å®ç°ç¡®å®šæ€§çš„ä»¤ç‰Œæ•°é‡ä»¥æ”¯æŒé™æ€ç¼–è¯‘ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥è·¯ç”±å™¨å’Œè‡ªé€‚åº”æ¯”ç‡ç­–ç•¥ï¼Œä»¥è‡ªä¸»å­¦ä¹ è‡ªé€‚åº”çš„å‰ªæè°ƒåº¦ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨åŒ…æ‹¬SD3.5åœ¨å†…çš„ä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒShiva-DiTå»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œç›¸æ¯”ç°æœ‰åŸºçº¿å®ç°äº†1.54å€çš„å®æ—¶åŠ é€Ÿï¼Œå¹¶å…·æœ‰æ›´ä¼˜çš„ä¿çœŸåº¦ï¼ŒåŒæ—¶æœ‰æ•ˆæ¶ˆé™¤äº†ä¸è§„åˆ™å¼ é‡çš„å¼€é”€ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05605" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05605.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05534</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05534" target="_blank">SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation</a>
      </h3>
      <p class="paper-title-zh">SSGï¼šç”¨äºå¤šå°ºåº¦è§†è§‰è‡ªå›å½’ç”Ÿæˆçš„æ¯”ä¾‹åŒ–ç©ºé—´å¼•å¯¼</p>
      <p class="paper-authors">Youngwoo Shin, Jiwan Hur, Junmo Kim</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€åœ¨æ¨ç†æ—¶ä½¿ç”¨çš„æ¯”ä¾‹åŒ–ç©ºé—´å¼•å¯¼ï¼ˆSSGï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼ºè°ƒç›®æ ‡é«˜é¢‘ä¿¡å·ï¼ˆè¯­ä¹‰æ®‹å·®ï¼‰æ¥çº æ­£è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨æ¨ç†æ—¶å¯èƒ½å‡ºç°çš„å±‚æ¬¡ç»“æ„æ¼‚ç§»é—®é¢˜ï¼Œä»è€Œæå‡ç”Ÿæˆè´¨é‡ä¸å¤šæ ·æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ä»ä¿¡æ¯è®ºè§’åº¦åˆ†æï¼Œæå‡ºç¡®ä¿æ¯ä¸ªå°ºåº¦è´¡çŒ®å…ˆå‰å°ºåº¦æœªè§£é‡Šçš„é«˜é¢‘å†…å®¹å¯ç¼“è§£è®­ç»ƒ-æ¨ç†å·®å¼‚ã€‚SSGé€šè¿‡ç¦»æ•£ç©ºé—´å¢å¼ºï¼ˆDSEï¼‰è¿™ä¸€é¢‘åŸŸå¤„ç†æ­¥éª¤ï¼Œä»ç²—ç²’åº¦å…ˆéªŒä¸­åˆ†ç¦»å‡ºè¯­ä¹‰æ®‹å·®ä½œä¸ºå¼•å¯¼ä¿¡å·ï¼Œåœ¨æ¨ç†æ—¶åŠ¨æ€å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä¿æŒç”±ç²—åˆ°ç»†çš„å±‚æ¬¡ç»“æ„ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä»»ä½•åŸºäºç¦»æ•£è§†è§‰æ ‡è®°çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œä¸æ ‡è®°åŒ–è®¾è®¡æˆ–æ¡ä»¶æ¨¡æ€æ— å…³ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒSSGèƒ½ä¸€è‡´åœ°æå‡ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒä½å»¶è¿Ÿï¼Œæ­ç¤ºäº†ç”±ç²—åˆ°ç»†å›¾åƒç”Ÿæˆä¸­å°šæœªå¼€å‘çš„æ•ˆç‡æ½œåŠ›ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œåœ¨å¤šä¸ªVARæ¨¡å‹ä¸Šå‡æœ‰æ•ˆã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05534" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05534.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05435</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05435" target="_blank">Stable Velocity: A Variance Perspective on Flow Matching</a>
      </h3>
      <p class="paper-title-zh">ç¨³å®šé€Ÿåº¦ï¼šä»æ–¹å·®è§†è§’çœ‹æµåŒ¹é…</p>
      <p class="paper-authors">Donglin Yang, Yongxing Zhang, Xin Yu, Liang Hou, Xin Tao ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡é€šè¿‡æ˜¾å¼åˆ†ææµåŒ¹é…è®­ç»ƒç›®æ ‡çš„é«˜æ–¹å·®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„â€œç¨³å®šé€Ÿåº¦â€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶æ”¹è¿›äº†è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œé‡‡æ ·é€Ÿåº¦ï¼Œä¸”ä¸ç‰ºç‰²æ ·æœ¬è´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è®ºæ–‡é¦–å…ˆä»ç†è®ºä¸Šåˆ†æäº†æ¡ä»¶é€Ÿåº¦çš„æ–¹å·®ï¼Œè¯†åˆ«å‡ºé è¿‘å…ˆéªŒåˆ†å¸ƒçš„é«˜æ–¹å·®åŒºåŸŸå’Œé è¿‘æ•°æ®åˆ†å¸ƒçš„ä½æ–¹å·®åŒºåŸŸã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼š1) ç”¨äºè®­ç»ƒçš„â€œç¨³å®šé€Ÿåº¦åŒ¹é…â€ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— åçš„æ–¹å·®ç¼©å‡ç›®æ ‡ï¼›2) â€œæ–¹å·®æ„ŸçŸ¥è¡¨ç¤ºå¯¹é½â€ï¼Œå®ƒåœ¨ä½æ–¹å·®åŒºåŸŸè‡ªé€‚åº”åœ°å¢å¼ºè¾…åŠ©ç›‘ç£ã€‚å¯¹äºæ¨ç†ï¼Œåˆ™æå‡ºäº†â€œç¨³å®šé€Ÿåº¦é‡‡æ ·â€ï¼Œåˆ©ç”¨ä½æ–¹å·®åŒºåŸŸåŠ¨åŠ›å­¦å¯é—­å¼ç®€åŒ–çš„ç‰¹æ€§æ¥åŠ é€Ÿé‡‡æ ·ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNet 256Ã—256ä»¥åŠSD3.5ã€Fluxã€Qwen-Imageã€Wan2.2ç­‰å¤§å‹é¢„è®­ç»ƒæ–‡ç”Ÿå›¾ã€æ–‡ç”Ÿè§†é¢‘æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡è®­ç»ƒæ•ˆç‡ï¼Œå¹¶åœ¨ä½æ–¹å·®åŒºåŸŸå†…å®ç°è¶…è¿‡2å€çš„é‡‡æ ·åŠ é€Ÿï¼Œä¸”æ ·æœ¬è´¨é‡æ²¡æœ‰ä¸‹é™ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05435" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05435.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05380</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05380" target="_blank">SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback</a>
      </h3>
      <p class="paper-title-zh">SAILï¼šåŸºäºæœ€å°‘äººç±»åé¦ˆçš„æ‰©æ•£æ¨¡å‹å¯¹é½çš„è‡ªæ”¾å¤§è¿­ä»£å­¦ä¹ </p>
      <p class="paper-authors">Xiaoxuan He, Siming Fu, Wanli Li, Zhiyuan Li, Dacheng Yin ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºSAILæ¡†æ¶ï¼Œä»…éœ€æå°‘äººå·¥æ ‡æ³¨åå¥½å¯¹ï¼Œå³å¯é€šè¿‡è¿­ä»£è‡ªå­¦ä¹ å®ç°æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½ï¼Œæ— éœ€å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> SAILé‡‡ç”¨é—­ç¯è‡ªå­¦ä¹ æœºåˆ¶ï¼šä»å°‘é‡äººå·¥æ ‡æ³¨åå¥½å¯¹å‡ºå‘ï¼Œæ¨¡å‹è¿­ä»£ç”Ÿæˆå¤šæ ·æ ·æœ¬ï¼ŒåŸºäºè‡ªèº«æ¼”åŒ–å‡ºçš„ç†è§£è¿›è¡Œè‡ªæ ‡æ³¨åå¥½ï¼Œå¹¶åˆ©ç”¨è‡ªå¢å¼ºæ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡å¼•å…¥æ’åºåå¥½æ··åˆç­–ç•¥ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆå§‹äººç±»å…ˆéªŒçš„éµå¾ªï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒSAILåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”ä»…éœ€ç°æœ‰æ–¹æ³•çº¦6%çš„åå¥½æ•°æ®é‡ï¼Œè¯æ˜æ‰©æ•£æ¨¡å‹å…·æœ‰æ˜¾è‘—çš„è‡ªæ”¹è¿›èƒ½åŠ›ï¼Œå¯æœ‰æ•ˆæ›¿ä»£å¤§è§„æ¨¡äººå·¥æ ‡æ³¨å’Œå¤–éƒ¨å¥–åŠ±æ¨¡å‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \textbf{SAIL} (\textbf{S}elf-\textbf{A}mplified \textbf{I}terative \textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05380" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05380.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.05362</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.05362" target="_blank">Imagine a City: CityGenAgent for Procedural 3D City Generation</a>
      </h3>
      <p class="paper-title-zh">æƒ³è±¡ä¸€åº§åŸå¸‚ï¼šç”¨äºç¨‹åºåŒ–3DåŸå¸‚ç”Ÿæˆçš„CityGenAgent</p>
      <p class="paper-authors">Zishan Liu, Zecong Tang, RuoCheng Wu, Xinzhe Zheng, Jingyu Hu ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†CityGenAgentï¼Œä¸€ä¸ªç”±è‡ªç„¶è¯­è¨€é©±åŠ¨çš„åˆ†å±‚ç¨‹åºåŒ–ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€å¯äº¤äº’çš„3DåŸå¸‚ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„è¯­ä¹‰å¯¹é½æ€§ã€è§†è§‰è´¨é‡å’Œç”¨æˆ·å¯æ§æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†åŸå¸‚ç”Ÿæˆåˆ†è§£ä¸ºä¸¤ä¸ªå¯è§£é‡Šçš„ç»„ä»¶ï¼šåŒºå—ç¨‹åºï¼ˆBlock Programï¼‰å’Œå»ºç­‘ç¨‹åºï¼ˆBuilding Programï¼‰ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼šé¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒæ¨¡å‹ç”Ÿæˆç¬¦åˆæ¨¡å¼çº¦æŸçš„æœ‰æ•ˆç¨‹åºï¼›ç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œé€šè¿‡è®¾è®¡ç©ºé—´å¯¹é½å¥–åŠ±å’Œè§†è§‰ä¸€è‡´æ€§å¥–åŠ±ï¼Œå¢å¼ºæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›å¹¶å¼¥åˆæ–‡æœ¬æè¿°ä¸è§†è§‰æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒCityGenAgentåœ¨è¯­ä¹‰å¯¹é½ã€è§†è§‰è´¨é‡å’Œå¯æ§æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å¾—ç›Šäºç¨‹åºåŒ–ç”Ÿæˆå’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯¥æ¡†æ¶æ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€å¯¹åŸå¸‚è¿›è¡Œç¼–è¾‘å’Œæ“ä½œï¼Œä¸ºå¯æ‰©å±•çš„3DåŸå¸‚ç”Ÿæˆå¥ å®šäº†åšå®åŸºç¡€ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.05362" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.05362.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>