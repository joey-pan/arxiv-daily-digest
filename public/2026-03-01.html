<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-03-01</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | æœºå™¨å­¦ä¹ : 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.22570</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22570" target="_blank">Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation</a>
      </h3>
      <p class="paper-title-zh">å¼•å¯¼è‡³å…³é‡è¦ï¼šé‡æ–°æ€è€ƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è¯„ä¼°é™·é˜±</p>
      <p class="paper-authors">Dian Xie, Shitong Shao, Lichen Bai, Zikai Zhou, Bojun Cheng ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æ­ç¤ºäº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¯„ä¼°ä¸­å­˜åœ¨çš„ä¸¥é‡åè§â€”â€”äººç±»åå¥½æ¨¡å‹å¯¹é«˜å¼•å¯¼å°ºåº¦å­˜åœ¨ç³»ç»Ÿæ€§åå¥½ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¼•å¯¼æ„ŸçŸ¥è¯„ä¼°æ¡†æ¶ï¼ˆGA-Evalï¼‰ä»¥å®ç°å…¬å¹³æ¯”è¾ƒã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œä½œè€…é€šè¿‡å®éªŒè¯æ˜ï¼Œä»…å¢åŠ æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰çš„å°ºåº¦å³å¯å› è¯­ä¹‰å¯¹é½å¢å¼ºè€Œæ˜¾è‘—æå‡é‡åŒ–è¯„åˆ†ï¼Œå³ä½¿å›¾åƒè´¨é‡å·²ä¸¥é‡å—æŸã€‚å…¶æ¬¡ï¼Œä»–ä»¬æå‡ºäº†GA-Evalæ¡†æ¶ï¼Œé€šè¿‡å¼•å¯¼å°ºåº¦æ ¡å‡†æ¥åˆ†ç¦»ä¸CFGæ­£äº¤å’Œå¹³è¡Œçš„æ•ˆåº”ï¼Œä»è€Œå…¬å¹³æ¯”è¾ƒä¸åŒå¼•å¯¼æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸ºéªŒè¯è¯„ä¼°é™·é˜±ï¼Œä½œè€…è¿˜è®¾è®¡äº†ä¸€ç§åœ¨å®è·µä¸­æ— æ•ˆä½†èƒ½åœ¨ä¼ ç»Ÿè¯„ä¼°ä¸­å–å¾—é«˜åˆ†çš„â€œè¶…è¶Šæ€§æ‰©æ•£å¼•å¯¼â€ï¼ˆTDGï¼‰æ–¹æ³•ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å…³é”®å‘ç°åŒ…æ‹¬ï¼š1ï¼‰ä¼ ç»Ÿè¯„ä¼°æ¡†æ¶å­˜åœ¨ä¸¥é‡åè§ï¼Œè¿‡åº¦åå¥½é«˜å¼•å¯¼å°ºåº¦ï¼›2ï¼‰åœ¨å…¬å¹³çš„GA-Evalè¯„ä¼°ä¸‹ï¼Œä»…å¢åŠ CFGå°ºåº¦å³å¯ä¸å¤§å¤šæ•°æ–°å…´å¼•å¯¼æ–¹æ³•ç«äº‰ï¼›3ï¼‰æ‰€æœ‰è¢«ç ”ç©¶çš„å¼•å¯¼æ–¹æ³•ç›¸å¯¹äºæ ‡å‡†CFGçš„èƒœç‡å‡æ˜¾è‘—ä¸‹é™ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“å‰è®¸å¤šå£°ç§°çš„æ”¹è¿›å¯èƒ½æºäºè¯„ä¼°åå·®è€Œéå®é™…æ€§èƒ½æå‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid and significant improvements? In this paper, we rethink recent progress on diffusion guidance. Our work mainly consists of four contributions. First, we reveal a critical evaluation pitfall that common human preference models exhibit a strong bias towards large guidance scales. Simply increasing the CFG scale can easily improve quantitative evaluation scores due to strong semantic alignment, even if image quality is severely damaged (e.g., oversaturation and artifacts). Second, we introduce a novel guidance-aware evaluation (GA-Eval) framework that employs effective guidance scale calibration to enable fair comparison between current guidance methods and CFG by identifying the effects orthogonal and parallel to CFG effects. Third, motivated by the evaluation pitfall, we design Transcendent Diffusion Guidance (TDG) method that can significantly improve human preference scores in the conventional evaluation framework but actually does not work in practice. Fourth, in extensive experiments, we empirically evaluate recent eight diffusion guidance methods within the conventional evaluation framework and the proposed GA-Eval framework. Notably, simply increasing the CFG scales can compete with most studied diffusion guidance methods, while all methods suffer severely from winning rate degradation over standard CFG. Our work would strongly motivate the community to rethink the evaluation paradigm and future directions of this field.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22570" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22570.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">æœºå™¨å­¦ä¹ </span>
          <span class="paper-id">2602.22507</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22507" target="_blank">Space Syntax-guided Post-training for Residential Floor Plan Generation</a>
      </h3>
      <p class="paper-title-zh">ç©ºé—´å¥æ³•å¼•å¯¼çš„åè®­ç»ƒç”¨äºä½å®…å¹³é¢å›¾ç”Ÿæˆ</p>
      <p class="paper-authors">Zhuoyang Jiang, Dongqing Zhang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§ç©ºé—´å¥æ³•å¼•å¯¼çš„åè®­ç»ƒèŒƒå¼ï¼Œå°†å»ºç­‘å­¦ä¸­çš„ç©ºé—´é…ç½®ä¸è¿æ¥æ€§å…ˆéªŒçŸ¥è¯†æ˜¾å¼æ³¨å…¥ç”Ÿæˆæ¨¡å‹ï¼Œä»¥æ”¹å–„ä½å®…å¹³é¢å›¾ä¸­å…¬å…±ç©ºé—´çš„ä¸»å¯¼æ€§ä¸åŠŸèƒ½å±‚æ¬¡ç»“æ„ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡ä¸€ä¸ªä¸å¯å¾®çš„â€œé¢„è¨€æœºâ€å°†å¹³é¢å¸ƒå±€è½¬æ¢ä¸ºçŸ©å½¢ç©ºé—´å›¾å¹¶è®¡ç®—ç©ºé—´æ•´åˆåº¦æŒ‡æ ‡ï¼Œä»¥é‡åŒ–å…¬å…±ç©ºé—´ä¸»å¯¼æ€§ã€‚å…·ä½“é‡‡ç”¨ä¸¤ç§åè®­ç»ƒç­–ç•¥ï¼šä¸€æ˜¯åŸºäºç©ºé—´å¥æ³•ç­›é€‰çš„è¿­ä»£é‡è®­ç»ƒä¸æ‰©æ•£æ¨¡å‹å¾®è°ƒï¼›äºŒæ˜¯ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä»¥ç©ºé—´å¥æ³•æŒ‡æ ‡ä½œä¸ºå¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œä¸¤ç§ç­–ç•¥å‡èƒ½æœ‰æ•ˆæå‡ç”Ÿæˆå¹³é¢ä¸­å…¬å…±ç©ºé—´çš„ä¸»å¯¼æ€§å¹¶æ¢å¤æ›´æ¸…æ™°çš„åŠŸèƒ½å±‚æ¬¡ï¼Œå…¶ä¸­å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨è®¡ç®—æ•ˆç‡å’Œæ•ˆæœç¨³å®šæ€§ä¸Šè¡¨ç°æ›´ä¼˜ã€‚æ‰€æå‡ºçš„åè®­ç»ƒèŒƒå¼ä¸ºå°†å»ºç­‘ç†è®ºèå…¥æ•°æ®é©±åŠ¨çš„å¹³é¢ç”Ÿæˆæä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Pre-trained generative models for residential floor plans are typically optimized to fit large-scale data distributions, which can under-emphasize critical architectural priors such as the configurational dominance and connectivity of domestic public spaces (e.g., living rooms and foyers). This paper proposes Space Syntax-guided Post-training (SSPT), a post-training paradigm that explicitly injects space syntax knowledge into floor plan generation via a non-differentiable oracle. The oracle converts RPLAN-style layouts into rectangle-space graphs through greedy maximal-rectangle decomposition and door-mediated adjacency construction, and then computes integration-based measurements to quantify public space dominance and functional hierarchy. To enable consistent evaluation and diagnosis, we further introduce SSPT-Bench (Eval-8), an out-of-distribution benchmark that post-trains models using conditions capped at $\leq 7$ rooms while evaluating on 8-room programs, together with a unified metric suite for dominance, stability, and profile alignment. SSPT is instantiated with two strategies: (i) iterative retraining via space-syntax filtering and diffusion fine-tuning, and (ii) reinforcement learning via PPO with space-syntax rewards. Experiments show that both strategies improve public-space dominance and restore clearer functional hierarchy compared to distribution-fitted baselines, while PPO achieves stronger gains with substantially higher compute efficiency and reduced variance. SSPT provides a scalable pathway for integrating architectural theory into data-driven plan generation and is compatible with other generative backbones given a post-hoc evaluation oracle.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22507" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22507.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.22150</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.22150" target="_blank">CoLoGen: Progressive Learning of Concept-Localization Duality for Unified Image Generation</a>
      </h3>
      <p class="paper-title-zh">CoLoGenï¼šæ¸è¿›å¼å­¦ä¹ æ¦‚å¿µ-å®šä½å¯¹å¶æ€§çš„ç»Ÿä¸€å›¾åƒç”Ÿæˆæ–¹æ³•</p>
      <p class="paper-authors">YuXin Song, Yu Lu, Haoyuan Sun, Huanjin Yao, Fanglong Liu ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†CoLoGenï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç»Ÿä¸€æ¡ä»¶å›¾åƒç”Ÿæˆä¸­æ¦‚å¿µç†è§£ä¸ç©ºé—´å®šä½ä¹‹é—´çš„è¡¨å¾å†²çªé—®é¢˜ã€‚å…¶ä¸»è¦è´¡çŒ®æ˜¯é€šè¿‡æ¸è¿›å¼å­¦ä¹ ç­–ç•¥ï¼Œåè°ƒå¹¶èåˆè¿™ä¸¤ç§å¼‚è´¨çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä¸ºç»Ÿä¸€å›¾åƒç”Ÿæˆæä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„è¡¨å¾è§†è§’ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> CoLoGené‡‡ç”¨åˆ†é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼šé¦–å…ˆåˆ†åˆ«æ„å»ºæ ¸å¿ƒçš„æ¦‚å¿µç†è§£èƒ½åŠ›å’Œç©ºé—´å®šä½èƒ½åŠ›ï¼›ç„¶åä½¿è¿™äº›èƒ½åŠ›é€‚åº”å¤šæ ·çš„è§†è§‰æ¡ä»¶ï¼›æœ€ååœ¨å¤æ‚æŒ‡ä»¤é©±åŠ¨ä»»åŠ¡ä¸­ç²¾ç»†åŒ–å®ƒä»¬çš„ååŒä½œç”¨ã€‚å…¶æ ¸å¿ƒæ˜¯æ¸è¿›å¼è¡¨å¾ç¼–ç»‡ï¼ˆPRWï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½åŠ¨æ€åœ°å°†ç‰¹å¾è·¯ç”±åˆ°ä¸“é—¨çš„ä¸“å®¶ç½‘ç»œï¼Œå¹¶åœ¨ä¸åŒé˜¶æ®µç¨³å®šåœ°æ•´åˆå®ƒä»¬çš„è¾“å‡ºã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å›¾åƒç¼–è¾‘ã€å¯æ§ç”Ÿæˆå’Œå®šåˆ¶åŒ–ç”Ÿæˆç­‰å¤šä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoLoGenå–å¾—äº†å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚è¿™éªŒè¯äº†é€šè¿‡æ¸è¿›å¼åè°ƒæ¦‚å¿µä¸å®šä½å¯¹å¶æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£è¡¨å¾å†²çªï¼Œä»è€Œå®ç°æ›´å¼ºå¤§ã€æ›´ç»Ÿä¸€çš„å›¾åƒç”Ÿæˆã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Unified conditional image generation remains difficult because different tasks depend on fundamentally different internal representations. Some require conceptual understanding for semantic synthesis, while others rely on localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept-localization representational conflict. To address this issue, we propose CoLoGen, a unified diffusion framework that progressively learns and reconciles this concept-localization duality. CoLoGen uses a staged curriculum that first builds core conceptual and localization abilities, then adapts them to diverse visual conditions, and finally refines their synergy for complex instruction-driven tasks. Central to this process is the Progressive Representation Weaving (PRW) module, which dynamically routes features to specialized experts and stably integrates their outputs across stages. Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance, offering a principled representational perspective for unified image generation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.22150" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.22150.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.21591</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.21591" target="_blank">CADC: Content Adaptive Diffusion-Based Generative Image Compression</a>
      </h3>
      <p class="paper-title-zh">CADCï¼šåŸºäºå†…å®¹è‡ªé€‚åº”æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼å›¾åƒå‹ç¼©</p>
      <p class="paper-authors">Xihua Sheng, Lingyu Zhu, Tianyu Zhang, Dong Liu, Shiqi Wang ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§å†…å®¹è‡ªé€‚åº”çš„æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è§£ç å™¨ï¼Œé€šè¿‡ä¸‰é¡¹æŠ€æœ¯åˆ›æ–°è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨é‡åŒ–ã€ä¿¡æ¯é›†ä¸­å’Œè¯­ä¹‰å¼•å¯¼æ–¹é¢çš„é€‚åº”æ€§ä¸è¶³é—®é¢˜ï¼Œå®ç°äº†åœ¨æä½ç ç‡ä¸‹æ›´é«˜è´¨é‡çš„é‡å»ºã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1) æå‡ºä¸ç¡®å®šæ€§å¼•å¯¼çš„è‡ªé€‚åº”é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ ç©ºé—´ä¸ç¡®å®šæ€§å›¾ä½¿é‡åŒ–å¤±çœŸä¸å›¾åƒå†…å®¹ç‰¹å¾è‡ªé€‚åº”å¯¹é½ï¼›2) è®¾è®¡è¾…åŠ©è§£ç å™¨å¼•å¯¼çš„ä¿¡æ¯é›†ä¸­æ–¹æ³•ï¼Œåˆ©ç”¨è½»é‡çº§è¾…åŠ©è§£ç å™¨ç¡®ä¿ä¸»è¦æ½œåœ¨é€šé“ä¸­å†…å®¹æ„ŸçŸ¥çš„ä¿¡æ¯ä¿ç•™ï¼›3) å¼€å‘å…ç ç‡è‡ªé€‚åº”æ–‡æœ¬æ¡ä»¶æ–¹æ³•ï¼Œä»è¾…åŠ©é‡å»ºå›¾åƒä¸­æå–å†…å®¹æ„ŸçŸ¥çš„æ–‡æœ¬æè¿°ï¼Œå®ç°æ— éœ€ç ç‡å¼€é”€çš„è¯­ä¹‰å¼•å¯¼ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> è¯¥æ–¹æ³•åœ¨æä½ç ç‡ä¸‹æ˜¾è‘—æå‡äº†é‡å»ºå›¾åƒçš„è§†è§‰è´¨é‡å’Œè¯­ä¹‰ä¿çœŸåº¦ï¼Œé€šè¿‡å†…å®¹è‡ªé€‚åº”æœºåˆ¶æœ‰æ•ˆå…‹æœäº†å‡åŒ€é‡åŒ–ã€ä¿¡æ¯ç“¶é¢ˆå’Œæ–‡æœ¬æç¤ºæ•ˆç‡ä½ä¸‹ä¸‰å¤§é™åˆ¶ï¼Œå®ç°äº†æ‰©æ•£å…ˆéªŒä¸ç¼–ç è¡¨ç¤ºçš„åŠ¨æ€å¯¹é½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion-based generative image compression has demonstrated remarkable potential for achieving realistic reconstruction at ultra-low bitrates. The key to unlocking this potential lies in making the entire compression process content-adaptive, ensuring that the encoder's representation and the decoder's generative prior are dynamically aligned with the semantic and structural characteristics of the input image. However, existing methods suffer from three critical limitations that prevent effective content adaptation. First, isotropic quantization applies a uniform quantization step, failing to adapt to the spatially varying complexity of image content and creating a misalignment with the diffusion model's noise-dependent prior. Second, the information concentration bottleneck -- arising from the dimensional mismatch between the high-dimensional noisy latent and the diffusion decoder's fixed input -- prevents the model from adaptively preserving essential semantic information in the primary channels. Third, existing textual conditioning strategies either need significant textual bitrate overhead or rely on generic, content-agnostic textual prompts, thereby failing to provide adaptive semantic guidance efficiently. To overcome these limitations, we propose a content-adaptive diffusion-based image codec with three technical innovations: 1) an Uncertainty-Guided Adaptive Quantization method that learns spatial uncertainty maps to adaptively align quantization distortion with content characteristics; 2) an Auxiliary Decoder-Guided Information Concentration method that uses a lightweight auxiliary decoder to enforce content-aware information preservation in the primary latent channels; and 3) a Bitrate-Free Adaptive Textual Conditioning method that derives content-aware textual descriptions from the auxiliary reconstructed image, enabling semantic guidance without bitrate cost.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.21591" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.21591.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.21416</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.21416" target="_blank">WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions</a>
      </h3>
      <p class="paper-title-zh">WildSVGï¼šé¢å‘çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„å¯é SVGç”Ÿæˆ</p>
      <p class="paper-authors">Marco Terral, Haotian Zhang, Tianyang Zhang, Meng Lin, Xiaoqing Xie ç­‰ (11 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†SVGæå–ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†é¦–ä¸ªç”¨äºç³»ç»Ÿè¯„ä¼°çœŸå®åœºæ™¯ä¸‹SVGç”Ÿæˆæ€§èƒ½çš„åŸºå‡†æ•°æ®é›†WildSVG Benchmarkï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹åˆé€‚è¯„ä¼°åŸºå‡†çš„ç©ºç™½ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ä½œè€…æ„å»ºäº†ç”±ä¸¤ä¸ªäº’è¡¥æ•°æ®é›†ç»„æˆçš„WildSVG Benchmarkï¼šä¸€æ˜¯åŸºäºçœŸå®å›¾åƒï¼ˆåŒ…å«å…¬å¸æ ‡å¿—åŠå…¶SVGæ ‡æ³¨ï¼‰æ„å»ºçš„Natural WildSVGï¼›äºŒæ˜¯é€šè¿‡å°†å¤æ‚SVGæ¸²æŸ“åˆæˆåˆ°çœŸå®åœºæ™¯ä¸­ä»¥æ¨¡æ‹Ÿå›°éš¾æ¡ä»¶çš„Synthetic WildSVGã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¯¹å½“å‰å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨çœŸå®åœºæ™¯ï¼ˆå­˜åœ¨å™ªå£°ã€æ‚ä¹±å’Œé¢†åŸŸåç§»ï¼‰ä¸‹çš„SVGæå–æ€§èƒ½è¿œæœªè¾¾åˆ°å¯é åº”ç”¨çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œè¿­ä»£ä¼˜åŒ–æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„å‘å±•è·¯å¾„ï¼Œä¸”æ¨¡å‹èƒ½åŠ›æ­£åœ¨ç¨³æ­¥æå‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.21416" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.21416.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>