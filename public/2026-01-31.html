<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-31</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22158</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22158" target="_blank">One-step Latent-free Image Generation with Pixel Mean Flows</a>
      </h3>
      <p class="paper-title-zh">åŸºäºåƒç´ å‡å€¼æµçš„å•æ­¥æ— æ½œå˜é‡å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºåƒç´ å‡å€¼æµï¼ˆpMFï¼‰æ–¹æ³•ï¼Œé¦–æ¬¡åœ¨æ— æ½œå˜é‡æ¡ä»¶ä¸‹å®ç°äº†é«˜è´¨é‡çš„å•æ­¥å›¾åƒç”Ÿæˆï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„å…³é”®ç©ºç™½ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•çš„æ ¸å¿ƒè®¾è®¡æ˜¯å°†ç½‘ç»œè¾“å‡ºç©ºé—´ä¸æŸå¤±ç©ºé—´åˆ†ç¦»ï¼šç½‘ç»œè¾“å‡ºç›®æ ‡è¢«è®¾è®¡åœ¨é¢„è®¾çš„ä½ç»´å›¾åƒæµå½¢ä¸Šï¼ˆå³x-predictionï¼‰ï¼Œè€ŒæŸå¤±åˆ™é€šè¿‡é€Ÿåº¦ç©ºé—´ä¸­çš„å‡å€¼æµï¼ˆMeanFlowï¼‰å®šä¹‰ã€‚é€šè¿‡å¼•å…¥å›¾åƒæµå½¢ä¸å¹³å‡é€Ÿåº¦åœºä¹‹é—´çš„ç®€å•å˜æ¢ï¼Œå®ç°äº†é«˜æ•ˆçš„å•æ­¥ç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒpMFåœ¨256Ã—256åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°2.22 FIDï¼Œåœ¨512Ã—512åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°2.48 FIDï¼Œè¯æ˜äº†å…¶åœ¨å•æ­¥æ— æ½œå˜é‡ç”Ÿæˆä»»åŠ¡ä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22158" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22158.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22125</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22125" target="_blank">Creative Image Generation with Diffusion Model</a>
      </h3>
      <p class="paper-title-zh">åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ›æ„å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Kunpeng Song, Ahmed Elgammal</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ›æ„å›¾åƒç”Ÿæˆæ–°æ¡†æ¶ï¼Œå°†åˆ›é€ åŠ›å®šä¹‰ä¸ºå›¾åƒåœ¨CLIPåµŒå…¥ç©ºé—´ä¸­å­˜åœ¨çš„é€†æ¦‚ç‡ï¼Œå¹¶å¼•å…¥å›æ‹‰æœºåˆ¶ï¼Œåœ¨ä¿æŒè§†è§‰ä¿çœŸåº¦çš„åŒæ—¶ç”Ÿæˆæ–°é¢–ç‹¬ç‰¹çš„å›¾åƒã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—ç”Ÿæˆå›¾åƒåœ¨CLIPåµŒå…¥ç©ºé—´ä¸­çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶é©±åŠ¨å…¶å‘ä½æ¦‚ç‡åŒºåŸŸç§»åŠ¨ï¼Œä»¥äº§ç”Ÿç½•è§ä¸”å¯Œæœ‰æƒ³è±¡åŠ›çš„è¾“å‡ºã€‚ä¸ä»¥å¾€ä¾èµ–æ‰‹åŠ¨æ¦‚å¿µæ··åˆæˆ–æ’é™¤å­ç±»åˆ«çš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§åŸåˆ™æ€§çš„æ¦‚ç‡é©±åŠ¨æœºåˆ¶ã€‚åŒæ—¶ï¼Œæ¡†æ¶å¼•å…¥äº†å›æ‹‰æœºåˆ¶ï¼Œä»¥å¹³è¡¡åˆ›æ„æ€§ä¸å›¾åƒè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆä¸”é«˜æ•ˆåœ°ç”Ÿæˆç‹¬ç‰¹ã€æ–°é¢–ä¸”å¼•äººæ·±æ€çš„å›¾åƒï¼Œä¸ºç”Ÿæˆæ¨¡å‹ä¸­çš„åˆ›é€ åŠ›ç ”ç©¶æä¾›äº†æ–°è§†è§’ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22125" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22125.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22094</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22094" target="_blank">RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</a>
      </h3>
      <p class="paper-title-zh">RefAny3Dï¼šç”¨äºå›¾åƒç”Ÿæˆçš„3Dèµ„äº§å‚è€ƒæ‰©æ•£æ¨¡å‹</p>
      <p class="paper-authors">Hanzhuo Huang, Qingyang Bao, Zekai Gu, Zhongshuo Du, Cheng Lin ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºé¦–ä¸ªèƒ½å¤Ÿä»¥3Dèµ„äº§ä½œä¸ºå‚è€ƒæ¡ä»¶çš„å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è”åˆå»ºæ¨¡é¢œè‰²ä¸ç©ºé—´åæ ‡ï¼Œå®ç°äº†ç”Ÿæˆå›¾åƒä¸3Då‚è€ƒä¹‹é—´çš„ç²¾ç¡®ä¸€è‡´æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é‡‡ç”¨åŒåˆ†æ”¯æ„ŸçŸ¥çš„è·¨åŸŸæ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶è¾“å…¥3Dèµ„äº§çš„å¤šè§†è§’RGBå›¾åƒå’Œç‚¹äº‘å›¾ï¼Œä»¥è”åˆå­¦ä¹ é¢œè‰²å’Œè§„èŒƒç©ºé—´åæ ‡ã€‚è®¾è®¡äº†ç©ºé—´å¯¹é½çš„åŒåˆ†æ”¯ç”Ÿæˆæ¶æ„ä¸åŸŸè§£è€¦ç”Ÿæˆæœºåˆ¶ï¼Œç¡®ä¿åŒæ—¶ç”Ÿæˆç©ºé—´å¯¹é½ä½†å†…å®¹è§£è€¦çš„RGBå›¾åƒå’Œç‚¹äº‘å›¾ï¼Œä»è€Œå…³è”2Då›¾åƒå±æ€§ä¸3Dèµ„äº§å±æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨3Dèµ„äº§ä½œä¸ºå‚è€ƒï¼Œç”Ÿæˆä¸ç»™å®šèµ„äº§ä¿æŒä¸€è‡´çš„å›¾åƒï¼Œä¸ºæ‰©æ•£æ¨¡å‹ä¸3Då†…å®¹åˆ›ä½œçš„ç»“åˆå¼€è¾Ÿäº†æ–°å¯èƒ½æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22094" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22094.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.22057</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.22057" target="_blank">Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models</a>
      </h3>
      <p class="paper-title-zh">åŸºäºåˆ¤åˆ«å™¨é©±åŠ¨æ‰©æ•£æ¨¡å‹çš„æ— ç›‘ç£åˆ†è§£ä¸é‡ç»„</p>
      <p class="paper-authors">Archer Wang, Emile Anand, Yilun Du, Marin SoljaÄiÄ‡</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒä¿¡å·æ¥æ”¹è¿›æ— ç›‘ç£å› å­åŒ–è¡¨ç¤ºå­¦ä¹ çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½æ›´å¥½åœ°å‘ç°æ½œåœ¨å› å­å¹¶æå‡ç»„åˆç”Ÿæˆçš„è´¨é‡ï¼ŒåŒæ—¶åœ¨å›¾åƒå’Œæœºå™¨äººè§†é¢‘è½¨è¿¹ä¸Šå±•ç¤ºäº†æ–°é¢–çš„åº”ç”¨ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹å­¦ä¹ æ— å› å­çº§åˆ«ç›‘ç£çš„å› å­åŒ–æ½œåœ¨ç©ºé—´ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªåˆ¤åˆ«å™¨æ¥åŒºåˆ†å•ä¸€æ¥æºçš„æ ·æœ¬ä¸è·¨æ¥æºå› å­é‡ç»„ç”Ÿæˆçš„æ ·æœ¬ï¼Œå¹¶ä¼˜åŒ–ç”Ÿæˆå™¨ä»¥æ¬ºéª—è¯¥åˆ¤åˆ«å™¨ï¼Œä»è€Œé¼“åŠ±é‡ç»„ç»“æœåœ¨ç‰©ç†å’Œè¯­ä¹‰ä¸Šçš„ä¸€è‡´æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨CelebA-HQã€Virtual KITTIã€CLEVRå’ŒFalcor3Dæ•°æ®é›†ä¸Šï¼Œæœ¬æ–¹æ³•åœ¨FIDåˆ†æ•°ä»¥åŠMIGå’ŒMCCè¡¡é‡çš„è§£çº ç¼ åº¦æ–¹é¢å‡ä¼˜äºå…ˆå‰åŸºçº¿ã€‚åœ¨æœºå™¨äººè§†é¢‘è½¨è¿¹åº”ç”¨ä¸­ï¼Œé€šè¿‡é‡ç»„å­¦ä¹ åˆ°çš„åŠ¨ä½œç»„ä»¶ï¼Œèƒ½å¤Ÿç”Ÿæˆæ˜¾è‘—å¢åŠ çŠ¶æ€ç©ºé—´è¦†ç›–èŒƒå›´çš„å¤šæ ·åŒ–åºåˆ—ï¼Œä»è€Œæå‡æ¢ç´¢æ•ˆç‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.22057" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.22057.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2601.21892</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.21892" target="_blank">Improving Classifier-Free Guidance of Flow Matching via Manifold Projection</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡æµå½¢æŠ•å½±æ”¹è¿›åŸºäºæµåŒ¹é…çš„æ— åˆ†ç±»å™¨å¼•å¯¼</p>
      <p class="paper-authors">Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡ä¸ºæ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æä¾›äº†ä¸€ä¸ªåŸºäºä¼˜åŒ–çš„ç†è®ºè§£é‡Šï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒ…å«æµå½¢çº¦æŸçš„åŒä¼¦ä¼˜åŒ–é‡‡æ ·æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€æç¤ºå¯¹é½å’Œå¯¹å¼•å¯¼å°ºåº¦çš„é²æ£’æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œè®ºæ–‡ä»ä¼˜åŒ–è§†è§’å°†æµåŒ¹é…ä¸­çš„é€Ÿåº¦åœºè§£é‡Šä¸ºä¸€ç³»åˆ—å¹³æ»‘è·ç¦»å‡½æ•°çš„æ¢¯åº¦ã€‚åŸºäºæ­¤ï¼Œä½œè€…å°†æ ‡å‡†CFGé‡‡æ ·é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå¸¦æµå½¢çº¦æŸçš„åŒä¼¦ä¼˜åŒ–é—®é¢˜ï¼Œè¿™è¦æ±‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œæµå½¢æŠ•å½±ã€‚è¯¥æŠ•å½±é€šè¿‡ä¸€ç§å¢é‡æ¢¯åº¦ä¸‹é™æ–¹æ¡ˆå®ç°ï¼Œå¹¶è¿›ä¸€æ­¥ç»“åˆå®‰å¾·æ£®åŠ é€Ÿæ³•è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œä¸”æ— éœ€é¢å¤–çš„æ¨¡å‹è¯„ä¼°ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> æ‰€æå‡ºçš„æ–¹æ³•æ˜¯å…è®­ç»ƒçš„ï¼Œèƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´åœ°æå‡ç”Ÿæˆä¿çœŸåº¦ã€æç¤ºå¯¹é½èƒ½åŠ›å’Œå¯¹å¼•å¯¼å°ºåº¦çš„é²æ£’æ€§ã€‚åœ¨DiT-XL-2-256ã€Fluxå’ŒStable Diffusion 3.5ç­‰å¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.21892" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.21892.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>