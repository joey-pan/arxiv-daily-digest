<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-24</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | æœºå™¨å­¦ä¹ : 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.19254</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.19254" target="_blank">RegionRoute: Regional Style Transfer with Diffusion Model</a>
      </h3>
      <p class="paper-title-zh">RegionRouteï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„åŒºåŸŸé£æ ¼è¿ç§»</p>
      <p class="paper-authors">Bowen Chen, Jake Zuena, Alan C. Bovik, Divya Kothandaraman</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›ç›‘ç£çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œé¦–æ¬¡å®ç°äº†æ— éœ€æ‰‹å·¥æ©ç çš„ã€çœŸæ­£æ„ä¹‰ä¸Šçš„å±€éƒ¨é£æ ¼è¿ç§»ï¼Œå¹¶å¼•å…¥äº†åŒºåŸŸé£æ ¼ç¼–è¾‘è¯„åˆ†æ ‡å‡†è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒæ—¶å¯¹é½é£æ ¼æ ‡è®°çš„æ³¨æ„åŠ›åˆ†æ•°ä¸ç›®æ ‡ç‰©ä½“æ©ç ï¼Œæ˜¾å¼åœ°æ•™å¯¼æ¨¡å‹åœ¨ä½•å¤„åº”ç”¨ç»™å®šé£æ ¼ã€‚è®¾è®¡äº†åŸºäºKLæ•£åº¦çš„FocusæŸå¤±å’ŒåŸºäºäºŒå…ƒäº¤å‰ç†µçš„CoveræŸå¤±ï¼Œå…±åŒç¡®ä¿ç²¾å‡†å®šä½å’Œå¯†é›†è¦†ç›–ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ¨¡å—åŒ–çš„LoRA-MoEè®¾è®¡ï¼Œå®ç°äº†é«˜æ•ˆã€å¯æ‰©å±•çš„å¤šé£æ ¼é€‚é…ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†æ—¶æ— éœ€æ©ç å³å¯å®ç°å•ç‰©ä½“é£æ ¼è¿ç§»ï¼Œç”Ÿæˆçš„ç»“æœåœ¨åŒºåŸŸå‡†ç¡®æ€§ã€è§†è§‰è¿è´¯æ€§ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ–¹æ³•ã€‚æ‰€æå‡ºçš„åŒºåŸŸé£æ ¼ç¼–è¾‘è¯„åˆ†èƒ½æœ‰æ•ˆè¡¡é‡ç›®æ ‡åŒºåŸŸçš„é£æ ¼åŒ¹é…åº¦ä¸æœªç¼–è¾‘åŒºåŸŸçš„èº«ä»½ä¿æŒåº¦ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Precise spatial control in diffusion-based style transfer remains challenging. This challenge arises because diffusion models treat style as a global feature and lack explicit spatial grounding of style representations, making it difficult to restrict style application to specific objects or regions. To our knowledge, existing diffusion models are unable to perform true localized style transfer, typically relying on handcrafted masks or multi-stage post-processing that introduce boundary artifacts and limit generalization. To address this, we propose an attention-supervised diffusion framework that explicitly teaches the model where to apply a given style by aligning the attention scores of style tokens with object masks during training. Two complementary objectives, a Focus loss based on KL divergence and a Cover loss using binary cross-entropy, jointly encourage accurate localization and dense coverage. A modular LoRA-MoE design further enables efficient and scalable multi-style adaptation. To evaluate localized stylization, we introduce the Regional Style Editing Score, which measures Regional Style Matching through CLIP-based similarity within the target region and Identity Preservation via masked LPIPS and pixel-level consistency on unedited areas. Experiments show that our method achieves mask-free, single-object style transfer at inference, producing regionally accurate and visually coherent results that outperform existing diffusion-based editing approaches.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.19254" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.19254.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.19083</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.19083" target="_blank">ChordEdit: One-Step Low-Energy Transport for Image Editing</a>
      </h3>
      <p class="paper-title-zh">ChordEditï¼šç”¨äºå›¾åƒç¼–è¾‘çš„ä¸€æ­¥å¼ä½èƒ½é‡ä¼ è¾“æ–¹æ³•</p>
      <p class="paper-authors">Liangsi Lu, Xuhang Chen, Minzhe Guo, Shichu Li, Jingchao Wang ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€æ— éœ€æ¨¡å‹åæ¼”ä¸”ä¸æ¨¡å‹æ— å…³çš„ChordEditæ–¹æ³•ï¼Œé¦–æ¬¡åœ¨ä¸€æ­¥å¼æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šå®ç°äº†é«˜ä¿çœŸåº¦çš„å®æ—¶å›¾åƒç¼–è¾‘ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å› é«˜èƒ½é‡è½¨è¿¹å¯¼è‡´çš„ç‰©ä½“æ‰­æ›²å’Œæœªç¼–è¾‘åŒºåŸŸå¤±çœŸçš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> å°†å›¾åƒç¼–è¾‘é‡æ–°å®šä¹‰ä¸ºæºæç¤ºè¯ä¸ç›®æ ‡æç¤ºè¯æ‰€å®šä¹‰åˆ†å¸ƒä¹‹é—´çš„ä¼ è¾“é—®é¢˜ï¼›åŸºäºåŠ¨æ€æœ€ä¼˜ä¼ è¾“ç†è®ºï¼Œæ¨å¯¼å‡ºä¸€ç§åŸç†æ€§çš„ä½èƒ½é‡æ§åˆ¶ç­–ç•¥ï¼›è¯¥ç­–ç•¥ç”Ÿæˆå¹³æ»‘ã€æ–¹å·®é™ä½çš„ç¼–è¾‘åœºï¼Œä½¿å…¶åœ¨å•æ¬¡å¤§æ­¥é•¿ç§¯åˆ†ä¸­ä¿æŒç¨³å®šï¼Œä»è€Œå®ç°ä¸€æ­¥å¼ç¼–è¾‘ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ChordEditèƒ½å¤Ÿç”Ÿæˆç¨³å®šã€ä½æ–¹å·®çš„ç¼–è¾‘è½¨è¿¹ï¼Œåœ¨ä¸€æ­¥æ¨ç†ä¸­å®ç°é«˜ä¿çœŸåº¦çš„å›¾åƒç¼–è¾‘ï¼Œæœ‰æ•ˆé¿å…äº†ç‰©ä½“æ‰­æ›²å’ŒèƒŒæ™¯ä¿¡æ¯ä¸¢å¤±ï¼›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¿æŒç¼–è¾‘ç²¾åº¦çš„åŒæ—¶ï¼Œé¦–æ¬¡åœ¨ä¸€æ­¥å¼æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šå®ç°äº†çœŸæ­£çš„å®æ—¶ç¼–è¾‘ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>The advent of one-step text-to-image (T2I) models offers unprecedented synthesis speed. However, their application to text-guided image editing remains severely hampered, as forcing existing training-free editors into a single inference step fails. This failure manifests as severe object distortion and a critical loss of consistency in non-edited regions, resulting from the high-energy, erratic trajectories produced by naive vector arithmetic on the models' structured fields. To address this problem, we introduce ChordEdit, a model agnostic, training-free, and inversion-free method that facilitates high-fidelity one-step editing. We recast editing as a transport problem between the source and target distributions defined by the source and target text prompts. Leveraging dynamic optimal transport theory, we derive a principled, low-energy control strategy. This strategy yields a smoothed, variance-reduced editing field that is inherently stable, facilitating the field to be traversed in a single, large integration step. A theoretically grounded and experimentally validated approach allows ChordEdit to deliver fast, lightweight and precise edits, finally achieving true real-time editing on these challenging models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.19083" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.19083.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">æœºå™¨å­¦ä¹ </span>
          <span class="paper-id">2602.19033</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.19033" target="_blank">A Markovian View of Iterative-Feedback Loops in Image Generative Models: Neural Resonance and Model Collapse</a>
      </h3>
      <p class="paper-title-zh">å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­è¿­ä»£åé¦ˆå¾ªç¯çš„é©¬å°”å¯å¤«è§†è§’ï¼šç¥ç»å…±æŒ¯ä¸æ¨¡å‹å´©æºƒ</p>
      <p class="paper-authors">Vibhas Kumar Vats, David J. Crandall, Samuel Goree</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†â€œç¥ç»å…±æŒ¯â€çš„æ¦‚å¿µï¼Œç”¨ä»¥è§£é‡Šç”Ÿæˆæ¨¡å‹åœ¨è¿­ä»£åé¦ˆè®­ç»ƒä¸­å‡ºç°çš„é•¿æœŸé€€åŒ–è¡Œä¸ºï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåŒ…å«å…«ç§æ¨¡å¼çš„æ¨¡å‹å´©æºƒåˆ†ç±»ä½“ç³»ï¼Œä¸ºç†è§£å’Œè¯Šæ–­æ¨¡å‹å´©æºƒæä¾›äº†ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> ç ”ç©¶å°†è¿­ä»£åé¦ˆè¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«é“¾ï¼Œå¹¶åˆ†æäº†å…¶æ”¶æ•›æ¡ä»¶ã€‚é€šè¿‡ç ”ç©¶MNISTå’ŒImageNetä¸Šçš„æ‰©æ•£æ¨¡å‹ã€CycleGANä»¥åŠä¸€ä¸ªéŸ³é¢‘åé¦ˆå®éªŒï¼Œè¿½è¸ªäº†æ½œåœ¨ç©ºé—´ä¸­å±€éƒ¨å’Œå…¨å±€æµå½¢å‡ ä½•ç»“æ„çš„æ¼”åŒ–è¿‡ç¨‹ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ç ”ç©¶å‘ç°ï¼Œå½“åé¦ˆè¿‡ç¨‹æ»¡è¶³éå†æ€§ä¸”æ½œåœ¨è¡¨ç¤ºå…·æœ‰æ–¹å‘æ€§æ”¶ç¼©æ—¶ï¼Œç³»ç»Ÿä¼šæ”¶æ•›åˆ°ä¸€ä¸ªä½ç»´ä¸å˜ç»“æ„ï¼Œå³å‘ç”Ÿâ€œç¥ç»å…±æŒ¯â€ï¼Œè¿™æœ€ç»ˆå¯¼è‡´æ¨¡å‹å´©æºƒã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æ€»ç»“å¹¶åˆ†ç±»äº†å…«ç§ä¸åŒçš„æ¨¡å‹å´©æºƒè¡Œä¸ºæ¨¡å¼ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>AI training datasets will inevitably contain AI-generated examples, leading to ``feedback'' in which the output of one model impacts the training of another. It is known that such iterative feedback can lead to model collapse, yet the mechanisms underlying this degeneration remain poorly understood. Here we show that a broad class of feedback processes converges to a low-dimensional invariant structure in latent space, a phenomenon we call neural resonance. By modeling iterative feedback as a Markov Chain, we show that two conditions are needed for this resonance to occur: ergodicity of the feedback process and directional contraction of the latent representation. By studying diffusion models on MNIST and ImageNet, as well as CycleGAN and an audio feedback experiment, we map how local and global manifold geometry evolve, and we introduce an eight-pattern taxonomy of collapse behaviors. Neural resonance provides a unified explanation for long-term degenerate behavior in generative models and provides practical diagnostics for identifying, characterizing, and eventually mitigating collapse.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.19033" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.19033.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.19019</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.19019" target="_blank">TokenTrace: Multi-Concept Attribution through Watermarked Token Recovery</a>
      </h3>
      <p class="paper-title-zh">TokenTraceï¼šé€šè¿‡æ°´å°ä»¤ç‰Œæ¢å¤å®ç°å¤šæ¦‚å¿µæº¯æº</p>
      <p class="paper-authors">Li Zhang, Shruti Agarwal, John Collomosse, Pengtao Xie, Vishal Asnani</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†TokenTraceï¼Œä¸€ç§æ–°é¢–çš„ä¸»åŠ¨æ°´å°æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹ç”Ÿæˆå¼AIå›¾åƒä¸­åŒæ—¶å­˜åœ¨çš„å¤šä¸ªæ¦‚å¿µï¼ˆå¦‚ç‰©ä½“å’Œè‰ºæœ¯é£æ ¼ï¼‰è¿›è¡Œé²æ£’çš„åˆ†ç¦»ä¸æº¯æºï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šæ¦‚å¿µç»„åˆåœºæ™¯ä¸‹éš¾ä»¥ç‹¬ç«‹å½’å› çš„éš¾é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡åœ¨è¯­ä¹‰åŸŸåµŒå…¥ç§˜å¯†ç­¾åï¼ŒåŒæ—¶æ‰°åŠ¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹çš„æ–‡æœ¬æç¤ºåµŒå…¥å’Œåˆå§‹æ½œåœ¨å™ªå£°ã€‚åœ¨æ£€ç´¢é˜¶æ®µï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºæŸ¥è¯¢çš„TokenTraceæ¨¡å—ï¼Œè¯¥æ¨¡å—ä»¥ç”Ÿæˆçš„å›¾åƒå’ŒæŒ‡å®šéœ€è¦æ£€ç´¢æ¦‚å¿µçš„æ–‡æœ¬æŸ¥è¯¢ä¸ºè¾“å…¥ï¼Œä»è€Œèƒ½å¤Ÿä»å•å¼ å›¾åƒä¸­åˆ†ç¦»å¹¶ç‹¬ç«‹éªŒè¯å¤šä¸ªæ¦‚å¿µçš„å­˜åœ¨ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•æ¦‚å¿µï¼ˆç‰©ä½“å’Œé£æ ¼ï¼‰å’Œå¤šæ¦‚å¿µæº¯æºä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è§†è§‰è´¨é‡ï¼Œå¹¶å¯¹å¸¸è§çš„å›¾åƒå˜æ¢å…·æœ‰é²æ£’æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Generative AI models pose a significant challenge to intellectual property (IP), as they can replicate unique artistic styles and concepts without attribution. While watermarking offers a potential solution, existing methods often fail in complex scenarios where multiple concepts (e.g., an object and an artistic style) are composed within a single image. These methods struggle to disentangle and attribute each concept individually. In this work, we introduce TokenTrace, a novel proactive watermarking framework for robust, multi-concept attribution. Our method embeds secret signatures into the semantic domain by simultaneously perturbing the text prompt embedding and the initial latent noise that guide the diffusion model's generation process. For retrieval, we propose a query-based TokenTrace module that takes the generated image and a textual query specifying which concepts need to be retrieved (e.g., a specific object or style) as inputs. This query-based mechanism allows the module to disentangle and independently verify the presence of multiple concepts from a single generated image. Extensive experiments show that our method achieves state-of-the-art performance on both single-concept (object and style) and multi-concept attribution tasks, significantly outperforming existing baselines while maintaining high visual quality and robustness to common transformations.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.19019" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.19019.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.18993</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.18993" target="_blank">SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models</a>
      </h3>
      <p class="paper-title-zh">SeaCacheï¼šç”¨äºåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„å…‰è°±æ¼”åŒ–æ„ŸçŸ¥ç¼“å­˜</p>
      <p class="paper-authors">Jiwoo Chung, Sangeek Hyun, MinKyu Lee, Byeongju Han, Geonho Cha ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å…‰è°±æ¼”åŒ–æ„ŸçŸ¥ç¼“å­˜è°ƒåº¦æ–¹æ³•ï¼ˆSeaCacheï¼‰ï¼Œé€šè¿‡è§£è€¦å†…å®¹ä¸å™ªå£°ï¼ŒåŸºäºå…‰è°±å¯¹é½è¡¨ç¤ºæ¥å†³ç­–ä¸­é—´ç‰¹å¾çš„å¤ç”¨ï¼Œæ˜¾è‘—æ”¹å–„äº†æ‰©æ•£æ¨¡å‹æ¨ç†é€Ÿåº¦ä¸ç”Ÿæˆè´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆé€šè¿‡ç†è®ºä¸å®è¯åˆ†ææ¨å¯¼å‡ºå…‰è°±æ¼”åŒ–æ„ŸçŸ¥ï¼ˆSEAï¼‰æ»¤æ³¢å™¨ï¼Œè¯¥æ»¤æ³¢å™¨èƒ½ä¿ç•™ä¸å†…å®¹ç›¸å…³çš„æˆåˆ†å¹¶æŠ‘åˆ¶å™ªå£°ï¼›ç„¶ååˆ©ç”¨SEAæ»¤æ³¢åçš„è¾“å…¥ç‰¹å¾æ¥ä¼°è®¡ç›¸é‚»æ—¶é—´æ­¥ä¹‹é—´çš„å†—ä½™åº¦ï¼›æœ€ååŸºäºæ­¤è®¾è®¡åŠ¨æ€ç¼“å­˜è°ƒåº¦ç­–ç•¥ï¼Œä½¿å…¶æ—¢èƒ½é€‚åº”ä¸åŒç”Ÿæˆå†…å®¹ï¼Œåˆå°Šé‡æ‰©æ•£æ¨¡å‹å†…åœ¨çš„å…‰è°±å…ˆéªŒï¼ˆä½é¢‘ç»“æ„å…ˆå‡ºç°ï¼Œé«˜é¢‘ç»†èŠ‚åç»†åŒ–ï¼‰ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å¤šç§è§†è§‰ç”Ÿæˆæ¨¡å‹å’ŒåŸºçº¿æ–¹æ³•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSeaCacheåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†å½“å‰æœ€ä¼˜çš„å»¶è¿Ÿ-è´¨é‡æƒè¡¡ï¼Œæ˜¾è‘—åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.18993" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.18993.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>