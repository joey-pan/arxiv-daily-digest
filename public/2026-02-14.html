<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-14</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12155</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12155" target="_blank">FAIL: Flow Matching Adversarial Imitation Learning for Image Generation</a>
      </h3>
      <p class="paper-title-zh">FAILï¼šç”¨äºå›¾åƒç”Ÿæˆçš„æµåŒ¹é…å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ </p>
      <p class="paper-authors">Yeyao Ma, Chen Li, Xiaosong Zhang, Han Hu, Weidi Xie</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFAILçš„æµåŒ¹é…å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯¹æŠ—è®­ç»ƒæœ€å°åŒ–ç”Ÿæˆç­–ç•¥ä¸ä¸“å®¶åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œæ— éœ€æ˜¾å¼çš„å¥–åŠ±å‡½æ•°æˆ–æˆå¯¹åå¥½æ•°æ®ï¼Œä»è€Œå®ç°äº†å¯¹æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆåè®­ç»ƒå¯¹é½ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> FAILå°†æµåŒ¹é…æ¨¡å‹çš„åè®­ç»ƒé—®é¢˜å½¢å¼åŒ–ä¸ºæ¨¡ä»¿å­¦ä¹ ä»»åŠ¡ï¼Œé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒç›´æ¥å¯¹é½ç”Ÿæˆåˆ†å¸ƒä¸é«˜è´¨é‡ç›®æ ‡åˆ†å¸ƒã€‚è®ºæ–‡æ¨å¯¼äº†ä¸¤ç§ç®—æ³•ï¼šFAIL-PDåˆ©ç”¨å¯å¾®ODEæ±‚è§£å™¨è®¡ç®—ä½æ–¹å·®çš„è·¯å¾„æ¢¯åº¦ï¼›FAIL-PGåˆ™æä¾›äº†ä¸€ç§é€‚ç”¨äºç¦»æ•£æˆ–è®¡ç®—å—é™åœºæ™¯çš„é»‘ç›’æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä»…éœ€å°‘é‡ä¸“å®¶æ¼”ç¤ºå³å¯å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨13,000ä¸ªæ¥è‡ªNano Banana proçš„æ¼”ç¤ºå¯¹FLUXæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒFAILåœ¨æç¤ºè·Ÿéšå’Œç¾å­¦è¯„ä¼°åŸºå‡†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ¨å¹¿åˆ°ç¦»æ•£å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¯ä½œä¸ºä¸€ç§é²æ£’çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç¼“è§£åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12155" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12155.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.11980</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.11980" target="_blank">Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</a>
      </h3>
      <p class="paper-title-zh">ç©ºé—´æ€ç»´é“¾ï¼šè¿æ¥ç†è§£ä¸ç”Ÿæˆæ¨¡å‹ä»¥è¿›è¡Œç©ºé—´æ¨ç†ç”Ÿæˆ</p>
      <p class="paper-authors">Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang ç­‰ (12 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„ç©ºé—´æ€ç»´é“¾æ¡†æ¶ï¼Œæœ‰æ•ˆå¼¥åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„é¸¿æ²Ÿï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æˆ–ç©ºé—´ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œé€šè¿‡ä½¿ç”¨äº¤é”™æ–‡æœ¬-åæ ‡æŒ‡ä»¤æ ¼å¼è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œå¢å¼ºå…¶å¯¹ç©ºé—´å¸ƒå±€çš„ç†è§£èƒ½åŠ›ã€‚ç„¶åï¼Œåˆ©ç”¨å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè§„åˆ’å™¨ï¼Œç”Ÿæˆè¯¦ç»†çš„å¸ƒå±€è§„åˆ’ï¼Œä»è€Œå°†å…¶ç©ºé—´è§„åˆ’èƒ½åŠ›ç›´æ¥è¿ç§»åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚æ•´ä¸ªæ¡†æ¶æ— éœ€è”åˆè®­ç»ƒï¼Œå®ç°äº†é«˜æ•ˆçš„æ¨¡å—åŒ–åä½œã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤æ‚ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å›¾åƒç¼–è¾‘åœºæ™¯ä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æœ‰æ•ˆæ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model's layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.11980" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.11980.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.11850</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.11850" target="_blank">Free Lunch for Stabilizing Rectified Flow Inversion</a>
      </h3>
      <p class="paper-title-zh">ç¨³å®šæ•´æµæµåæ¼”çš„å…è´¹åˆé¤</p>
      <p class="paper-authors">Chenru Wang, Beier Zhu, Chi Zhang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ— éœ€è®­ç»ƒçš„æ¢¯åº¦æ ¡æ­£æ–¹æ³•â€”â€”è¿‘ç«¯å‡å€¼åæ¼”ï¼ˆPMIï¼‰å’Œæ¨¡ä»¿-CFGï¼Œæ—¨åœ¨è§£å†³æ•´æµæµæ¨¡å‹åœ¨åæ¼”è¿‡ç¨‹ä¸­å› è¯¯å·®ç´¯ç§¯å¯¼è‡´çš„é€Ÿåº¦åœºä¸ç¨³å®šé—®é¢˜ï¼Œä»è€Œæ˜¾è‘—æå‡å›¾åƒé‡å»ºä¸ç¼–è¾‘çš„è´¨é‡ä¸æ•ˆç‡ã€‚</div>
        
        
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Rectified-Flow (RF)-based generative models have recently emerged as strong alternatives to traditional diffusion models, demonstrating state-of-the-art performance across various tasks. By learning a continuous velocity field that transforms simple noise into complex data, RF-based models not only enable high-quality generation, but also support training-free inversion, which facilitates downstream tasks such as reconstruction and editing. However, existing inversion methods, such as vanilla RF-based inversion, suffer from approximation errors that accumulate across timesteps, leading to unstable velocity fields and degraded reconstruction and editing quality. To address this challenge, we propose Proximal-Mean Inversion (PMI), a training-free gradient correction method that stabilizes the velocity field by guiding it toward a running average of past velocities, constrained within a theoretically derived spherical Gaussian. Furthermore, we introduce mimic-CFG, a lightweight velocity correction scheme for editing tasks, which interpolates between the current velocity and its projection onto the historical average, balancing editing effectiveness and structural consistency. Extensive experiments on PIE-Bench demonstrate that our methods significantly improve inversion stability, image reconstruction quality, and editing fidelity, while reducing the required number of neural function evaluations. Our approach achieves state-of-the-art performance on the PIE-Bench with enhanced efficiency and theoretical soundness.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.11850" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.11850.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.11440</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.11440" target="_blank">Ctrl&Shift: High-Quality Geometry-Aware Object Manipulation in Visual Generation</a>
      </h3>
      <p class="paper-title-zh">Ctrl&Shiftï¼šè§†è§‰ç”Ÿæˆä¸­é«˜è´¨é‡ã€å‡ ä½•æ„ŸçŸ¥çš„ç‰©ä½“æ“æ§</p>
      <p class="paper-authors">Penghui Ruan, Bojia Zi, Xianbiao Qi, Youze Huang, Rong Xiao ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªæ— éœ€æ˜¾å¼3Då»ºæ¨¡ã€èƒ½ç»Ÿä¸€ç»†ç²’åº¦å‡ ä½•æ§åˆ¶ä¸ç°å®ä¸–ç•Œæ³›åŒ–èƒ½åŠ›çš„ç‰©ä½“æ“æ§æ¡†æ¶ï¼Œé€šè¿‡å°†æ“æ§åˆ†è§£ä¸ºç‰©ä½“ç§»é™¤å’Œç›¸æœºå§¿æ€æ§åˆ¶ä¸‹çš„å‚è€ƒå¼•å¯¼ä¿®å¤ä¸¤é˜¶æ®µï¼Œå®ç°äº†èƒŒæ™¯ä¿æŒã€è§†è§’å‡ ä½•ä¸€è‡´ä¸ç”¨æˆ·å¯æ§å˜æ¢çš„è”åˆä¼˜åŒ–ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åŸºäºç«¯åˆ°ç«¯æ‰©æ•£æ¨¡å‹ï¼Œæ ¸å¿ƒæ˜¯å°†ç‰©ä½“æ“æ§åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç‰©ä½“ç§»é™¤å’Œæ˜¾å¼ç›¸æœºå§¿æ€æ§åˆ¶ä¸‹çš„å‚è€ƒå¼•å¯¼ä¿®å¤ï¼Œå¹¶å°†äºŒè€…ç¼–ç åˆ°ç»Ÿä¸€çš„æ‰©æ•£è¿‡ç¨‹ä¸­ã€‚ä¸ºäº†å®ç°å¯¹èƒŒæ™¯ã€ç‰©ä½“èº«ä»½å’Œå§¿æ€ä¿¡å·çš„è§£è€¦æ§åˆ¶ï¼Œè®¾è®¡äº†ä¸€ç§å¤šä»»åŠ¡ã€å¤šé˜¶æ®µçš„è®­ç»ƒç­–ç•¥ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ä¸ªå¯æ‰©å±•çš„çœŸå®ä¸–ç•Œæ•°æ®é›†æ„å»ºæµç¨‹ï¼Œç”¨äºç”Ÿæˆå¸¦æœ‰ä¼°è®¡ç›¸å¯¹ç›¸æœºå§¿æ€çš„é…å¯¹å›¾åƒå’Œè§†é¢‘æ ·æœ¬ï¼Œä»¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCtrl&Shiftåœ¨ä¿çœŸåº¦ã€è§†è§’ä¸€è‡´æ€§å’Œå¯æ§æ€§æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é¦–æ¬¡åœ¨ä¸ä¾èµ–ä»»ä½•æ˜¾å¼3Då»ºæ¨¡çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¯¹ç‰©ä½“æ“æ§çš„ç»†ç²’åº¦å‡ ä½•æ§åˆ¶ä¸ç°å®ä¸–ç•Œæ³›åŒ–èƒ½åŠ›çš„ç»Ÿä¸€ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Object-level manipulation, relocating or reorienting objects in images or videos while preserving scene realism, is central to film post-production, AR, and creative editing. Yet existing methods struggle to jointly achieve three core goals: background preservation, geometric consistency under viewpoint shifts, and user-controllable transformations. Geometry-based approaches offer precise control but require explicit 3D reconstruction and generalize poorly; diffusion-based methods generalize better but lack fine-grained geometric control. We present Ctrl&Shift, an end-to-end diffusion framework to achieve geometry-consistent object manipulation without explicit 3D representations. Our key insight is to decompose manipulation into two stages, object removal and reference-guided inpainting under explicit camera pose control, and encode both within a unified diffusion process. To enable precise, disentangled control, we design a multi-task, multi-stage training strategy that separates background, identity, and pose signals across tasks. To improve generalization, we introduce a scalable real-world dataset construction pipeline that generates paired image and video samples with estimated relative camera poses. Extensive experiments demonstrate that Ctrl&Shift achieves state-of-the-art results in fidelity, viewpoint consistency, and controllability. To our knowledge, this is the first framework to unify fine-grained geometric control and real-world generalization for object manipulation, without relying on any explicit 3D modeling.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.11440" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.11440.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.11401</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.11401" target="_blank">Latent Forcing: Reordering the Diffusion Trajectory for Pixel-Space Image Generation</a>
      </h3>
      <p class="paper-title-zh">æ½œåœ¨å¼ºåˆ¶ï¼šé‡æ’åºæ‰©æ•£è½¨è¿¹ä»¥å®ç°åƒç´ ç©ºé—´å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Alan Baade, Eric Ryan Chan, Kyle Sargent, Changan Chen, Justin Johnson ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åä¸ºâ€œæ½œåœ¨å¼ºåˆ¶â€çš„æ¶æ„æ”¹è¿›æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨åŸå§‹åƒç´ ç©ºé—´è¿›è¡Œé«˜æ•ˆå›¾åƒç”Ÿæˆçš„åŒæ—¶ï¼Œä¿ç•™æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ä¼˜åŠ¿ï¼Œå¹¶åœ¨åŸºäºæ‰©æ•£Transformerçš„åƒç´ ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡è”åˆå¤„ç†æ½œåœ¨è¡¨ç¤ºå’ŒåŸå§‹åƒç´ ï¼Œå¹¶åˆ†åˆ«ä¸ºå®ƒä»¬è®¾è®¡ç‹¬ç«‹çš„å™ªå£°è°ƒåº¦ï¼Œæ¥é‡æ–°æ’åºå»å™ªè½¨è¿¹ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯è®©æ½œåœ¨è¡¨ç¤ºåœ¨ç”Ÿæˆé«˜é¢‘åƒç´ ç‰¹å¾ä¹‹å‰å……å½“ä¸­é—´è®¡ç®—çš„â€œè‰ç¨¿çº¸â€ï¼Œä»è€Œåœ¨åƒç´ ç©ºé—´å®ç°ç«¯åˆ°ç«¯å»ºæ¨¡ã€‚ä½œè€…ç‰¹åˆ«ç ”ç©¶äº†æ¡ä»¶ä¿¡å·çš„é¡ºåºå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNetä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒç­‰è®¡ç®—è§„æ¨¡ä¸‹ï¼Œä¸ºåŸºäºæ‰©æ•£Transformerçš„åƒç´ ç”Ÿæˆå»ºç«‹äº†æ–°çš„æœ€ä¼˜ç»“æœã€‚åˆ†ææ­ç¤ºäº†æ¡ä»¶ä¿¡å·é¡ºåºçš„å…³é”®ä½œç”¨ï¼Œå¹¶ä»¥æ­¤è§£é‡Šäº†åˆ†è¯å™¨ä¸æ‰©æ•£æ¨¡å‹åœ¨REPAè’¸é¦ä¸­çš„å·®å¼‚ã€æ¡ä»¶ä¸éæ¡ä»¶ç”Ÿæˆçš„åŒºåˆ«ï¼Œä»¥åŠåˆ†è¯å™¨é‡å»ºè´¨é‡ä¸å¯æ‰©æ•£æ€§ä¹‹é—´çš„å…³ç³»ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Latent diffusion models excel at generating high-quality images but lose the benefits of end-to-end modeling. They discard information during image encoding, require a separately trained decoder, and model an auxiliary distribution to the raw data. In this paper, we propose Latent Forcing, a simple modification to existing architectures that achieves the efficiency of latent diffusion while operating on raw natural images. Our approach orders the denoising trajectory by jointly processing latents and pixels with separately tuned noise schedules. This allows the latents to act as a scratchpad for intermediate computation before high-frequency pixel features are generated. We find that the order of conditioning signals is critical, and we analyze this to explain differences between REPA distillation in the tokenizer and the diffusion model, conditional versus unconditional generation, and how tokenizer reconstruction quality relates to diffusability. Applied to ImageNet, Latent Forcing achieves a new state-of-the-art for diffusion transformer-based pixel generation at our compute scale.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.11401" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.11401.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>