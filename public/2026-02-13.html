<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-13</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12127</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 95/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12127" target="_blank">PosterOmni: Generalized Artistic Poster Creation via Task Distillation and Unified Reward Feedback</a>
      </h3>
      <p class="paper-title-zh">PosterOmniï¼šé€šè¿‡ä»»åŠ¡è’¸é¦ä¸ç»Ÿä¸€å¥–åŠ±åé¦ˆçš„é€šç”¨è‰ºæœ¯æµ·æŠ¥åˆ›ä½œ</p>
      <p class="paper-authors">Sixiang Chen, Jianyu Lai, Jialin Gao, Hengyu Shi, Zhongying Liu ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„è‰ºæœ¯æµ·æŠ¥åˆ›ä½œæ¡†æ¶PosterOmniï¼Œé€šè¿‡æ•´åˆå±€éƒ¨ç¼–è¾‘ä¸å…¨å±€åˆ›ä½œä¸¤ç§æ¨¡å¼ï¼Œå¹¶è®¾è®¡ç»Ÿä¸€çš„æ•°æ®-è’¸é¦-å¥–åŠ±æµç¨‹ï¼Œå®ç°äº†å¤šä»»åŠ¡å›¾åƒåˆ°æµ·æŠ¥çš„é«˜è´¨é‡ç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆæ„å»ºäº†è¦†ç›–å…­ç§ä»»åŠ¡ç±»å‹çš„å¤šåœºæ™¯å›¾åƒåˆ°æµ·æŠ¥æ•°æ®é›†ï¼Œæ¶µç›–åŸºäºå®ä½“çš„ç¼–è¾‘å’ŒåŸºäºæ¦‚å¿µçš„åˆ›ä½œã€‚å…¶æ¬¡ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦åœ¨å±€éƒ¨ä¸å…¨å±€ä¸“å®¶æ¨¡å‹ä¹‹é—´è¿›è¡Œç›‘ç£å¾®è°ƒã€‚æœ€åï¼Œåº”ç”¨ç»Ÿä¸€çš„PosterOmniå¥–åŠ±åé¦ˆæœºåˆ¶ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è”åˆå¯¹é½è§†è§‰å®ä½“ä¿æŒä¸ç¾å­¦åå¥½ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒPosterOmniåœ¨å‚è€ƒä¾ä»æ€§ã€å…¨å±€æ„å›¾è´¨é‡å’Œç¾å­¦å’Œè°åº¦æ–¹é¢æ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†æ‰€æœ‰å¼€æºåŸºçº¿æ¨¡å‹ï¼Œç”šè‡³ä¼˜äºå¤šä¸ªä¸“æœ‰ç³»ç»Ÿã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜å»ºç«‹äº†ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†PosterOmni-Benchï¼Œç”¨äºå…¨é¢è¯„ä¼°å±€éƒ¨ç¼–è¾‘å’Œå…¨å±€åˆ›ä½œä»»åŠ¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Image-to-poster generation is a high-demand task requiring not only local adjustments but also high-level design understanding. Models must generate text, layout, style, and visual elements while preserving semantic fidelity and aesthetic coherence. The process spans two regimes: local editing, where ID-driven generation, rescaling, filling, and extending must preserve concrete visual entities; and global creation, where layout- and style-driven tasks rely on understanding abstract design concepts. These intertwined demands make image-to-poster a multi-dimensional process coupling entity-preserving editing with concept-driven creation under image-prompt control. To address these challenges, we propose PosterOmni, a generalized artistic poster creation framework that unlocks the potential of a base edit model for multi-task image-to-poster generation. PosterOmni integrates the two regimes, namely local editing and global creation, within a single system through an efficient data-distillation-reward pipeline: (i) constructing multi-scenario image-to-poster datasets covering six task types across entity-based and concept-based creation; (ii) distilling knowledge between local and global experts for supervised fine-tuning; and (iii) applying unified PosterOmni Reward Feedback to jointly align visual entity-preserving and aesthetic preference across all tasks. Additionally, we establish PosterOmni-Bench, a unified benchmark for evaluating both local editing and global creation. Extensive experiments show that PosterOmni significantly enhances reference adherence, global composition quality, and aesthetic harmony, outperforming all open-source baselines and even surpassing several proprietary systems.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12127" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12127.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12280</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12280" target="_blank">Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</a>
      </h3>
      <p class="paper-title-zh">æƒŠé¸¿ä¸€ç¬”ï¼šçŸ¢é‡ç»˜ç”»ä¸­çš„æ¸è¿›å¼è¯­ä¹‰é”™è§‰</p>
      <p class="paper-authors">Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¸è¿›å¼è¯­ä¹‰é”™è§‰çŸ¢é‡ç»˜ç”»ä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†â€œStroke of Surpriseâ€ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡é¡ºåºæ·»åŠ ç¬”ç”»ä½¿å•ä¸€è‰å›¾åœ¨ä¸åŒç»˜åˆ¶é˜¶æ®µå‘ˆç°æˆªç„¶ä¸åŒçš„è¯­ä¹‰è§£é‡Šï¼Œå°†è§†è§‰å­—è°œä»ç©ºé—´ç»´åº¦æ‰©å±•åˆ°æ—¶é—´ç»´åº¦ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é‡‡ç”¨åºåˆ—æ„ŸçŸ¥çš„è”åˆä¼˜åŒ–æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŒåˆ†æ”¯åˆ†æ•°è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æœºåˆ¶ã€‚è¯¥æ¡†æ¶åŠ¨æ€ä¼˜åŒ–åˆå§‹ç¬”ç”»ï¼ˆå‰ç¼€ï¼‰ï¼Œä½¿å…¶æ—¢èƒ½æ„æˆç¬¬ä¸€ä¸ªè¿è´¯ç‰©ä½“ï¼Œåˆèƒ½ä½œä¸ºæ·»åŠ åç»­ç¬”ç”»ï¼ˆå¢é‡ï¼‰åç¬¬äºŒä¸ªæ¦‚å¿µçš„ç»“æ„åŸºç¡€ï¼Œä»è€Œå‘ç°é€‚ç”¨äºä¸¤ä¸ªç›®æ ‡çš„â€œå…±åŒç»“æ„å­ç©ºé—´â€ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å åŠ æŸå¤±ï¼Œå¼ºåˆ¶ç©ºé—´äº’è¡¥æ€§ï¼Œç¡®ä¿ç»“æ„æ•´åˆè€Œéç®€å•é®æŒ¡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯è¯†åˆ«æ€§å’Œé”™è§‰å¼ºåº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚å®ƒæˆåŠŸå®ç°äº†çŸ¢é‡è‰å›¾çš„æ¸è¿›å¼è¯­ä¹‰è½¬æ¢ï¼ˆä¾‹å¦‚ä»é¸­å­å˜ä¸ºç»µç¾Šï¼‰ï¼ŒéªŒè¯äº†é€šè¿‡æ—¶é—´åºåˆ—çš„ç¬”ç”»æ·»åŠ æ¥åˆ›é€ è¯­ä¹‰é”™è§‰çš„æœ‰æ•ˆæ€§ï¼Œçªç ´äº†ä¼ ç»Ÿè§†è§‰é”™è§‰ä¸»è¦ä¾èµ–ç©ºé—´æ“çºµçš„å±€é™ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12280" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12280.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12271</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12271" target="_blank">MonarchRT: Efficient Attention for Real-Time Video Generation</a>
      </h3>
      <p class="paper-title-zh">MonarchRTï¼šé¢å‘å®æ—¶è§†é¢‘ç”Ÿæˆçš„é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶</p>
      <p class="paper-authors">Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†Monarch-RTï¼Œä¸€ç§ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç»“æ„åŒ–æ³¨æ„åŠ›å‚æ•°åŒ–æ–¹æ³•ï¼Œé¦–æ¬¡åœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶å®ç°äº†é«˜è¾¾95%çš„æ³¨æ„åŠ›ç¨€ç–åº¦ï¼Œä»è€Œåœ¨å•å¼ RTX 5090 GPUä¸Šå®ç°äº†16 FPSçš„çœŸæ­£å®æ—¶è§†é¢‘ç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åŸºäºå¯¹è§†é¢‘æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†æï¼Œå‘ç°å…¶ç»“åˆäº†ç”±æ—¶ç©ºä½ç½®é©±åŠ¨çš„å‘¨æœŸæ€§ç»“æ„ã€åŠ¨æ€ç¨€ç–è¯­ä¹‰å¯¹åº”å…³ç³»ä»¥åŠå¯†é›†æ··åˆã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºä½¿ç”¨MonarchçŸ©é˜µå¯¹æ³¨æ„åŠ›è¿›è¡Œå› å­åˆ†è§£ï¼Œé€šè¿‡é€‚å½“å¯¹é½çš„å—ç»“æ„å’Œæ‰©å±•çš„å¹³é“ºMonarchå‚æ•°åŒ–ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°é«˜è¡¨è¾¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¾®è°ƒå’Œå®šåˆ¶çš„Tritonå†…æ ¸å…‹æœäº†å‚æ•°åŒ–çš„å¼€é”€ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒMonarch-RTåœ¨ä¸“ä¸ºåŒå‘æ¨¡å‹è®¾è®¡çš„ç°æœ‰ç¨€ç–åŸºçº¿æ–¹æ³•ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„æ•ˆèƒ½ã€‚å½“åº”ç”¨äºæœ€å…ˆè¿›çš„æ¨¡å‹Self-Forcingæ—¶ï¼ŒMonarch-RTåœ¨è´¨é‡æ— æŸçš„æƒ…å†µä¸‹å®ç°äº†é«˜è¾¾95%çš„æ³¨æ„åŠ›ç¨€ç–åº¦ã€‚å…¶ä¼˜åŒ–å®ç°åœ¨Nvidia RTX 5090ã€H100å’ŒB200 GPUä¸Šåˆ†åˆ«ä¼˜äºFlashAttention-2ã€FlashAttention-3å’ŒFlashAttention-4å†…æ ¸ï¼Œå†…æ ¸åŠ é€Ÿæ¯”è¾¾åˆ°1.4-11.8å€ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12271" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12271.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12221</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12221" target="_blank">Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching</a>
      </h3>
      <p class="paper-title-zh">ä¸¤å…¨å…¶ç¾ï¼šåŸºäºç»Ÿä¸€ç¦»æ•£æµåŒ¹é…çš„å¤šæ¨¡æ€æ¨ç†ä¸ç”Ÿæˆ</p>
      <p class="paper-authors">Onkar Susladkar, Tushar Prakash, Gayatri Deshmukh, Kiet A. Nguyen, Jiaxun Zhang ç­‰ (11 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†UniDFlowï¼Œä¸€ä¸ªç»Ÿä¸€çš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„ä½ç§©é€‚é…å™¨è§£è€¦å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆï¼Œå¹¶å¼•å…¥åŸºäºå‚è€ƒçš„å¤šæ¨¡æ€åå¥½å¯¹é½æ–¹æ³•ï¼Œåœ¨æ— éœ€å¤§è§„æ¨¡é‡è®­ç»ƒçš„æƒ…å†µä¸‹æå‡äº†ç”Ÿæˆç»“æœçš„å¿ å®åº¦ä¸å¯æ§æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é‡‡ç”¨ç»Ÿä¸€çš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡ã€‚é¦–å…ˆï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„ä½ç§©é€‚é…å™¨å°†ç†è§£ä¸ç”Ÿæˆç›®æ ‡è§£è€¦ï¼Œé¿å…ç›®æ ‡å†²çªå’Œè¡¨ç¤ºçº ç¼ ã€‚å…¶æ¬¡ï¼Œæå‡ºä¸€ç§åŸºäºå‚è€ƒçš„å¤šæ¨¡æ€åå¥½å¯¹é½ç­–ç•¥ï¼Œåœ¨ç›¸åŒæ¡ä»¶ä¸‹ä¼˜åŒ–ç›¸å¯¹è¾“å‡ºç»“æœï¼Œä»è€Œæ”¹å–„ç”Ÿæˆè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> UniDFlowåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœªç»è¿‡æ˜¾å¼ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå®Œæˆå›¾åƒä¿®å¤ã€ä¸Šä¸‹æ–‡å›¾åƒç”Ÿæˆã€åŸºäºå‚è€ƒçš„ç¼–è¾‘å’Œç»„åˆç”Ÿæˆç­‰ä»»åŠ¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12221" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12221.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12205</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12205" target="_blank">DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing</a>
      </h3>
      <p class="paper-title-zh">DeepGen 1.0ï¼šä¸€ç§ç”¨äºæ¨è¿›å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„è½»é‡çº§ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹</p>
      <p class="paper-authors">Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song ç­‰ (20 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªä»…å«50äº¿å‚æ•°çš„è½»é‡çº§ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹DeepGen 1.0ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½è¶…è¶Šæˆ–åª²ç¾è§„æ¨¡å¤§å¾—å¤šçš„æ¨¡å‹ï¼›åŒæ—¶å¼€æºäº†è®­ç»ƒä»£ç ã€æƒé‡ä¸æ•°æ®é›†ï¼Œä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç ”ç©¶æä¾›äº†é«˜æ•ˆã€é«˜æ€§èƒ½çš„æ›¿ä»£æ–¹æ¡ˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. æå‡ºå †å é€šé“æ¡¥æ¥ï¼ˆSCBï¼‰æ·±åº¦å¯¹é½æ¡†æ¶ï¼Œé€šè¿‡æå–è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šå±‚å±‚æ¬¡ç‰¹å¾å¹¶ä¸å¯å­¦ä¹ çš„â€˜æ€è€ƒä»¤ç‰Œâ€™èåˆï¼Œä¸ºç”Ÿæˆä¸»å¹²ç½‘ç»œæä¾›ç»“æ„åŒ–ã€å¯Œå«æ¨ç†çš„å¼•å¯¼ã€‚2. è®¾è®¡äº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ä¸‰é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆåœ¨å¤§è§„æ¨¡å›¾æ–‡å¯¹å’Œç¼–è¾‘ä¸‰å…ƒç»„ä¸Šè¿›è¡Œå¯¹é½é¢„è®­ç»ƒï¼›éšååœ¨é«˜è´¨é‡æ··åˆä»»åŠ¡ä¸Šè¿›è¡Œè”åˆç›‘ç£å¾®è°ƒï¼›æœ€åé‡‡ç”¨æ··åˆå¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ ï¼ˆMR-GRPOï¼‰è¿›ä¸€æ­¥æå‡ç”Ÿæˆè´¨é‡ä¸äººç±»åå¥½å¯¹é½ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> 1. å°½ç®¡ä»…ä½¿ç”¨çº¦5000ä¸‡æ ·æœ¬è®­ç»ƒï¼ŒDeepGen 1.0åœ¨å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­å–å¾—é¢†å…ˆæ€§èƒ½ï¼šåœ¨WISEåŸºå‡†ä¸Šè¶…è¶Š800äº¿å‚æ•°çš„HunyuanImageè¾¾28%ï¼Œåœ¨UniREditBenchä¸Šè¶…è¶Š270äº¿å‚æ•°çš„Qwen-Image-Editè¾¾37%ã€‚2. æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ã€è¯­ä¹‰ç†è§£ä¸ç»†ç²’åº¦æ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å®šçš„è®­ç»ƒè¿›ç¨‹å¹¶é¿å…äº†è§†è§‰ä¼ªå½±ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12205" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12205.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>