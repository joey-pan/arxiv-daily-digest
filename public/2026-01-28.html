<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-28</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.16836</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.16836" target="_blank">ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models</a>
      </h3>
      <p class="paper-title-zh">ColorConceptBenchï¼šä¸€ä¸ªç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æ¦‚ç‡æ€§é¢œè‰²-æ¦‚å¿µç†è§£èƒ½åŠ›çš„åŸºå‡†</p>
      <p class="paper-authors">Chenxi Ruan, Yu Xiao, Yihan Hou, Guosheng Hu, Wei Zeng</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªç³»ç»Ÿè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯¹éšå«é¢œè‰²æ¦‚å¿µç†è§£èƒ½åŠ›çš„äººå·¥æ ‡æ³¨åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æŠ½è±¡è¯­ä¹‰é¢œè‰²å…³è”ä¸Šçš„æ ¹æœ¬æ€§ç¼ºé™·ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ„å»ºäº†åŒ…å«1,281ä¸ªéšå«é¢œè‰²æ¦‚å¿µå’Œ6,369ä¸ªäººå·¥æ ‡æ³¨çš„æ•°æ®é›†ï¼Œé€šè¿‡æ¦‚ç‡æ€§é¢œè‰²åˆ†å¸ƒè€Œéæ˜¾å¼é¢œè‰²åç§°æ¥è¯„ä¼°æ¨¡å‹ã€‚å¯¹ä¸ƒç§ä¸»æµæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§æµ‹è¯•ï¼Œå¹¶å°è¯•äº†æ¨¡å‹ç¼©æ”¾ã€å¼•å¯¼å¢å¼ºç­‰æ ‡å‡†å¹²é¢„æªæ–½ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯¹æŠ½è±¡è¯­ä¹‰çš„é¢œè‰²å…³è”ç¼ºä¹æ•æ„Ÿæ€§ï¼Œä¸”è¿™ç§å±€é™æ€§æ— æ³•é€šè¿‡å¸¸è§„å¹²é¢„æ‰‹æ®µï¼ˆå¦‚æ‰©å¤§æ¨¡å‹è§„æ¨¡æˆ–å¢å¼ºå¼•å¯¼ï¼‰æœ‰æ•ˆæ”¹å–„ï¼Œè¡¨æ˜å®ç°ç±»äººçš„é¢œè‰²è¯­ä¹‰ç†è§£éœ€è¦æ¨¡å‹å­¦ä¹ ä¸è¡¨å¾éšå«æ„ä¹‰çš„æ–¹å¼å‘ç”Ÿæ ¹æœ¬æ€§è½¬å˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.16836" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.16836.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.19433</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.19433" target="_blank">RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming</a>
      </h3>
      <p class="paper-title-zh">RoamScene3Dï¼šé€šè¿‡è‡ªé€‚åº”å¯¹è±¡æ„ŸçŸ¥æ¼«æ¸¸å®ç°æ²‰æµ¸å¼æ–‡æœ¬åˆ°3Dåœºæ™¯ç”Ÿæˆ</p>
      <p class="paper-authors">Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†RoamScene3Dæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰æ¨ç†ä¸å‡ ä½•çº¦æŸç›¸ç»“åˆï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆ3Dåœºæ™¯æ—¶å­˜åœ¨çš„ç©ºé—´ç›²åŒºã€è½¨è¿¹åƒµåŒ–ä»¥åŠé®æŒ¡å†…å®¹æ¨æ–­å›°éš¾çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†åœºæ™¯çš„ä¸€è‡´æ€§ä¸çœŸå®æ„Ÿã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºç¼–ç å¯¹è±¡å…³ç³»çš„åœºæ™¯å›¾ï¼Œä»¥æŒ‡å¯¼ç›¸æœºæ„ŸçŸ¥æ˜¾è‘—å¯¹è±¡è¾¹ç•Œå¹¶è§„åˆ’è‡ªé€‚åº”çš„æ¼«æ¸¸è½¨è¿¹ã€‚å…¶æ¬¡ï¼Œé’ˆå¯¹é™æ€2Då…ˆéªŒçš„ä¸è¶³ï¼Œæå‡ºäº†è¿åŠ¨æ³¨å…¥ä¿®å¤æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨é›†æˆäº†çœŸå®ç›¸æœºè½¨è¿¹çš„åˆæˆå…¨æ™¯æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»è€Œèƒ½å¤Ÿé€‚åº”ç›¸æœºè¿åŠ¨ï¼Œæ›´åˆç†åœ°å¡«å……å› è§†è§’å˜åŒ–äº§ç”Ÿçš„ç©ºæ´ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆä¸€è‡´ä¸”é€¼çœŸçš„3Dåœºæ™¯æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚é€šè¿‡è¯­ä¹‰æ¨ç†å’Œå‡ ä½•çº¦æŸçš„æœ‰æ•ˆç»“åˆï¼ŒRoamScene3Dèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åœºæ™¯è¯­ä¹‰å¸ƒå±€ï¼Œå¹¶è‡ªé€‚åº”åœ°æ¢ç´¢åœºæ™¯ä»¥æ¨æ–­è¢«é®æŒ¡çš„å†…å®¹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.19433" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.19433.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.18585</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.18585" target="_blank">GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization</a>
      </h3>
      <p class="paper-title-zh">GimmBOï¼šåŸºäºè´å¶æ–¯ä¼˜åŒ–çš„äº¤äº’å¼ç”Ÿæˆå›¾åƒæ¨¡å‹èåˆ</p>
      <p class="paper-authors">Chenxi Liu, Selena Ling, Alec Jacobson</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåä¸ºGimmBOçš„äº¤äº’å¼ç³»ç»Ÿï¼Œé€šè¿‡åå¥½è´å¶æ–¯ä¼˜åŒ–ï¼ˆPBOï¼‰å¸®åŠ©ç”¨æˆ·é«˜æ•ˆæ¢ç´¢æ‰©æ•£æ¨¡å‹é€‚é…å™¨ï¼ˆadapterï¼‰çš„æƒé‡èåˆç©ºé—´ï¼Œè§£å†³äº†æ‰‹åŠ¨è°ƒèŠ‚æƒé‡æ—¶ç»´åº¦ç¾éš¾å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè´å¶æ–¯ä¼˜åŒ–åç«¯ï¼šé¦–å…ˆï¼Œé’ˆå¯¹çœŸå®ä½¿ç”¨ä¸­è§‚å¯Ÿåˆ°çš„æƒé‡ç¨€ç–æ€§å’ŒèŒƒå›´å—é™ç‰¹æ€§ï¼Œè®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„é‡‡æ ·ç­–ç•¥ï¼›å…¶æ¬¡ï¼Œé€šè¿‡äº¤äº’å¼åå¥½åé¦ˆå¼•å¯¼ä¼˜åŒ–è¿‡ç¨‹ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä»¥ç›´è§‚çš„åå¥½æ¯”è¾ƒï¼ˆè€Œéç²¾ç¡®æ•°å€¼ï¼‰æ¥æ¢ç´¢é«˜ç»´æƒé‡ç»„åˆç©ºé—´ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒGimmBOåœ¨æ¨¡æ‹Ÿç”¨æˆ·æµ‹è¯•å’ŒçœŸå®ç”¨æˆ·ç ”ç©¶ä¸­å‡è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€æ›´é«˜çš„æˆåŠŸç‡ï¼Œä¸”æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè´å¶æ–¯ä¼˜åŒ–å’Œçº¿æ€§æœç´¢åŸºçº¿ï¼›æ¡†æ¶è¿˜å±•ç¤ºäº†è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œå¯æ”¯æŒå¤šç§é€‚é…å™¨èåˆåœºæ™¯ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.18585" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.18585.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.16981</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.16981" target="_blank">SyncLight: Controllable and Consistent Multi-View Relighting</a>
      </h3>
      <p class="paper-title-zh">SyncLightï¼šå¯æ§ä¸”ä¸€è‡´çš„å¤šè§†è§’é‡å…‰ç…§</p>
      <p class="paper-authors">David Serrano-Lozano, Anand Bhattad, Luis Herranz, Jean-FranÃ§ois Lalonde, Javier Vazquez-Corral</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªèƒ½å¤Ÿåœ¨æœªæ ‡å®šçš„å¤šè§†è§’é™æ€åœºæ™¯å›¾åƒä¸­å®ç°å‚æ•°åŒ–ã€ä¸€è‡´é‡å…‰ç…§çš„æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰ç”Ÿæˆæ–¹æ³•åœ¨å¤šè§†è§’åº”ç”¨ä¸­éš¾ä»¥ä¿æŒä¸¥æ ¼å…‰ç…§ä¸€è‡´æ€§çš„å…³é”®é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæ½œåœ¨æ¡¥åŒ¹é…å…¬å¼è®­ç»ƒçš„å¤šè§†è§’æ‰©æ•£Transformeræ¨¡å‹ï¼Œé€šè¿‡å•æ¬¡æ¨ç†å³å¯å¯¹æ•´ä¸ªå›¾åƒé›†è¿›è¡Œé«˜ä¿çœŸé‡å…‰ç…§ã€‚è®­ç»ƒä½¿ç”¨äº†ä¸€ä¸ªå¤§è§„æ¨¡æ··åˆæ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„åˆæˆç¯å¢ƒï¼ˆæ¥è‡ªç°æœ‰èµ„æºå’Œæ–°è®¾è®¡åœºæ™¯ï¼‰ä»¥åŠåœ¨æ ‡å®šå…‰ç…§ä¸‹çš„é«˜ä¿çœŸå®ä¸–ç•Œå¤šè§†è§’é‡‡é›†æ•°æ®ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å°½ç®¡ä»…ä½¿ç”¨å›¾åƒå¯¹è¿›è¡Œè®­ç»ƒï¼ŒSyncLightèƒ½å¤Ÿé›¶æ ·æœ¬æ³›åŒ–åˆ°ä»»æ„æ•°é‡çš„è§†è§’ï¼Œæœ‰æ•ˆåœ°å°†å…‰ç…§å˜åŒ–ä¼ æ’­åˆ°æ‰€æœ‰è§†å›¾ä¸­ï¼Œä¸”æ— éœ€ç›¸æœºä½å§¿ä¿¡æ¯ã€‚è¿™ä¸ºå¤šè§†è§’é‡‡é›†ç³»ç»Ÿå®ç°äº†å®ç”¨çš„é‡å…‰ç…§å·¥ä½œæµç¨‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.16981" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.16981.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.19717</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.19717" target="_blank">DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization</a>
      </h3>
      <p class="paper-title-zh">DiffStyle3Dï¼šé€šè¿‡æ³¨æ„åŠ›ä¼˜åŒ–å®ç°ä¸€è‡´çš„3Dé«˜æ–¯é£æ ¼åŒ–</p>
      <p class="paper-authors">Yitong Yang, Xuexin Liu, Yinglin Wang, Jing Wang, Hao Dou ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹3Dé«˜æ–¯é£æ ¼åŒ–èŒƒå¼ï¼Œé€šè¿‡ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œä¼˜åŒ–ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆå¼•å…¥æ³¨æ„åŠ›æ„ŸçŸ¥æŸå¤±ï¼Œé€šè¿‡åœ¨è‡ªæ³¨æ„åŠ›ç©ºé—´å¯¹é½é£æ ¼ç‰¹å¾å¹¶ä¿ç•™å†…å®¹ç‰¹å¾æ¥å®ç°é£æ ¼è¿ç§»ã€‚å…¶æ¬¡ï¼Œå—3Dé£æ ¼åŒ–å‡ ä½•ä¸å˜æ€§å¯å‘ï¼Œæå‡ºå‡ ä½•å¼•å¯¼çš„å¤šè§†å›¾ä¸€è‡´æ€§æ–¹æ³•ï¼Œå°†å‡ ä½•ä¿¡æ¯èå…¥è‡ªæ³¨æ„åŠ›ä»¥å»ºæ¨¡è·¨è§†å›¾å¯¹åº”å…³ç³»ã€‚æ­¤å¤–ï¼ŒåŸºäºå‡ ä½•ä¿¡æ¯æ„å»ºå‡ ä½•æ„ŸçŸ¥æ©ç ï¼Œé¿å…å¤šè§†å›¾é‡å åŒºåŸŸçš„å†—ä½™ä¼˜åŒ–ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiffStyle3Dåœ¨é£æ ¼åŒ–è´¨é‡å’Œè§†è§‰çœŸå®æ„Ÿæ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´ä¸€è‡´ä¸”é«˜è´¨é‡çš„é£æ ¼åŒ–3Då†…å®¹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.19717" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.19717.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>