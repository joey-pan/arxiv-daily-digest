<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-28</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17868</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17868" target="_blank">VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding</a>
      </h3>
      <p class="paper-title-zh">VidLaDAï¼šåŸºäºåŒå‘æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè§†é¢‘ç†è§£æ–¹æ³•</p>
      <p class="paper-authors">Zhihao He, Tieyuan Chen, Kangyu Wang, Ziran Qin, Yang Shao ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºé¦–ä¸ªåŸºäºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVidLaDAï¼‰ï¼Œé€šè¿‡åŒå‘æ³¨æ„åŠ›æœºåˆ¶å…‹æœä¼ ç»Ÿè‡ªå›å½’æ¨¡å‹çš„å› æœæ©ç åå·®ï¼›å¹¶è®¾è®¡äº†MARS-Cacheæ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡è§£ç é€Ÿåº¦ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. é‡‡ç”¨æ‰©æ•£è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œåˆ©ç”¨åŒå‘æ³¨æ„åŠ›æ•è·è§†é¢‘æ—¶ç©ºåºåˆ—ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚2. æå‡ºMARS-Cacheæ¡†æ¶ï¼Œé€šè¿‡å¼‚æ­¥è§†è§‰ç¼“å­˜åˆ·æ–°æœºåˆ¶åŠ¨æ€æ›´æ–°å…³é”®ç‰¹å¾ã€‚3. ç»“åˆå¸§çº§åˆ†å—æ³¨æ„åŠ›ä¸é”šç‚¹ä»¤ç‰ŒæŠ€æœ¯ï¼Œåœ¨å‰ªæå†—ä½™ä¿¡æ¯çš„åŒæ—¶ä¿æŒå…¨å±€æ—¶ç©ºå…³è”æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> 1. VidLaDAåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ‰©æ•£åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½åª²ç¾Qwen2.5-VLã€LLaVA-Videoç­‰å‰æ²¿è‡ªå›å½’æ¨¡å‹ã€‚2. MARS-Cacheå®ç°è¶…è¿‡12å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸”ä¸å½±å“æ¨¡å‹æ¨ç†ç²¾åº¦ã€‚3. å¼€æºä»£ç ä¸æ¨¡å‹æƒé‡å·²å‘å¸ƒã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17868" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17868.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17830</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17830" target="_blank">VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</a>
      </h3>
      <p class="paper-title-zh">VAE-REPAï¼šåŸºäºå˜åˆ†è‡ªç¼–ç å™¨è¡¨å¾å¯¹é½çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹è®­ç»ƒ</p>
      <p class="paper-authors">Mengmeng Wang, Dengyang Jiang, Liuzhuozheng Li, Yucheng Lin, Guojiang Shen ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§è½»é‡çº§çš„å†…åœ¨å¼•å¯¼æ¡†æ¶VAE-REPAï¼Œé€šè¿‡å°†æ‰©æ•£å˜æ¢å™¨çš„ä¸­é—´ç‰¹å¾ä¸é¢„è®­ç»ƒVAEç‰¹å¾å¯¹é½ï¼Œæ˜¾è‘—åŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨è¡¨å¾ç¼–ç å™¨æˆ–åŒæ¨¡å‹æ¶æ„ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åˆ©ç”¨ç°æˆçš„é¢„è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æå–çš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å› VAEçš„é‡å»ºç‰¹æ€§è€Œå¤©ç„¶ç¼–ç äº†ä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ã€ç»“æ„æ¨¡å¼å’ŒåŸºç¡€è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„æŠ•å½±å±‚ï¼Œå°†æ‰©æ•£å˜æ¢å™¨çš„ä¸­é—´æ½œåœ¨ç‰¹å¾ä¸VAEç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå¹¶ä½¿ç”¨ç‰¹å¾å¯¹é½æŸå¤±è¿›è¡Œç›‘ç£ã€‚æ•´ä¸ªè®¾è®¡æ— éœ€é¢å¤–è¡¨å¾ç¼–ç å™¨æˆ–ç»´æŠ¤åŒæ¨¡å‹ï¼Œå®ç°äº†ç®€å•é«˜æ•ˆçš„è®­ç»ƒæµç¨‹ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒVAE-REPAç›¸æ¯”åŸå§‹æ‰©æ•£å˜æ¢å™¨ï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ”¶æ•›é€Ÿåº¦ä¸Šå‡æœ‰æå‡ï¼›å…¶æ€§èƒ½åŒ¹é…æˆ–ä¼˜äºç°æœ‰å…ˆè¿›çš„åŠ é€Ÿæ–¹æ³•ï¼Œä¸”ä»…å¢åŠ çº¦4%çš„è®¡ç®—å¼€é”€ï¼ˆGFLOPsï¼‰ï¼ŒåŒæ—¶å®Œå…¨é¿å…äº†å¤–éƒ¨å¼•å¯¼æ¨¡å‹å¸¦æ¥çš„é¢å¤–æˆæœ¬ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17830" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17830.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17733</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17733" target="_blank">Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles</a>
      </h3>
      <p class="paper-title-zh">ç®€åŒ–å¤æ‚æ€§ï¼šåŸºäºç»„åˆå¼k-å•å…ƒç²’å­çš„è”åˆB-Repç”Ÿæˆ</p>
      <p class="paper-authors">Junran Lu, Yuanqi Li, Hengji Li, Jie Guo, Yanwen Guo</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§å°†è¾¹ç•Œè¡¨ç¤ºï¼ˆB-Repï¼‰é‡æ„ä¸ºç»„åˆå¼k-å•å…ƒç²’å­é›†åˆçš„æ–°èŒƒå¼ï¼Œå®ç°äº†æ‹“æ‰‘ä¸å‡ ä½•çš„è”åˆç”Ÿæˆï¼Œå¹¶å¢å¼ºäº†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†æ¯ä¸ªæ‹“æ‰‘å®ä½“ï¼ˆå¦‚é¡¶ç‚¹ã€è¾¹ã€é¢ï¼‰ç¼–ç ä¸ºä¸€ç»„ç»„åˆç²’å­ï¼Œç›¸é‚»å•å…ƒåœ¨å…¶å…±äº«è¾¹ç•Œå¤„ä½¿ç”¨ç›¸åŒçš„æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›å‡ ä½•è€¦åˆã€‚é€šè¿‡æ‰“ç ´ä¼ ç»Ÿå±‚çº§ç»“æ„ï¼Œç»Ÿä¸€è¡¨ç¤ºä¸åŒé˜¶çš„å•å…ƒï¼Œå¹¶é‡‡ç”¨å¤šæ¨¡æ€æµåŒ¹é…æ¡†æ¶æ¥åˆæˆç²’å­é›†åˆï¼Œä»¥æ”¯æŒæ— æ¡ä»¶ç”Ÿæˆå’Œæ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ç”Ÿæˆé«˜ä¿çœŸã€æœ‰æ•ˆæ€§æ›´å¼ºçš„CADæ¨¡å‹ï¼Œåœ¨ç¼–è¾‘æ€§ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½ç›´æ¥åˆæˆéæµå½¢ç»“æ„ï¼ˆå¦‚çº¿æ¡†ï¼‰ï¼ŒåŒæ—¶è‡ªç„¶åœ°æ”¯æŒå±€éƒ¨ä¿®å¤ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Boundary Representation (B-Rep) is the widely adopted standard in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness. We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17733" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17733.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17340</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17340" target="_blank">TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution</a>
      </h3>
      <p class="paper-title-zh">TEXTS-Diffï¼šé¢å‘çœŸå®ä¸–ç•Œæ–‡æœ¬å›¾åƒè¶…åˆ†è¾¨ç‡çš„æ–‡æœ¬æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹</p>
      <p class="paper-authors">Haodong He, Xin Zhan, Yancheng Bai, Rui Lan, Lei Sun ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡çš„çœŸå®ä¸–ç•Œæ–‡æœ¬å›¾åƒæ•°æ®é›†Real-Textsï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–‡æœ¬æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹TEXTS-Diffï¼Œä»¥åŒæ—¶æå‡èƒŒæ™¯å’Œæ–‡æœ¬åŒºåŸŸçš„è¶…åˆ†è¾¨ç‡é‡å»ºè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡æ„å»ºåŒ…å«ä¸­è‹±æ–‡è‡ªç„¶æ–‡æœ¬å®ä¾‹çš„Real-Textsæ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†ä¸­æ–‡æœ¬å›¾åƒç¨€ç¼ºå’ŒèƒŒæ™¯é‡å»ºå—é™çš„é—®é¢˜ã€‚TEXTS-Diffæ¨¡å‹åˆ©ç”¨æŠ½è±¡æ¦‚å¿µå¢å¼ºå¯¹è§†è§‰åœºæ™¯ä¸­æ–‡æœ¬å…ƒç´ çš„ç†è§£ï¼Œå¹¶ç»“åˆå…·ä½“æ–‡æœ¬åŒºåŸŸæ¥ç»†åŒ–æ–‡æœ¬ç»†èŠ‚ã€‚è¯¥è®¾è®¡æ—¨åœ¨å‡å°‘æ–‡æœ¬åŒºåŸŸçš„å¤±çœŸå’Œå¹»è§‰ä¼ªå½±ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„è§†è§‰åœºæ™¯ä¿çœŸåº¦ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›å’Œæ–‡æœ¬æ¢å¤å‡†ç¡®æ€§ã€‚æ¨¡å‹ã€ä»£ç åŠæ•°æ®é›†å°†å…¨éƒ¨å¼€æºã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17340" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17340.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17228</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17228" target="_blank">Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification</a>
      </h3>
      <p class="paper-title-zh">åŸºäºæ½œåœ¨æ‰©æ•£çš„åŠç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•ç”¨äºç—…ç†å›¾åƒåˆ†ç±»</p>
      <p class="paper-authors">Tengyue Zhang, Ruiwen Ding, Luoting Zhuang, Yuxiao Wu, Erika F. Rodriguez ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åŠç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ—¢ä¿ç•™æºåŸŸç»„ç»‡ç»“æ„ã€åˆå…·å¤‡ç›®æ ‡åŸŸå¤–è§‚ç‰¹å¾çš„åˆæˆå›¾åƒï¼Œæœ‰æ•ˆæå‡äº†è®¡ç®—ç—…ç†å­¦æ¨¡å‹çš„åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åœ¨æºåŸŸå’Œç›®æ ‡åŸŸçš„æœªæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å°†åŸºç¡€æ¨¡å‹ç‰¹å¾ã€é˜Ÿåˆ—èº«ä»½å’Œç»„ç»‡åˆ¶å¤‡æ–¹æ³•ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ç”Ÿæˆçš„åˆæˆå›¾åƒåœ¨ä¿æŒæºåŸŸç»„ç»‡ç»“æ„çš„åŒæ—¶ï¼Œå¼•å…¥äº†ç›®æ ‡åŸŸçš„å¤–è§‚ç‰¹æ€§ã€‚éšåï¼Œè¿™äº›åˆæˆå›¾åƒä¸æºåŸŸçš„çœŸå®æ ‡è®°å›¾åƒå…±åŒç”¨äºè®­ç»ƒä¸‹æ¸¸åˆ†ç±»å™¨ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨è‚ºè…ºç™Œé¢„åé¢„æµ‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç›®æ ‡åŸŸæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œä¸”æœªæŸå®³æºåŸŸæ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œç›®æ ‡åŸŸæµ‹è¯•é›†çš„åŠ æƒF1åˆ†æ•°ä»0.611æå‡è‡³0.706ï¼Œå®è§‚F1åˆ†æ•°ä»0.641æå‡è‡³0.716ï¼Œè¯æ˜äº†åŸºäºç›®æ ‡æ„ŸçŸ¥çš„æ‰©æ•£åˆæˆæ•°æ®å¢å¼ºå¯¹äºæ”¹å–„è®¡ç®—ç—…ç†å­¦åŸŸæ³›åŒ–çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17228" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17228.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>