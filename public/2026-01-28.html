<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-28</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.19785</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.19785" target="_blank">GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance</a>
      </h3>
      <p class="paper-title-zh">GeoDiff3Dï¼šåŸºäºå‡ ä½•çº¦æŸçš„2Dæ‰©æ•£å¼•å¯¼çš„è‡ªç›‘ç£3Dåœºæ™¯ç”Ÿæˆ</p>
      <p class="paper-authors">Haozhi Zhu, Miaomiao Zhao, Dingyao Liu, Runze Tian, Yan Zhang ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„è‡ªç›‘ç£æ¡†æ¶GeoDiff3Dï¼Œå®ƒåˆ©ç”¨ç²—ç³™å‡ ä½•ä½œä¸ºç»“æ„é”šç‚¹ï¼Œå¹¶é€šè¿‡å‡ ä½•çº¦æŸçš„2Dæ‰©æ•£æ¨¡å‹æä¾›çº¹ç†ä¸°å¯Œçš„å‚è€ƒå›¾åƒï¼Œæ˜¾è‘—é™ä½äº†å¯¹å¤§è§„æ¨¡çœŸå®æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨ç²—ç³™å‡ ä½•ä½“ä½œä¸ºåœºæ™¯çš„ç»“æ„åŸºç¡€ï¼›ç„¶åé€šè¿‡ä¸€ä¸ªå‡ ä½•çº¦æŸçš„2Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šè§†è§’çš„çº¹ç†å‚è€ƒå›¾åƒï¼Œä¸”ä¸ä¸¥æ ¼è¦æ±‚è¿™äº›å›¾åƒä¹‹é—´å…·æœ‰ä¸¥æ ¼çš„ä¸€è‡´æ€§ï¼›æœ€åé€šè¿‡ä½“ç´ å¯¹é½çš„3Dç‰¹å¾èšåˆå’ŒåŒé‡è‡ªç›‘ç£æœºåˆ¶ï¼Œæ•´åˆå¤šè§†è§’ä¿¡æ¯å¹¶ä¿æŒåœºæ™¯çš„è¿è´¯æ€§ä¸ç»†èŠ‚ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å¤æ‚åœºæ™¯ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGeoDiff3Dç›¸æ¯”ç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šå‡æœ‰æå‡ï¼Œèƒ½æœ‰æ•ˆå‡å°‘ç»“æ„ä¼ªå½±å’Œå‡ ä½•ä¸ä¸€è‡´é—®é¢˜ï¼ŒåŒæ—¶ä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°å¿«é€Ÿã€é«˜è´¨é‡çš„3Dåœºæ™¯ç”Ÿæˆã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.19785" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.19785.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.19577</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.19577" target="_blank">MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation</a>
      </h3>
      <p class="paper-title-zh">MaDiSï¼šé©¾é©­æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹ç”¨äºæ‰‹è¯­ç”Ÿæˆ</p>
      <p class="paper-authors">Ronglai Zuo, Rolandos Alexandros Potamias, Qi Sun, Evangelos Ververas, Jiankang Deng ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åŸºäºæ©ç æ‰©æ•£çš„æ‰‹è¯­ç”Ÿæˆè¯­è¨€æ¨¡å‹ï¼ˆMaDiSï¼‰ï¼Œé€šè¿‡åŒå‘ä¾èµ–å»ºæ¨¡å’Œå¹¶è¡Œå¤šä»¤ç‰Œç”Ÿæˆï¼Œè§£å†³äº†è‡ªå›å½’æ¨¡å‹å•å‘ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ä¸‰å±‚æ¬¡è·¨æ¨¡æ€é¢„è®­ç»ƒæ–¹æ¡ˆä»¥å­¦ä¹ æ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> MaDiSé‡‡ç”¨æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹è¢«æ©ç çš„ä»¤ç‰Œæ¥æ•è·åŒå‘ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œæ”¯æŒå¹¶è¡Œç”Ÿæˆå¤šä¸ªä»¤ç‰Œä»¥åŠ é€Ÿæ¨ç†ã€‚æ¨¡å‹è®¾è®¡äº†ä¸‰å±‚æ¬¡è·¨æ¨¡æ€é¢„è®­ç»ƒï¼ˆä»¤ç‰Œç©ºé—´ã€æ½œåœ¨ç©ºé—´å’Œ3Dç‰©ç†ç©ºé—´ï¼‰ï¼Œå¹¶æå‡ºäº†å¸¦æ—¶é—´æ£€æŸ¥ç‚¹çš„å»æ©ç ç­–ç•¥ä»¥å¤§å¹…é™ä½è®­ç»ƒå¤æ‚åº¦ï¼ŒåŒæ—¶ä½¿ç”¨æ··åˆéƒ¨ä»¶åµŒå…¥å±‚é€šè¿‡å¯å­¦ä¹ é—¨æ§å’Œä¼˜åŒ–ç æœ¬èåˆä¸åŒèº«ä½“éƒ¨ä½çš„ä»¤ç‰Œä¿¡æ¯ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨CSL-Dailyã€Phoenix-2014Tå’ŒHow2Signæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMaDiSåœ¨DTWè¯¯å·®ä»¥åŠæ–°æå‡ºçš„SiBLEUå’ŒSiCLIPç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šå‡å–å¾—ä¼˜è¶Šæ€§èƒ½ï¼ŒåŒæ—¶å°†æ¨ç†å»¶è¿Ÿé™ä½äº†è¿‘30%ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Sign language generation (SLG) aims to translate written texts into expressive sign motions, bridging communication barriers for the Deaf and Hard-of-Hearing communities. Recent studies formulate SLG within the language modeling framework using autoregressive language models, which suffer from unidirectional context modeling and slow token-by-token inference. To address these limitations, we present MaDiS, a masked-diffusion-based language model for SLG that captures bidirectional dependencies and supports efficient parallel multi-token generation. We further introduce a tri-level cross-modal pretraining scheme that jointly learns from token-, latent-, and 3D physical-space objectives, leading to richer and more grounded sign representations. To accelerate model convergence in the fine-tuning stage, we design a novel unmasking strategy with temporal checkpoints, reducing the combinatorial complexity of unmasking orders by over $10^{41}$ times. In addition, a mixture-of-parts embedding layer is developed to effectively fuse information stored in different part-wise sign tokens through learnable gates and well-optimized codebooks. Extensive experiments on CSL-Daily, Phoenix-2014T, and How2Sign demonstrate that MaDiS achieves superior performance across multiple metrics, including DTW error and two newly introduced metrics, SiBLEU and SiCLIP, while reducing inference latency by nearly 30%. Code and models will be released on our project page.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.19577" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.19577.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.18260</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.18260" target="_blank">Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images</a>
      </h3>
      <p class="paper-title-zh">ä»æ·±åº¦åˆ°è§£å‰–ï¼šåŸºäºä½“è¡¨æ·±åº¦å›¾åƒå­¦ä¹ å†…éƒ¨å™¨å®˜å®šä½</p>
      <p class="paper-authors">Eytan Kats, Kai Geissler, Daniel Mensing, Jochen G. Hirsch, Stefan Heldman ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºä¸€ä¸ªåŸºäºå­¦ä¹ çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»å•å¼ ä½“è¡¨äºŒç»´æ·±åº¦å›¾åƒé¢„æµ‹å¤šä¸ªå†…éƒ¨å™¨å®˜çš„ä¸‰ç»´ä½ç½®å’Œå½¢çŠ¶ï¼Œä¸ºè‡ªåŠ¨åŒ–æ‚£è€…æ‘†ä½æä¾›æ–°æ–¹æ³•ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> åˆ©ç”¨å¤§è§„æ¨¡å…¨èº«MRIæ‰«ææ•°æ®é›†ï¼Œåˆæˆä¸è§£å‰–åˆ†å‰²ç»“æœé…å¯¹çš„æ·±åº¦å›¾åƒï¼›é‡‡ç”¨ç»Ÿä¸€çš„å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ˜¾å¼è¿›è¡Œè¡¨é¢é‡å»ºï¼›è¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶å®šä½éª¨éª¼å’Œè½¯ç»„ç»‡ç­‰å¤šç§è§£å‰–ç»“æ„ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å‡†ç¡®é¢„æµ‹å¤šç§å†…éƒ¨å™¨å®˜çš„ä½ç½®ä¸å½¢çŠ¶ï¼ŒéªŒè¯äº†å°†æ·±åº¦ä¼ æ„Ÿå™¨é›†æˆåˆ°æ”¾å°„å­¦å·¥ä½œæµç¨‹ä¸­ä»¥ä¼˜åŒ–æ‰«æç¨‹åºã€é€šè¿‡è‡ªåŠ¨åŒ–æ‚£è€…æ‘†ä½æå‡æ‚£è€…ä½“éªŒçš„æ½œåŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.18260" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.18260.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.18168</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.18168" target="_blank">TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration</a>
      </h3>
      <p class="paper-title-zh">TempDiffRegï¼šç”¨äºéåˆšæ€§2D-3Dè¡€ç®¡é…å‡†çš„æ—¶åºæ‰©æ•£æ¨¡å‹</p>
      <p class="paper-authors">Zehua Liu, Shihao Zou, Jincai Huang, Yanfang Zhang, Chao Tong ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†çš„è¡€ç®¡é…å‡†ç­–ç•¥ï¼Œé€šè¿‡å¼•å…¥ç»“æ„æ„ŸçŸ¥é€è§†nç‚¹ï¼ˆSA-PnPï¼‰å…¨å±€å¯¹é½æ¨¡å—å’Œæ—¶åºæ‰©æ•£æ¨¡å‹ï¼ˆTempDiffRegï¼‰ï¼Œæ˜¾è‘—æå‡äº†ç»åŠ¨è„‰åŒ–ç–—æ “å¡æœ¯ï¼ˆTACEï¼‰ä¸­2D-3Dè¡€ç®¡é…å‡†çš„å‡†ç¡®æ€§ä¸è§£å‰–åˆç†æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é‡‡ç”¨ä»ç²—åˆ°ç»†çš„ä¸¤é˜¶æ®µç­–ç•¥ï¼šé¦–å…ˆï¼Œä½¿ç”¨ç»“æ„æ„ŸçŸ¥é€è§†nç‚¹ï¼ˆSA-PnPï¼‰æ¨¡å—è¿›è¡Œå…¨å±€å¯¹é½ï¼Œå»ºç«‹2Dä¸3Dè¡€ç®¡ç»“æ„çš„å¯¹åº”å…³ç³»ï¼›å…¶æ¬¡ï¼Œæå‡ºæ—¶åºæ‰©æ•£æ¨¡å‹ï¼ˆTempDiffRegï¼‰ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ—¶åºä¸Šä¸‹æ–‡ä¿¡æ¯è¿­ä»£åœ°è¿›è¡Œè¡€ç®¡å½¢å˜ï¼Œä»¥æ•æ‰å¤æ‚çš„è§£å‰–å˜å¼‚å’Œå±€éƒ¨ç»“æ„å˜åŒ–ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨åŒ…å«23åæ‚£è€…ã€626ä¸ªå¤šå¸§é…å¯¹æ ·æœ¬çš„æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨é…å‡†ç²¾åº¦å’Œè§£å‰–åˆç†æ€§ä¸Šå‡ä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œå…¶å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸º0.63æ¯«ç±³ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º0.51æ¯«ç±³ï¼Œç›¸æ¯”æœ€å…·ç«äº‰åŠ›çš„ç°æœ‰æ–¹æ³•ï¼ŒMSEé™ä½äº†66.7%ï¼ŒMAEé™ä½äº†17.7%ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.18168" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.18168.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17868</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17868" target="_blank">VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding</a>
      </h3>
      <p class="paper-title-zh">VidLaDAï¼šåŸºäºåŒå‘æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè§†é¢‘ç†è§£æ–¹æ³•</p>
      <p class="paper-authors">Zhihao He, Tieyuan Chen, Kangyu Wang, Ziran Qin, Yang Shao ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªåŸºäºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVidLaDAï¼‰ï¼Œé€šè¿‡åŒå‘æ³¨æ„åŠ›æœºåˆ¶å…‹æœä¼ ç»Ÿè‡ªå›å½’æ¨¡å‹ä¸­çš„å› æœæ©ç åå·®ï¼›å¹¶è®¾è®¡äº†MARS-Cacheæ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡è§£ç é€Ÿåº¦ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. é‡‡ç”¨æ‰©æ•£è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œåˆ©ç”¨åŒå‘æ³¨æ„åŠ›æ•è·è§†é¢‘æ—¶ç©ºåºåˆ—ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚2. æå‡ºMARS-Cacheæ¡†æ¶ï¼Œé€šè¿‡å¼‚æ­¥è§†è§‰ç¼“å­˜åˆ·æ–°æœºåˆ¶åŠ¨æ€æ›´æ–°å…³é”®ç‰¹å¾ã€‚3. ç»“åˆå¸§çº§åˆ†å—æ³¨æ„åŠ›ä¸é”šç‚¹ä»¤ç‰ŒæŠ€æœ¯ï¼Œåœ¨å‰ªæå†—ä½™ä¿¡æ¯çš„åŒæ—¶ç»´æŒå…¨å±€æ—¶ç©ºå…³è”æ€§ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> 1. VidLaDAåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ‰©æ•£åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½åª²ç¾Qwen2.5-VLã€LLaVA-Videoç­‰å‰æ²¿è‡ªå›å½’æ¨¡å‹ã€‚2. MARS-Cacheå®ç°è¶…è¿‡12å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸”æœªæŸå®³æ¨¡å‹æ¨ç†ç²¾åº¦ã€‚3. å¼€æºä»£ç ä¸æ¨¡å‹æƒé‡å·²å‘å¸ƒã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17868" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17868.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>