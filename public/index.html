<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-28</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.19180</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.19180" target="_blank">SNR-Edit: Structure-Aware Noise Rectification for Inversion-Free Flow-Based Editing</a>
      </h3>
      <p class="paper-title-zh">SNR-Editï¼šé¢å‘å…åæ¼”åŸºäºæµçš„å›¾åƒç¼–è¾‘çš„ç»“æ„æ„ŸçŸ¥å™ªå£°æ ¡æ­£</p>
      <p class="paper-authors">Lifan Jiang, Boxi Wu, Yuhang Pei, Tianrun Wu, Yongyuan Chen ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†SNR-Editï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒã€å…åæ¼”çš„æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å™ªå£°æ§åˆ¶å®ç°ç²¾ç¡®çš„æ½œåœ¨è½¨è¿¹æ ¡æ­£ï¼Œè§£å†³äº†ç°æœ‰åŸºäºæµçš„å…åæ¼”ç¼–è¾‘æ–¹æ³•å› å›ºå®šé«˜æ–¯å™ªå£°å¯¼è‡´çš„è½¨è¿¹åå·®å’Œç»“æ„é€€åŒ–é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é‡‡ç”¨ç»“æ„æ„ŸçŸ¥å™ªå£°æ ¡æ­£æœºåˆ¶ï¼Œå°†åˆ†å‰²çº¦æŸæ³¨å…¥åˆå§‹å™ªå£°ä¸­ï¼Œä»è€Œå°†æºè½¨è¿¹çš„éšæœºæˆåˆ†é”šå®šåˆ°çœŸå®å›¾åƒçš„éšå¼åæ¼”ä½ç½®ã€‚è¿™ä¸€è½»é‡çº§ä¿®æ”¹å‡å°‘äº†æº-ç›®æ ‡ä¼ è¾“è¿‡ç¨‹ä¸­çš„è½¨è¿¹æ¼‚ç§»ï¼Œæ— éœ€æ¨¡å‹è°ƒä¼˜æˆ–åæ¼”æ“ä½œå³å¯ç”Ÿæˆæ›´å¹³æ»‘çš„æ½œåœ¨è½¨è¿¹ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨SD3å’ŒFLUXæ¨¡å‹ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSNR-Editåœ¨PIE-Benchå’ŒSNR-Benchä¸Šå–å¾—äº†ä¼˜å¼‚çš„åƒç´ çº§æŒ‡æ ‡å’ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„åˆ†æ€§èƒ½ï¼ŒåŒæ—¶æ¯å¼ å›¾åƒä»…å¢åŠ çº¦1ç§’çš„è®¡ç®—å¼€é”€ï¼Œå®ç°äº†é«˜ä¿çœŸçš„ç»“æ„ä¿æŒã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Inversion-free image editing using flow-based generative models challenges the prevailing inversion-based pipelines. However, existing approaches rely on fixed Gaussian noise to construct the source trajectory, leading to biased trajectory dynamics and causing structural degradation or quality loss. To address this, we introduce SNR-Edit, a training-free framework achieving faithful Latent Trajectory Correction via adaptive noise control. Mechanistically, SNR-Edit uses structure-aware noise rectification to inject segmentation constraints into the initial noise, anchoring the stochastic component of the source trajectory to the real image's implicit inversion position and reducing trajectory drift during source--target transport. This lightweight modification yields smoother latent trajectories and ensures high-fidelity structural preservation without requiring model tuning or inversion. Across SD3 and FLUX, evaluations on PIE-Bench and SNR-Bench show that SNR-Edit delivers performance on pixel-level metrics and VLM-based scoring, while adding only about 1s overhead per image.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.19180" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.19180.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.19115</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.19115" target="_blank">FBSDiff++: Improved Frequency Band Substitution of Diffusion Features for Efficient and Highly Controllable Text-Driven Image-to-Image Translation</a>
      </h3>
      <p class="paper-title-zh">FBSDiff++ï¼šæ”¹è¿›çš„æ‰©æ•£ç‰¹å¾é¢‘å¸¦æ›¿æ¢ï¼Œç”¨äºé«˜æ•ˆä¸”é«˜åº¦å¯æ§çš„æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘</p>
      <p class="paper-authors">Xiang Gao, Yunpeng Jia</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€å³æ’å³ç”¨çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç¿»è¯‘æ¡†æ¶FBSDiff++ï¼Œé€šè¿‡æ”¹è¿›çš„é¢‘å¸¦æ›¿æ¢æœºåˆ¶ï¼Œåœ¨æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹ç¿»è¯‘å¼ºåº¦ã€å±€éƒ¨ç¼–è¾‘å’Œé£æ ¼åˆ›å»ºçš„çµæ´»æ§åˆ¶ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•ä»é¢‘åŸŸè§†è§’å‡ºå‘ï¼Œé€šè¿‡åŠ¨æ€æ›¿æ¢æ½œåœ¨æ‰©æ•£ç‰¹å¾çš„ä¸åŒé¢‘å¸¦ï¼ˆä½é¢‘ã€ä¸­é¢‘ã€é«˜é¢‘ï¼‰æ¥åˆ†åˆ«å®ç°å¤–è§‚ã€å¸ƒå±€å’Œè½®å»“å¼•å¯¼çš„å›¾åƒç¿»è¯‘ã€‚FBSDiff++åœ¨åŸå§‹FBSDiffåŸºç¡€ä¸Šè¿›è¡Œäº†ä¸‰æ–¹é¢æ”¹è¿›ï¼šä¼˜åŒ–æ¨¡å‹æ¶æ„ä»¥å¤§å¹…åŠ é€Ÿæ¨ç†ï¼›æ”¹è¿›é¢‘å¸¦æ›¿æ¢æ¨¡å—ä»¥æ”¯æŒä»»æ„åˆ†è¾¨ç‡å’Œå®½é«˜æ¯”çš„è¾“å…¥ï¼›é€šè¿‡å¾®è°ƒæ ¸å¿ƒæ–¹æ³•æ‰©å±•äº†å±€éƒ¨ç¼–è¾‘å’Œé£æ ¼åŒ–å†…å®¹åˆ›å»ºçš„åŠŸèƒ½ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒFBSDiff++åœ¨è§†è§‰è´¨é‡ã€æ•ˆç‡ã€å¤šåŠŸèƒ½æ€§å’Œå¯æ§æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼Œå…¶ä¸­æ¨ç†é€Ÿåº¦æå‡äº†8.9å€ï¼Œå¹¶èƒ½çµæ´»å®ç°è¿ç»­çš„ç›¸å…³å¼ºåº¦æ§åˆ¶åŠå±€éƒ¨å›¾åƒæ“ä½œã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>With large-scale text-to-image (T2I) diffusion models achieving significant advancements in open-domain image creation, increasing attention has been focused on their natural extension to the realm of text-driven image-to-image (I2I) translation, where a source image acts as visual guidance to the generated image in addition to the textual guidance provided by the text prompt. We propose FBSDiff, a novel framework adapting off-the-shelf T2I diffusion model into the I2I paradigm from a fresh frequency-domain perspective. Through dynamic frequency band substitution of diffusion features, FBSDiff realizes versatile and highly controllable text-driven I2I in a plug-and-play manner (without need for model training, fine-tuning, or online optimization), allowing appearance-guided, layout-guided, and contour-guided I2I translation by progressively substituting low-frequency band, mid-frequency band, and high-frequency band of latent diffusion features, respectively. In addition, FBSDiff flexibly enables continuous control over I2I correlation intensity simply by tuning the bandwidth of the substituted frequency band. To further promote image translation efficiency, flexibility, and functionality, we propose FBSDiff++ which improves upon FBSDiff mainly in three aspects: (1) accelerate inference speed by a large margin (8.9$\times$ speedup in inference) with refined model architecture; (2) improve the Frequency Band Substitution module to allow for input source images of arbitrary resolution and aspect ratio; (3) extend model functionality to enable localized image manipulation and style-specific content creation with only subtle adjustments to the core method. Extensive qualitative and quantitative experiments verify superiority of FBSDiff++ in I2I translation visual quality, efficiency, versatility, and controllability compared to related advanced approaches.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.19115" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.19115.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.18585</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.18585" target="_blank">GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization</a>
      </h3>
      <p class="paper-title-zh">GimmBOï¼šåŸºäºè´å¶æ–¯ä¼˜åŒ–çš„äº¤äº’å¼ç”Ÿæˆå›¾åƒæ¨¡å‹èåˆ</p>
      <p class="paper-authors">Chenxi Liu, Selena Ling, Alec Jacobson</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåä¸ºGimmBOçš„äº¤äº’å¼ç³»ç»Ÿï¼Œé€šè¿‡åå¥½è´å¶æ–¯ä¼˜åŒ–ï¼ˆPBOï¼‰å¸®åŠ©ç”¨æˆ·é«˜æ•ˆæ¢ç´¢æ‰©æ•£æ¨¡å‹é€‚é…å™¨çš„æƒé‡èåˆç©ºé—´ï¼Œè§£å†³äº†æ‰‹åŠ¨è°ƒæ•´æƒé‡æ—¶ç»´åº¦ç¾éš¾å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åŸºäºçœŸå®ä½¿ç”¨åœºæ™¯ä¸­è§‚å¯Ÿåˆ°çš„ç¨€ç–æ€§å’Œæƒé‡èŒƒå›´å—é™çš„ç‰¹ç‚¹ï¼Œè®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è´å¶æ–¯ä¼˜åŒ–åç«¯ã€‚é¦–å…ˆé€šè¿‡å…¨å±€æ¢ç´¢ç¡®å®šæœ‰å¸Œæœ›çš„æƒé‡åŒºåŸŸï¼Œç„¶ååœ¨è¯¥åŒºåŸŸå†…è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»è€Œåœ¨é«˜ç»´ç©ºé—´ä¸­æé«˜é‡‡æ ·æ•ˆç‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚ç³»ç»Ÿæ”¯æŒç”¨æˆ·é€šè¿‡åå¥½åé¦ˆï¼ˆå¦‚é€‰æ‹©æ›´å–œæ¬¢çš„ç”Ÿæˆå›¾åƒï¼‰æ¥å¼•å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> é€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·å’ŒçœŸå®ç”¨æˆ·ç ”ç©¶è¯„ä¼°è¡¨æ˜ï¼ŒGimmBOç›¸æ¯”åŸºç¡€çš„è´å¶æ–¯ä¼˜åŒ–å’Œçº¿æ€§æœç´¢åŸºçº¿ï¼Œåœ¨æ”¶æ•›é€Ÿåº¦ã€æˆåŠŸç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶èƒ½ç¨³å®šè·å¾—æ›´ä¼˜çš„èåˆç»“æœã€‚æ¡†æ¶è¿˜å±•ç¤ºäº†è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œå¯æ”¯æŒå¤šç§æ‰©å±•åº”ç”¨ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.18585" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.18585.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.18543</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.18543" target="_blank">GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning</a>
      </h3>
      <p class="paper-title-zh">GenAgentï¼šé€šè¿‡æ™ºèƒ½ä½“å¤šæ¨¡æ€æ¨ç†æ‰©å±•æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†GenAgentï¼Œä¸€ä¸ªé€šè¿‡æ™ºèƒ½ä½“æ¡†æ¶å°†è§†è§‰ç†è§£ä¸ç”Ÿæˆè§£è€¦çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨å¤šè½®è‡ªä¸»äº¤äº’å’Œé“¾å¼æ€ç»´æ¥è¿­ä»£ä¼˜åŒ–å›¾åƒç”Ÿæˆï¼Œé¿å…äº†ä¼ ç»Ÿç»Ÿä¸€æ¨¡å‹çš„é«˜è®­ç»ƒæˆæœ¬å’Œèƒ½åŠ›æƒè¡¡é—®é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é‡‡ç”¨æ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”±å¤šæ¨¡æ€æ¨¡å‹è´Ÿè´£è§†è§‰ç†è§£ï¼Œå°†å›¾åƒç”Ÿæˆæ¨¡å‹ä½œä¸ºå¯è°ƒç”¨å·¥å…·ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆä½¿ç”¨é«˜è´¨é‡å·¥å…·è°ƒç”¨å’Œåæ€æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒä»¥å¯åŠ¨æ™ºèƒ½ä½“è¡Œä¸ºï¼›ç„¶åè¿›è¡Œç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆé’ˆå¯¹æœ€ç»ˆå›¾åƒè´¨é‡çš„ç‚¹å¥–åŠ±å’Œé’ˆå¯¹åæ€å‡†ç¡®æ€§çš„å¯¹å¥–åŠ±ï¼Œå¹¶åˆ©ç”¨è½¨è¿¹é‡é‡‡æ ·å¢å¼ºå¤šè½®æ¢ç´¢ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> GenAgentæ˜¾è‘—æå‡äº†åŸºç¡€ç”Ÿæˆå™¨ï¼ˆFLUX.1-devï¼‰åœ¨GenEval++ï¼ˆ+23.6%ï¼‰å’ŒWISEï¼ˆ+14%ï¼‰åŸºå‡†ä¸Šçš„æ€§èƒ½ã€‚æ¡†æ¶å±•ç°å‡ºä¸‰ä¸ªå…³é”®ç‰¹æ€§ï¼š1ï¼‰èƒ½æ³›åŒ–åˆ°ä¸åŒèƒ½åŠ›çš„ç”Ÿæˆå™¨ï¼›2ï¼‰æµ‹è¯•æ—¶æ€§èƒ½éšäº¤äº’è½®æ¬¡å¢åŠ è€ŒæŒç»­æå‡ï¼›3ï¼‰èƒ½è‡ªåŠ¨é€‚åº”ä¸åŒä»»åŠ¡çš„è‡ªé€‚åº”æ¨ç†èƒ½åŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.18543" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.18543.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17927</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17927" target="_blank">RemEdit: Efficient Diffusion Editing with Riemannian Geometry</a>
      </h3>
      <p class="paper-title-zh">RemEditï¼šåŸºäºé»æ›¼å‡ ä½•çš„é«˜æ•ˆæ‰©æ•£ç¼–è¾‘</p>
      <p class="paper-authors">Eashan Adhikarla, Brian D. Davison</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> RemEditæ¡†æ¶é€šè¿‡å°†æ½œåœ¨ç©ºé—´å»ºæ¨¡ä¸ºé»æ›¼æµå½¢å¹¶å¼•å…¥ä»»åŠ¡ç‰¹å®šçš„æ³¨æ„åŠ›å‰ªææœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜è¯­ä¹‰ä¿çœŸåº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘çš„æ¨ç†é€Ÿåº¦ï¼Œè§£å†³äº†è¯­ä¹‰ä¿çœŸåº¦ä¸æ¨ç†é€Ÿåº¦ä¹‹é—´çš„å…³é”®æƒè¡¡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆå°†æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´è§†ä¸ºé»æ›¼æµå½¢ï¼Œåˆ©ç”¨ä¸€ä¸ªåŸºäºMambaçš„æ¨¡å—é«˜æ•ˆå­¦ä¹ æµå½¢ç»“æ„ï¼Œä»è€Œèƒ½å¤Ÿç›´æ¥è®¡ç®—ç²¾ç¡®çš„æµ‹åœ°çº¿è·¯å¾„ä»¥å®ç°å¹³æ»‘çš„è¯­ä¹‰ç¼–è¾‘ã€‚å…¶æ¬¡ï¼Œé€šè¿‡åŒSLERPæ··åˆæŠ€æœ¯å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ„ŸçŸ¥æç¤ºå¢å¼ºæ¥è¿›ä¸€æ­¥ä¼˜åŒ–ç¼–è¾‘æ§åˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„ä»»åŠ¡ç‰¹å®šæ³¨æ„åŠ›å‰ªæå¤´ï¼Œå­¦ä¹ ä¿ç•™å¯¹ç¼–è¾‘è‡³å…³é‡è¦çš„æ ‡è®°ï¼Œå®ç°æœ‰æ•ˆåŠ é€Ÿè€Œä¸æŸå¤±è¯­ä¹‰è´¨é‡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒRemEditåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…ˆå‰çš„å…ˆè¿›ç¼–è¾‘æ¡†æ¶ï¼Œå¹¶åœ¨é«˜è¾¾50%çš„å‰ªæç‡ä¸‹ä»èƒ½ä¿æŒå®æ—¶æ€§èƒ½ï¼Œä¸ºå®ç”¨ä¸”å¼ºå¤§çš„å›¾åƒç¼–è¾‘è®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17927" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17927.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>