<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-01-28</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 3 | æœºå™¨å­¦ä¹ : 2</p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17673</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17673" target="_blank">Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing</a>
      </h3>
      <p class="paper-title-zh">Uni-RSï¼šä¸€ç§é¢å‘é¥æ„Ÿã€ç©ºé—´ä¿çœŸçš„ç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆæ¨¡å‹</p>
      <p class="paper-authors">Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†é¦–ä¸ªé¢å‘é¥æ„Ÿé¢†åŸŸçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹Uni-RSï¼Œæ—¨åœ¨æ˜¾å¼è§£å†³é¥æ„Ÿå›¾åƒç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„ç©ºé—´ä¸å¯¹ç§°æ€§é—®é¢˜ï¼Œå³ç¼“è§£ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒæ—¶éš¾ä»¥å¿ å®æ‰§è¡Œç©ºé—´å…³ç³»çš„â€œç©ºé—´åè½¬è¯…å’’â€ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œå¼•å…¥æ˜¾å¼çš„ç©ºé—´å¸ƒå±€è§„åˆ’æ¨¡å—ï¼Œå°†æ–‡æœ¬æŒ‡ä»¤è½¬åŒ–ä¸ºç©ºé—´å¸ƒå±€è§„åˆ’ï¼Œå®ç°å‡ ä½•è§„åˆ’ä¸è§†è§‰åˆæˆçš„è§£è€¦ã€‚å…¶æ¬¡ï¼Œé€šè¿‡ç©ºé—´æ„ŸçŸ¥æŸ¥è¯¢ç›‘ç£æœºåˆ¶ï¼Œä½¿å¯å­¦ä¹ æŸ¥è¯¢åå‘äºæŒ‡ä»¤ä¸­æ˜ç¡®æŒ‡å®šçš„ç©ºé—´å…³ç³»ã€‚æœ€åï¼Œå¼€å‘å›¾åƒ-æè¿°ç©ºé—´å¸ƒå±€å˜æ¢æ–¹æ³•ï¼Œè®©æ¨¡å‹ç³»ç»Ÿæ€§åœ°æ¥è§¦å‡ ä½•ä¸€è‡´çš„ç©ºé—´å˜æ¢æ•°æ®ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ç©ºé—´å¿ å®åº¦ï¼ŒåŒæ—¶åœ¨å›¾åƒæè¿°ã€è§†è§‰å®šä½ã€è§†è§‰é—®ç­”ç­‰å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17673" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17673.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.19606</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.19606" target="_blank">GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining</a>
      </h3>
      <p class="paper-title-zh">GMS-CAVPï¼šé€šè¿‡å¤šå°ºåº¦å¯¹æ¯”ä¸ç”Ÿæˆå¼é¢„è®­ç»ƒæ”¹è¿›éŸ³è§†é¢‘å¯¹åº”å…³ç³»</p>
      <p class="paper-authors">Shentong Mo, Zehua Chen, Jun Zhu</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§ç»“åˆå¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£ç”Ÿæˆç›®æ ‡çš„æ–°å‹éŸ³è§†é¢‘å¯¹åº”å»ºæ¨¡æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†è·¨æ¨¡æ€æ£€ç´¢ä¸ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆï¼Œè®¾è®¡å¤šå°ºåº¦å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»ç»†ç²’åº¦åˆ°ç²—ç²’åº¦æ•æ‰éŸ³è§†é¢‘çš„è¯­ä¹‰ä¸æ—¶åºå¯¹åº”å…³ç³»ï¼›å…¶æ¬¡ï¼Œå¼•å…¥åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç›®æ ‡ï¼Œå®ç°éŸ³è§†é¢‘é—´çš„è·¨æ¨¡æ€è½¬æ¢ä¸åˆæˆï¼›æœ€ç»ˆé€šè¿‡åˆ¤åˆ«ä¸ç”Ÿæˆç›®æ ‡çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä¿ƒè¿›æ›´æ·±å±‚æ¬¡çš„è·¨æ¨¡æ€ç†è§£ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨VGGSoundã€AudioSetå’ŒPanda70Mæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGMS-CAVPåœ¨ç”Ÿæˆä¸æ£€ç´¢ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å¤šå°ºåº¦å»ºæ¨¡ä¸ç”Ÿæˆå¼ç›®æ ‡å¯¹æå‡éŸ³è§†é¢‘å¯¹åº”å…³ç³»çš„æœ‰æ•ˆæ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.19606" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.19606.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">æœºå™¨å­¦ä¹ </span>
        <span class="paper-id">2601.17917</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17917" target="_blank">Streaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding</a>
      </h3>
      <p class="paper-title-zh">Streaming-dLLMï¼šé€šè¿‡åç¼€å‰ªæä¸åŠ¨æ€è§£ç åŠ é€Ÿæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹</p>
      <p class="paper-authors">Zhongyu Xiao, Zhiwei Hao, Jianyuan Guo, Yong Luo, Jia Liu ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†Streaming-dLLMï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒå³å¯åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸ŠåŒæ—¶ä¼˜åŒ–æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ¡†æ¶ï¼Œé€šè¿‡å‰ªæå†—ä½™åç¼€æ ‡è®°å’ŒåŠ¨æ€è°ƒæ•´å»å™ªè¿‡ç¨‹æ¥æ˜¾è‘—åŠ é€Ÿç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> åœ¨ç©ºé—´ç»´åº¦ä¸Šï¼Œå¼•å…¥è¡°å‡å¼•å¯¼çš„åç¼€å»ºæ¨¡ï¼Œé€šè¿‡å‰ªæä¿¡æ¯ç¨€ç–çš„åç¼€åŒºåŸŸä¸­çš„å†—ä½™æ©ç æ ‡è®°æ¥è¿‘ä¼¼å®Œæ•´ä¸Šä¸‹æ–‡ï¼›åœ¨æ—¶é—´ç»´åº¦ä¸Šï¼Œé‡‡ç”¨åŠ¨æ€ç½®ä¿¡åº¦æ„ŸçŸ¥ç­–ç•¥ä¸æå‰é€€å‡ºæœºåˆ¶ï¼Œå…è®¸æ¨¡å‹å¯¹å·²æ”¶æ•›çš„æ ‡è®°è·³è¿‡ä¸å¿…è¦çš„å»å™ªè¿­ä»£ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒStreaming-dLLMåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œæœ€é«˜å¯å®ç°68.2å€çš„æ¨ç†åŠ é€Ÿï¼Œæœ‰æ•ˆè§£å†³äº†æ‰©æ•£è§£ç è¿‡ç¨‹ä¸­çš„ç©ºé—´å†—ä½™ä¸æ—¶é—´æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17917" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17917.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
        <span class="paper-id">2601.17857</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17857" target="_blank">SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction</a>
      </h3>
      <p class="paper-title-zh">SynMindï¼šå‡å°‘åŸºäºfMRIçš„å›¾åƒé‡å»ºä¸­çš„è¯­ä¹‰å¹»è§‰</p>
      <p class="paper-authors">Lan Yang, Minghan Yang, Ke Li, Honggang Zhang, Kaiyue Pang ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºSynMindæ¡†æ¶ï¼Œé€šè¿‡å°†fMRIä¿¡å·è§£æä¸ºæ˜ç¡®çš„ã€å¥å­çº§åˆ«çš„è¯­ä¹‰æè¿°ï¼Œæ˜¾è‘—å‡å°‘äº†å›¾åƒé‡å»ºä¸­çš„è¯­ä¹‰é”™ä½é—®é¢˜ï¼Œå®ç°äº†æ›´ç¬¦åˆäººç±»è§†è§‰æ„ŸçŸ¥çš„é‡å»ºç»“æœã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆã€ç±»äººçš„å¤šç²’åº¦æ–‡æœ¬è¡¨ç¤ºï¼Œä»¥æ•è·ç›®æ ‡å›¾åƒçš„ç‰©ä½“èº«ä»½å’Œç©ºé—´ç»“æ„ï¼›ç„¶åå°†è¿™äº›æ˜ç¡®çš„è¯­ä¹‰ç¼–ç ä¸è§†è§‰å…ˆéªŒç»“åˆï¼Œå…±åŒä½œä¸ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶è¾“å…¥ï¼Œä»è€Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­åˆ†ç¦»è¯­ä¹‰æ¨ç†ä¸å¤–è§‚ç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒSynMindåœ¨å¤šæ•°å®šé‡æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”ä»…ä½¿ç”¨è¾ƒå°çš„Stable Diffusion 1.4æ¨¡å‹å’Œå•å¼ æ¶ˆè´¹çº§GPUå³å¯è¶…è¶ŠåŸºäºSDXLçš„å¯¹æ¯”æ–¹æ³•ï¼›å¤§è§„æ¨¡äººå·¥è¯„ä¼°è¯å®å…¶é‡å»ºç»“æœä¸äººç±»è§†è§‰æ„ŸçŸ¥æ›´ä¸€è‡´ï¼›ç¥ç»å¯è§†åŒ–åˆ†ææ˜¾ç¤ºï¼ŒSynMindæ¿€æ´»äº†æ›´å¹¿æ³›ä¸”è¯­ä¹‰ç›¸å…³çš„å¤§è„‘åŒºåŸŸï¼Œå‡å°‘äº†å¯¹é«˜çº§è§†è§‰åŒºåŸŸçš„è¿‡åº¦ä¾èµ–ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17857" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17857.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <span class="category-badge">æœºå™¨å­¦ä¹ </span>
        <span class="paper-id">2601.17883</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2601.17883" target="_blank">EEG Foundation Models: Progresses, Benchmarking, and Open Problems</a>
      </h3>
      <p class="paper-title-zh">è„‘ç”µå›¾åŸºç¡€æ¨¡å‹ï¼šè¿›å±•ã€åŸºå‡†æµ‹è¯•ä¸å¼€æ”¾æ€§é—®é¢˜</p>
      <p class="paper-authors">Dingkun Liu, Yuheng Chen, Zhu Chen, Zhenyao Cui, Yaozhi Wen ç­‰ (8 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡é¦–æ¬¡å¯¹ç°æœ‰EEGåŸºç¡€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„å›é¡¾ä¸å…¬å¹³å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è·¨è¢«è¯•æ³›åŒ–ä¸å°‘æ ·æœ¬æ ¡å‡†ç­‰å®é™…åœºæ™¯ä¸­çš„è¡¨ç°ä¸å±€é™ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆå›é¡¾äº†50ä¸ªä»£è¡¨æ€§æ¨¡å‹ï¼Œå°†å…¶è®¾è®¡é€‰æ‹©ç»Ÿä¸€å½’çº³ä¸ºæ•°æ®æ ‡å‡†åŒ–ã€æ¨¡å‹æ¶æ„å’Œè‡ªç›‘ç£é¢„è®­ç»ƒç­–ç•¥ä¸‰ä¸ªç»´åº¦ï¼›éšååœ¨æ¶µç›–9ç§BCIèŒƒå¼çš„13ä¸ªEEGæ•°æ®é›†ä¸Šï¼Œå¯¹12ä¸ªå¼€æºåŸºç¡€æ¨¡å‹åŠä¸“ä¸šåŸºçº¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†ç•™ä¸€è¢«è¯•è·¨è¢«è¯•æ³›åŒ–ä¸å°‘æ ·æœ¬è¢«è¯•å†…æ ¡å‡†ä¸¤ç§å®é™…éƒ¨ç½²åœºæ™¯ï¼Œå¹¶å¯¹æ¯”äº†å…¨å‚æ•°å¾®è°ƒä¸çº¿æ€§æ¢æµ‹ä¸¤ç§è¿ç§»ç­–ç•¥ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒç»“æœè¡¨æ˜ï¼š1ï¼‰çº¿æ€§æ¢æµ‹é€šå¸¸ä¸è¶³ä»¥å……åˆ†è¿ç§»é¢„è®­ç»ƒè¡¨å¾ï¼›2ï¼‰ä»å¤´è®­ç»ƒçš„ä¸“ä¸šæ¨¡å‹åœ¨è®¸å¤šä»»åŠ¡ä¸Šä»å…·æœ‰ç«äº‰åŠ›ï¼›3ï¼‰åœ¨å½“å‰æ•°æ®è§„æ¨¡å’Œè®­ç»ƒæ–¹å¼ä¸‹ï¼Œæ›´å¤§çš„åŸºç¡€æ¨¡å‹æœªå¿…å¸¦æ¥æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2601.17883" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2601.17883.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>