<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-22</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 4 | æœºå™¨å­¦ä¹ : 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.17558</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.17558" target="_blank">RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward</a>
      </h3>
      <p class="paper-title-zh">RetouchIQï¼šåŸºäºé€šç”¨å¥–åŠ±æ¨¡å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»£ç†ç”¨äºæŒ‡ä»¤é©±åŠ¨çš„å›¾åƒæ¶¦é¥°</p>
      <p class="paper-authors">Qiucheng Wu, Jing Shi, Simon Jenni, Kushal Kafle, Tianyu Wang ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†RetouchIQæ¡†æ¶ï¼Œé€šè¿‡ä¸€ä¸ªé€šç”¨å¥–åŠ±æ¨¡å‹é©±åŠ¨çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»£ç†ï¼Œå°†ç”¨æˆ·çš„é«˜å±‚æ¬¡å®¡ç¾æŒ‡ä»¤è½¬åŒ–ä¸ºä¸“ä¸šå›¾åƒç¼–è¾‘è½¯ä»¶ä¸­çš„å¯æ‰§è¡Œæ“ä½œï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«19ä¸‡æ¡æŒ‡ä»¤-æ¨ç†å¯¹çš„æ•°æ®é›†åŠæ–°åŸºå‡†ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ¡†æ¶é¦–å…ˆåˆ©ç”¨MLLMä»£ç†è§£æç”¨æˆ·ç¼–è¾‘æ„å›¾å¹¶ç”Ÿæˆå¯æ‰§è¡Œçš„å›¾åƒè°ƒæ•´å‚æ•°ã€‚ä¸ºè§£å†³åˆ›æ„ç¼–è¾‘ä¸­ä¸»è§‚æ€§å¯¼è‡´çš„å¥–åŠ±ä¿¡å·éš¾ä»¥å®šä¹‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªé€šç”¨å¥–åŠ±æ¨¡å‹â€”â€”è¯¥æ¨¡å‹æ˜¯ä¸€ä¸ªç»è¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„MLLMï¼Œèƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªç¼–è¾‘æ¡ˆä¾‹ç”Ÿæˆä¸€ç»„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€æ¨ç†æä¾›æ ‡é‡åé¦ˆï¼Œä»è€Œä¸ºå¼ºåŒ–å­¦ä¹ æä¾›é«˜è´¨é‡ã€ä¸æŒ‡ä»¤ä¸€è‡´çš„æ¢¯åº¦ä¿¡å·ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒRetouchIQåœ¨è¯­ä¹‰ä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä»¥å¾€åŸºäºMLLMå’Œæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘ç³»ç»Ÿã€‚ç ”ç©¶è¯æ˜äº†é€šç”¨å¥–åŠ±é©±åŠ¨çš„MLLMä»£ç†å¯ä½œä¸ºçµæ´»ã€å¯è§£é‡Šä¸”å¯æ‰§è¡Œçš„åŠ©æ‰‹ï¼Œåº”ç”¨äºä¸“ä¸šå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.17558" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.17558.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">æœºå™¨å­¦ä¹ </span>
          <span class="paper-id">2602.17270</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.17270" target="_blank">Unified Latents (UL): How to train your latents</a>
      </h3>
      <p class="paper-title-zh">ç»Ÿä¸€éšå˜é‡ï¼ˆULï¼‰ï¼šå¦‚ä½•è®­ç»ƒä½ çš„éšå˜é‡</p>
      <p class="paper-authors">Jonathan Heek, Emiel Hoogeboom, Thomas Mensink, Tim Salimans</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ç»Ÿä¸€éšå˜é‡ï¼ˆULï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†ç¼–ç å™¨è¾“å‡ºå™ªå£°ä¸å…ˆéªŒæ¨¡å‹çš„æœ€å°å™ªå£°æ°´å¹³å…³è”ï¼Œå®ç°äº†åœ¨æ‰©æ•£å…ˆéªŒçº¦æŸä¸‹å­¦ä¹ éšå˜é‡è¡¨ç¤ºï¼Œå¹¶ä»¥ç®€å•çš„è®­ç»ƒç›®æ ‡è·å¾—äº†éšå˜é‡æ¯”ç‰¹ç‡çš„ç´§è‡´ä¸Šç•Œã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªè”åˆä¼˜åŒ–æ¡†æ¶ï¼šç¼–ç å™¨å­¦ä¹ çš„éšå˜é‡è¡¨ç¤ºåŒæ—¶å—åˆ°æ‰©æ•£å…ˆéªŒçš„çº¦æŸï¼Œå¹¶ç”±æ‰©æ•£æ¨¡å‹è¿›è¡Œè§£ç ã€‚é€šè¿‡å°†ç¼–ç å™¨è¾“å‡ºçš„å™ªå£°ä¸å…ˆéªŒæ¨¡å‹ä¸­çš„æœ€å°å™ªå£°æ°´å¹³ç›´æ¥å…³è”ï¼Œæ¨å¯¼å‡ºä¸€ä¸ªç®€åŒ–çš„è®­ç»ƒç›®æ ‡ã€‚è¯¥æ¡†æ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶ä¼˜åŒ–éšå˜é‡çš„æ­£åˆ™åŒ–ä¸é‡å»ºè´¨é‡ï¼Œä¸”è®¡ç®—æ•ˆç‡è¾ƒé«˜ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ImageNet-512æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†1.4çš„ç«äº‰æ€§FIDåˆ†æ•°ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„é‡å»ºè´¨é‡ï¼ˆPSNRï¼‰ï¼Œä¸”è®­ç»ƒæ‰€éœ€çš„FLOPsä½äºåŸºäºStable Diffusionéšå˜é‡è®­ç»ƒçš„æ¨¡å‹ã€‚åœ¨Kinetics-600è§†é¢‘æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä»¥1.3çš„FVDåˆ†æ•°åˆ›é€ äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.17270" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.17270.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.17200</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.17200" target="_blank">GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation</a>
      </h3>
      <p class="paper-title-zh">GASSï¼šé¢å‘æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è§£è€¦å¤šæ ·æ€§å¢å¼ºçš„å‡ ä½•æ„ŸçŸ¥çƒé¢é‡‡æ ·</p>
      <p class="paper-authors">Ye Zhu, Kaleb S. Newman, Johannes F. Lutzeyer, Adriana Romero-Soriano, Michal Drozdzal ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥çƒé¢é‡‡æ ·æ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼æ§åˆ¶ä¸æç¤ºç›¸å…³å’Œæ— å…³çš„ä¸¤ç§å˜åŒ–æ¥æºï¼Œåœ¨ä¿æŒå›¾åƒè´¨é‡å’Œè¯­ä¹‰å¯¹é½çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¤šæ ·æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆåœ¨CLIPåµŒå…¥ç©ºé—´ä¸­ï¼Œå°†å¤šæ ·æ€§åº¦é‡åˆ†è§£ä¸ºä¸¤ä¸ªæ­£äº¤æ–¹å‘ï¼šæ–‡æœ¬åµŒå…¥æ–¹å‘ï¼ˆæ•æ‰ä¸æç¤ºç›¸å…³çš„è¯­ä¹‰å˜åŒ–ï¼‰å’Œä¸€ä¸ªä¸ä¹‹æ­£äº¤çš„æ–¹å‘ï¼ˆæ•æ‰ä¸æç¤ºæ— å…³çš„å˜åŒ–ï¼Œå¦‚èƒŒæ™¯ï¼‰ã€‚åŸºäºæ­¤åˆ†è§£ï¼ŒGASSé€šè¿‡æ‰©å¤§ç”Ÿæˆå›¾åƒåµŒå…¥åœ¨è¿™ä¸¤ä¸ªè½´ä¸Šçš„å‡ ä½•æŠ•å½±åˆ†å¸ƒï¼Œå¹¶æ²¿ç”Ÿæˆè½¨è¿¹å¼•å¯¼æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œå®ç°è§£è€¦çš„å¤šæ ·æ€§å¢å¼ºã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒGASSåœ¨ä¸åŒå†»ç»“çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆéª¨å¹²æ¨¡å‹ï¼ˆåŒ…æ‹¬U-Netå’ŒDiTæ¶æ„ï¼Œæ‰©æ•£ä¸æµæ¨¡å‹ï¼‰å’Œå¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡èƒ½æœ‰æ•ˆæå‡ç”Ÿæˆå¤šæ ·æ€§ï¼Œä¸”å¯¹å›¾åƒä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½çš„å½±å“æå°ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.17200" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.17200.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.17047</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.17047" target="_blank">Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers</a>
      </h3>
      <p class="paper-title-zh">Amber-Imageï¼šå¤§è§„æ¨¡æ‰©æ•£Transformerçš„é«˜æ•ˆå‹ç¼©</p>
      <p class="paper-authors">Chaojie Yang, Tian Li, Yue Zhang, Jun Gao</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªæ— éœ€ä»å¤´è®­ç»ƒçš„é«˜æ•ˆå‹ç¼©æ¡†æ¶ï¼Œå°†å¤§è§„æ¨¡åŒæµMMDiTæ¨¡å‹å‹ç¼©ä¸ºè½»é‡åŒ–æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ä¸éƒ¨ç½²é—¨æ§›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆé‡‡ç”¨æ—¶é—´æ­¥æ•æ„Ÿçš„æ·±åº¦å‰ªæç­–ç•¥ï¼Œä¿ç•™å…³é”®å±‚å¹¶é€šè¿‡å±€éƒ¨æƒé‡å¹³å‡é‡æ–°åˆå§‹åŒ–ï¼Œå†ç»“åˆåˆ†å±‚è’¸é¦ä¸å…¨å‚æ•°å¾®è°ƒè¿›è¡Œä¼˜åŒ–ï¼›è¿›ä¸€æ­¥å¼•å…¥æ··åˆæµæ¶æ„ï¼Œå°†æ·±å±‚åŒæµè½¬æ¢ä¸ºå•æµï¼ˆä»¥å›¾åƒåˆ†æ”¯åˆå§‹åŒ–ï¼‰ï¼Œå¹¶é€šè¿‡æ¸è¿›è’¸é¦ä¸è½»é‡å¾®è°ƒè¿›è¡Œç²¾ç‚¼ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å‹ç¼©åçš„æ¨¡å‹å‚æ•°é‡å‡å°‘70%ï¼Œä¸”æ•´ä¸ªä»10Båˆ°6Bçš„å‹ç¼©è®­ç»ƒæµç¨‹ä»…éœ€ä¸åˆ°2000 GPUå°æ—¶ï¼Œåœ¨DPG-Benchå’ŒLongText-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜ä¿çœŸåˆæˆä¸ä¼˜è¶Šçš„æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›ï¼Œæ€§èƒ½åª²ç¾æ›´å¤§è§„æ¨¡çš„åŸå§‹æ¨¡å‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.17047" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.17047.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.16968</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.16968" target="_blank">DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers</a>
      </h3>
      <p class="paper-title-zh">DDiTï¼šé¢å‘é«˜æ•ˆæ‰©æ•£å˜æ¢å™¨çš„åŠ¨æ€è¡¥ä¸è°ƒåº¦</p>
      <p class="paper-authors">Dahye Kim, Deepti Ghadiyaram, Raghudeep Gadde</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§åŠ¨æ€ä»¤ç‰ŒåŒ–ç­–ç•¥ï¼Œé€šè¿‡åœ¨å»å™ªè¿‡ç¨‹ä¸­æ ¹æ®å†…å®¹å¤æ‚åº¦å’Œæ—¶é—´æ­¥åŠ¨æ€è°ƒæ•´è¡¥ä¸å¤§å°ï¼Œæ˜¾è‘—é™ä½äº†æ‰©æ•£å˜æ¢å™¨çš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨å»å™ªæ—©æœŸä½¿ç”¨è¾ƒå¤§çš„ç²—ç²’åº¦è¡¥ä¸æ•æ‰å…¨å±€ç»“æ„ï¼Œåœ¨åæœŸä½¿ç”¨è¾ƒå°çš„ç»†ç²’åº¦è¡¥ä¸å®Œå–„å±€éƒ¨ç»†èŠ‚ã€‚å…·ä½“å®ç°ä¸­ï¼Œåœ¨æ¨ç†é˜¶æ®µæ ¹æ®æ—¶é—´æ­¥å’Œå†…å®¹å¤æ‚åº¦åŠ¨æ€è°ƒåº¦ä¸åŒå¤§å°çš„è¡¥ä¸ï¼Œä»è€Œå‡å°‘æ€»ä½“è®¡ç®—é‡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨FLUX-1.Devå’ŒWan 2.1æ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº†æœ€é«˜3.52å€å’Œ3.2å€çš„åŠ é€Ÿï¼Œä¸”æœªæŸå®³ç”Ÿæˆå›¾åƒçš„æ„ŸçŸ¥è´¨é‡å’Œå¯¹æ–‡æœ¬æç¤ºçš„éµå¾ªç¨‹åº¦ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.16968" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.16968.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>