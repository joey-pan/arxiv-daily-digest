<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-05</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | äººå·¥æ™ºèƒ½: 1 | è®¡ç®—æœºè§†è§‰: 3 | äººæœºäº¤äº’: 1</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.04361</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.04361" target="_blank">SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration</a>
      </h3>
      <p class="paper-title-zh">SparVARï¼šæ¢ç´¢è§†è§‰è‡ªå›å½’å»ºæ¨¡ä¸­çš„ç¨€ç–æ€§ä»¥å®ç°å…è®­ç»ƒåŠ é€Ÿ</p>
      <p class="paper-authors">Zekun Li, Ning Wang, Tongxin Bai, Changwang Mei, Peisong Wang ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†SparVARï¼Œä¸€ä¸ªå…è®­ç»ƒçš„åŠ é€Ÿæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨VARæ³¨æ„åŠ›ä¸­çš„å¼ºæ³¨æ„åŠ›æ±‡èšç‚¹ã€è·¨å°ºåº¦æ¿€æ´»ç›¸ä¼¼æ€§å’Œæ˜¾è‘—å±€éƒ¨æ€§ä¸‰ä¸ªç‰¹æ€§ï¼Œåœ¨ä¸è·³è¿‡é«˜åˆ†è¾¨ç‡å°ºåº¦çš„å‰æä¸‹æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆä»ç¨€ç–å†³ç­–å°ºåº¦åŠ¨æ€é¢„æµ‹åç»­é«˜åˆ†è¾¨ç‡å°ºåº¦çš„ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼›å…¶æ¬¡ï¼Œé€šè¿‡é«˜æ•ˆçš„ç´¢å¼•æ˜ å°„æœºåˆ¶æ„å»ºå°ºåº¦è‡ªç›¸ä¼¼ç¨€ç–æ³¨æ„åŠ›ï¼Œå®ç°å¤§è§„æ¨¡ä¸‹çš„é«˜æ•ˆç¨€ç–æ³¨æ„åŠ›è®¡ç®—ï¼›æœ€åï¼Œæå‡ºè·¨å°ºåº¦å±€éƒ¨ç¨€ç–æ³¨æ„åŠ›ï¼Œå¹¶å®ç°é«˜æ•ˆçš„å—çŠ¶ç¨€ç–å†…æ ¸ï¼Œå…¶å‰å‘é€Ÿåº¦æ¯”FlashAttentionå¿«5å€ä»¥ä¸Šã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒSparVARèƒ½å°†ä¸€ä¸ª80äº¿å‚æ•°æ¨¡å‹ç”Ÿæˆ1024Ã—1024é«˜åˆ†è¾¨ç‡å›¾åƒçš„æ—¶é—´ç¼©çŸ­è‡³1ç§’ï¼Œä¸”ä¸è·³è¿‡æœ€åçš„é«˜åˆ†è¾¨ç‡å°ºåº¦ã€‚ä¸ä½¿ç”¨FlashAttentionåŠ é€Ÿçš„VARåŸºçº¿ç›¸æ¯”ï¼Œæœ¬æ–¹æ³•åœ¨å‡ ä¹ä¿ç•™æ‰€æœ‰é«˜é¢‘ç»†èŠ‚çš„åŒæ—¶å®ç°äº†1.57å€çš„åŠ é€Ÿï¼›è‹¥ä¸ç°æœ‰çš„å°ºåº¦è·³è¿‡ç­–ç•¥ç»“åˆï¼ŒåŠ é€Ÿæ¯”å¯è¾¾2.28å€ï¼Œä¸”è§†è§‰ç”Ÿæˆè´¨é‡ä¿æŒç«äº‰åŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at https://github.com/CAS-CLab/SparVAR.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.04361" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.04361.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.04167</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.04167" target="_blank">Point2Insert: Video Object Insertion via Sparse Point Guidance</a>
      </h3>
      <p class="paper-title-zh">Point2Insertï¼šåŸºäºç¨€ç–ç‚¹å¼•å¯¼çš„è§†é¢‘å¯¹è±¡æ’å…¥</p>
      <p class="paper-authors">Yu Zhou, Xiaoyan Yang, Bojia Zi, Lihan Zhang, Ruijie Sun ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªä»…éœ€å°‘é‡ç¨€ç–ç‚¹ï¼ˆè€Œéå¯†é›†æ©ç ï¼‰å³å¯å®ç°è§†é¢‘ä¸­å¯¹è±¡ç²¾ç¡®ã€çµæ´»æ’å…¥çš„æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ ‡æ³¨æˆæœ¬ä¸å®šä½ç²¾åº¦ä¸Šçš„éš¾é¢˜ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼šç¬¬ä¸€é˜¶æ®µè®­ç»ƒä¸€ä¸ªåŸºäºç¨€ç–ç‚¹æˆ–äºŒå€¼æ©ç æç¤ºçš„æ’å…¥æ¨¡å‹ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¯¹è±¡ç§»é™¤æ¨¡å‹åˆæˆçš„é…å¯¹è§†é¢‘è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”è§†é¢‘æ’å…¥ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦å°†æ©ç å¼•å¯¼æ¨¡å‹çš„å¯é æ’å…¥è¡Œä¸ºè¿ç§»åˆ°ç‚¹å¼•å¯¼æ¨¡å‹ä¸­ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒPoint2Insertåœ¨è§†é¢‘å¯¹è±¡æ’å…¥ä»»åŠ¡ä¸Š consistently ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¿‡äº†å‚æ•°é‡å¤§10å€çš„æ¨¡å‹ï¼ŒéªŒè¯äº†ç¨€ç–ç‚¹å¼•å¯¼åœ¨ç²¾åº¦ä¸æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\times$10 more parameters.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.04167" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.04167.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">äººæœºäº¤äº’</span>
          <span class="paper-id">2602.03838</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.03838" target="_blank">PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization</a>
      </h3>
      <p class="paper-title-zh">PrevizWhizï¼šç»“åˆç²—ç•¥3Dåœºæ™¯ä¸2Dè§†é¢‘å¼•å¯¼ç”Ÿæˆå¼è§†é¢‘é¢„å¯è§†åŒ–</p>
      <p class="paper-authors">Erzhen Hu, Frederik Brudy, David Ledo, George Fitzmaurice, Fraser Anderson</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†PrevizWhizç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆç²—ç•¥3Dåœºæ™¯ä¸ç”Ÿæˆå¼å›¾åƒ/è§†é¢‘æ¨¡å‹ï¼Œä¸ºç”µå½±åˆ¶ä½œäººæä¾›äº†ä¸€ç§èƒ½å¿«é€Ÿåˆ›å»ºé£æ ¼åŒ–è§†é¢‘é¢„è§ˆçš„æ–°æ–¹æ³•ï¼Œæœ‰æ•ˆé™ä½äº†æŠ€æœ¯é—¨æ§›å¹¶åŠ é€Ÿäº†åˆ›æ„è¿­ä»£ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥ç³»ç»Ÿçš„å·¥ä½œæµç¨‹é¦–å…ˆå°†ç²—ç•¥3Dåœºæ™¯ä¸ç”Ÿæˆæ¨¡å‹ç»“åˆï¼Œè¿›è¡Œå¸§çº§å›¾åƒé£æ ¼åŒ–å¹¶å…è®¸è°ƒæ•´ä¸åŸç´ æçš„ç›¸ä¼¼åº¦ï¼›å…¶æ¬¡æ”¯æŒé€šè¿‡è¿åŠ¨è·¯å¾„æˆ–å¤–éƒ¨è§†é¢‘è¾“å…¥è¿›è¡ŒåŸºäºæ—¶é—´çš„ç¼–è¾‘ï¼›æœ€åå°†ç»“æœç»†åŒ–ä¸ºé«˜ä¿çœŸè§†é¢‘ç‰‡æ®µã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¯¹ç”µå½±åˆ¶ä½œäººçš„ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿé™ä½äº†æŠ€æœ¯éšœç¢ã€åŠ å¿«äº†åˆ›ä½œè¿­ä»£é€Ÿåº¦ï¼Œå¹¶æœ‰æ•ˆå¼¥åˆäº†æ²Ÿé€šå·®è·ï¼›åŒæ—¶ä¹Ÿæ­ç¤ºäº†AIè¾…åŠ©ç”µå½±åˆ¶ä½œä¸­è¿ç»­æ€§ã€ä½œè€…èº«ä»½å’Œä¼¦ç†è€ƒé‡ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews. The workflow integrates frame-level image restyling with adjustable resemblance, time-based editing through motion paths or external video inputs, and refinement into high-fidelity video clips. A study with filmmakers demonstrates that our system lowers technical barriers for film-makers, accelerates creative iteration, and effectively bridges the communication gap, while also surfacing challenges of continuity, authorship, and ethical consideration in AI-assisted filmmaking.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.03838" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.03838.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">äººå·¥æ™ºèƒ½</span>
          <span class="paper-id">2602.03828</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.03828" target="_blank">AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations</a>
      </h3>
      <p class="paper-title-zh">AutoFigureï¼šç”Ÿæˆä¸ç²¾ç‚¼å¯ç”¨äºå‘è¡¨çš„ç§‘å­¦æ’å›¾</p>
      <p class="paper-authors">Minjun Zhu, Zhen Lin, Yixuan Weng, Panzhong Lu, Qiujie Xie ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†é¦–ä¸ªç”¨äºä»é•¿ç¯‡ç§‘å­¦æ–‡æœ¬ç”Ÿæˆç§‘å­¦æ’å›¾çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†FigureBenchï¼Œå¹¶è®¾è®¡äº†é¦–ä¸ªèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ç§‘å­¦æ’å›¾çš„æ™ºèƒ½ä½“æ¡†æ¶AutoFigureã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•ä¸Šï¼Œä½œè€…é¦–å…ˆæ„å»ºäº†åŒ…å«3300ä¸ªé«˜è´¨é‡æ–‡æœ¬-æ’å›¾å¯¹çš„FigureBenchåŸºå‡†æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†AutoFigureæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ¸²æŸ“æœ€ç»ˆç»“æœå‰ï¼Œä¼šè¿›è¡Œæ·±å…¥çš„æ€è€ƒã€é‡ç»„å’ŒéªŒè¯ï¼Œä»¥ç”Ÿæˆç»“æ„åˆç†ä¸”ç¾è§‚çš„å¸ƒå±€ã€‚å…¶æ ¸å¿ƒæ˜¯é€šè¿‡ä¸€ä¸ªæ™ºèƒ½ä½“æµç¨‹æ¥ç¡®ä¿æ’å›¾çš„å®Œæ•´æ€§ä¸è§†è§‰å¸å¼•åŠ›ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å…³é”®å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoFrameåœ¨å¹¿æ³›çš„æµ‹è¯•ä¸­ consistently è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆè¾¾åˆ°å‘è¡¨è´¨é‡è¦æ±‚çš„ç§‘å­¦æ’å›¾ã€‚ç›¸å…³ä»£ç ã€æ•°æ®é›†å’Œæ¼”ç¤ºå¹³å°å‡å·²å¼€æºã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.03828" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.03828.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.03826</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.03826" target="_blank">Continuous Control of Editing Models via Adaptive-Origin Guidance</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡è‡ªé€‚åº”åŸç‚¹å¼•å¯¼å®ç°å¯¹ç¼–è¾‘æ¨¡å‹çš„è¿ç»­æ§åˆ¶</p>
      <p class="paper-authors">Alon Wolf, Chen Katzir, Kfir Aberman, Or Patashnik</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†è‡ªé€‚åº”åŸç‚¹å¼•å¯¼æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ‰©æ•£ç¼–è¾‘æ¨¡å‹æ— æ³•å¹³æ»‘æ§åˆ¶æ–‡æœ¬å¼•å¯¼ç¼–è¾‘å¼ºåº¦çš„é—®é¢˜ï¼Œå®ç°äº†ä»åŸå§‹è¾“å…¥åˆ°ç¼–è¾‘ç»“æœçš„è¿ç»­è¿‡æ¸¡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸èº«ä»½æ“ä½œå¯¹åº”çš„èº«ä»½æŒ‡ä»¤ï¼Œå°†æ ‡å‡†çš„æ— æ¡ä»¶é¢„æµ‹åŸç‚¹æ›¿æ¢ä¸ºèº«ä»½æ¡ä»¶åŒ–çš„è‡ªé€‚åº”åŸç‚¹ã€‚æ ¹æ®ç¼–è¾‘å¼ºåº¦ï¼Œåœ¨èº«ä»½é¢„æµ‹ä¸æ ‡å‡†æ— æ¡ä»¶é¢„æµ‹ä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œä»è€Œç¡®ä¿ç¼–è¾‘è¿‡ç¨‹çš„å¹³æ»‘æ€§ã€‚è¯¥æ–¹æ³•å¯é›†æˆåˆ°æ ‡å‡†è®­ç»ƒæ¡†æ¶ä¸­ï¼Œæ— éœ€é’ˆå¯¹æ¯æ¬¡ç¼–è¾‘è¿›è¡Œç‰¹æ®Šå¤„ç†æˆ–ä¾èµ–ä¸“ç”¨æ•°æ®é›†ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å›¾åƒå’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”ç°æœ‰çš„åŸºäºæ»‘å—çš„ç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½æä¾›æ›´å¹³æ»‘ã€æ›´ä¸€è‡´çš„æ§åˆ¶æ•ˆæœï¼Œå®ç°äº†å¯¹ç¼–è¾‘å¼ºåº¦çš„ç»†ç²’åº¦è°ƒæ§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.03826" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.03826.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>