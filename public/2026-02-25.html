<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-25</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.20672</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 95/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.20672" target="_blank">BBQ-to-Image: Numeric Bounding Box and Qolor Control in Large-Scale Text-to-Image Models</a>
      </h3>
      <p class="paper-title-zh">BBQ-to-Imageï¼šå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„æ•°å€¼è¾¹ç•Œæ¡†ä¸é¢œè‰²æ§åˆ¶</p>
      <p class="paper-authors">Eliran Kachlon, Alexander Visheratin, Nimrod Sarid, Tal Hacham, Eyal Gutflaish ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§èƒ½å¤Ÿç›´æ¥åŸºäºæ•°å€¼è¾¹ç•Œæ¡†å’ŒRGBä¸‰å…ƒç»„è¿›è¡Œå›¾åƒç”Ÿæˆçš„å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå®ç°äº†å¯¹ç‰©ä½“ä½ç½®ã€å°ºå¯¸å’Œé¢œè‰²çš„ç²¾ç¡®æ•°å€¼æ§åˆ¶ï¼Œå¼¥åˆäº†æè¿°æ€§è¯­è¨€ä¸ä¸“ä¸šå·¥ä½œæµç¨‹ä¹‹é—´çš„å‚æ•°é¸¿æ²Ÿã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åœ¨ç»Ÿä¸€çš„ç»“æ„åŒ–æ–‡æœ¬æ¡†æ¶ä¸‹ï¼Œé€šè¿‡ä½¿ç”¨åŒ…å«å‚æ•°åŒ–æ ‡æ³¨ï¼ˆå¦‚è¾¹ç•Œæ¡†åæ ‡å’ŒRGBå€¼ï¼‰çš„å¢å¼ºæè¿°è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥ç†è§£å¹¶å“åº”æ•°å€¼å‚æ•°ã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„æˆ–è¿›è¡Œæ¨ç†æ—¶ä¼˜åŒ–ï¼ŒåŒæ—¶æ”¯æŒé€šè¿‡ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ï¼ˆå¦‚å¯¹è±¡æ‹–æ‹½å’Œé¢œè‰²é€‰æ‹©å™¨ï¼‰è¿›è¡Œäº¤äº’ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨å…¨é¢è¯„ä¼°ä¸­ï¼ŒBBQæ¨¡å‹åœ¨è¾¹ç•Œæ¡†å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨RGBé¢œè‰²ä¿çœŸåº¦ä¸Šè¶…è¶Šäº†ç°æœ‰å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶ç»“æœæ”¯æŒäº†ä¸€ç§æ–°èŒƒå¼ï¼šå°†ç”¨æˆ·æ„å›¾è½¬åŒ–ä¸ºä¸­é—´ç»“æ„åŒ–è¯­è¨€ï¼Œå†ç”±ä¸€ä¸ªåŸºäºæµçš„Transformerä½œä¸ºæ¸²æŸ“å™¨è¿›è¡Œæ¶ˆè´¹ï¼Œä»è€Œè‡ªç„¶åœ°å®¹çº³æ•°å€¼å‚æ•°ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Text-to-image models have rapidly advanced in realism and controllability, with recent approaches leveraging long, detailed captions to support fine-grained generation. However, a fundamental parametric gap remains: existing models rely on descriptive language, whereas professional workflows require precise numeric control over object location, size, and color. In this work, we introduce BBQ, a large-scale text-to-image model that directly conditions on numeric bounding boxes and RGB triplets within a unified structured-text framework. We obtain precise spatial and chromatic control by training on captions enriched with parametric annotations, without architectural modifications or inference-time optimization. This also enables intuitive user interfaces such as object dragging and color pickers, replacing ambiguous iterative prompting with precise, familiar controls. Across comprehensive evaluations, BBQ achieves strong box alignment and improves RGB color fidelity over state-of-the-art baselines. More broadly, our results support a new paradigm in which user intent is translated into an intermediate structured language, consumed by a flow-based transformer acting as a renderer and naturally accommodating numeric parameters.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.20672" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.20672.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.20989</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.20989" target="_blank">Cycle-Consistent Tuning for Layered Image Decomposition</a>
      </h3>
      <p class="paper-title-zh">ç”¨äºåˆ†å±‚å›¾åƒåˆ†è§£çš„å¾ªç¯ä¸€è‡´æ€§è°ƒä¼˜</p>
      <p class="paper-authors">Zheng Gu, Min Lu, Zhida Sun, Dani Lischinski, Daniel Cohen-O ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡çš„åˆ†å±‚å›¾åƒåˆ†è§£æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹æ‰©æ•£åŸºç¡€æ¨¡å‹å®ç°è§†è§‰å±‚çš„åˆ†ç¦»ï¼Œå¹¶é€šè¿‡å¾ªç¯ä¸€è‡´æ€§è°ƒä¼˜ç­–ç•¥å’Œæ¸è¿›å¼è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚äº¤äº’åœºæ™¯ä¸‹çš„åˆ†è§£é²æ£’æ€§ä¸å‡†ç¡®æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è½»é‡çº§çš„LoRAé€‚é…è¿›è¡Œå¾®è°ƒã€‚æ ¸å¿ƒæ˜¯å¼•å…¥å¾ªç¯ä¸€è‡´æ€§è°ƒä¼˜ç­–ç•¥ï¼Œè”åˆè®­ç»ƒåˆ†è§£æ¨¡å‹å’Œåˆæˆæ¨¡å‹ï¼Œå¼ºåˆ¶è¦æ±‚åˆ†è§£åé‡æ–°åˆæˆçš„å›¾åƒä¸åŸå§‹å›¾åƒä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ¸è¿›å¼è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹ï¼Œè¿­ä»£åœ°ä½¿ç”¨æ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡æ ·æœ¬æ‰©å……è®­ç»ƒé›†ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å¿—-ç‰©ä½“åˆ†è§£ç­‰å…·æœ‰å¤æ‚éçº¿æ€§äº¤äº’çš„ä»»åŠ¡ä¸­ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®ä¸”è¿è´¯çš„åˆ†è§£ï¼Œå¹¶æœ‰æ•ˆä¿ç•™å„å±‚çš„å®Œæ•´æ€§ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ³›åŒ–è‡³å…¶ä»–ç±»å‹çš„å›¾åƒåˆ†è§£ä»»åŠ¡ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºç»Ÿä¸€åˆ†å±‚å›¾åƒåˆ†è§£æ¡†æ¶çš„æ½œåŠ›ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.20989" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.20989.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.20951</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.20951" target="_blank">See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis</a>
      </h3>
      <p class="paper-title-zh">è§‚å¯Ÿä¸ä¿®å¤ç¼ºé™·ï¼šé€šè¿‡æ™ºèƒ½ä½“æ•°æ®åˆæˆä½¿è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç†è§£è§†è§‰ä¼ªå½±</p>
      <p class="paper-authors">Jaehyun Park, Minyoung Ahn, Minkyu Kim, Jonghyun Lee, Jae-Gil Lee ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ArtiAgentæ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆæˆåŒ…å«ä¸°å¯Œä¼ªå½±æ ‡æ³¨çš„å¤§è§„æ¨¡å›¾åƒæ•°æ®é›†ï¼Œè§£å†³äº†äººå·¥æ ‡æ³¨æˆæœ¬é«˜ã€éš¾ä»¥æ‰©å±•çš„é—®é¢˜ï¼Œä¸ºè§†è§‰ä¼ªå½±çš„è¯†åˆ«ä¸ç¼“è§£ç ”ç©¶æä¾›äº†é«˜æ•ˆçš„æ•°æ®åŸºç¡€ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ™ºèƒ½ä½“çš„ç³»ç»Ÿï¼š1ï¼‰æ„ŸçŸ¥æ™ºèƒ½ä½“ä»çœŸå®å›¾åƒä¸­è¯†åˆ«å¹¶å®šä½å®ä½“å’Œå­å®ä½“ï¼›2ï¼‰åˆæˆæ™ºèƒ½ä½“é€šè¿‡åœ¨æ‰©æ•£å˜æ¢å™¨ä¸­è¿›è¡Œæ–°é¢–çš„å—çº§åµŒå…¥æ“ä½œï¼Œåˆ©ç”¨ä¼ªå½±æ³¨å…¥å·¥å…·å‘å›¾åƒä¸­å¼•å…¥ä¼ªå½±ï¼›3ï¼‰ç­–å±•æ™ºèƒ½ä½“å¯¹åˆæˆçš„ä¼ªå½±è¿›è¡Œç­›é€‰ï¼Œå¹¶ä¸ºæ¯ä¸ªå®ä¾‹ç”Ÿæˆå±€éƒ¨å’Œå…¨å±€çš„è§£é‡Šè¯´æ˜ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åˆ©ç”¨ArtiAgentåˆæˆäº†10ä¸‡å¼ å¸¦æœ‰ä¸°å¯Œä¼ªå½±æ ‡æ³¨çš„å›¾åƒï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸åº”ç”¨ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œè¡¨æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆã€è‡ªåŠ¨åœ°ç”Ÿæˆé«˜è´¨é‡çš„ä¼ªå½±-å›¾åƒå¯¹æ•°æ®ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.20951" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.20951.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.20903</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.20903" target="_blank">TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering</a>
      </h3>
      <p class="paper-title-zh">TextPeckerï¼šé€šè¿‡å¥–åŠ±ç»“æ„å¼‚å¸¸é‡åŒ–æ¥å¢å¼ºè§†è§‰æ–‡æœ¬æ¸²æŸ“</p>
      <p class="paper-authors">Hanshen Zhu, Yuliang Liu, Xuecheng Wu, An-Lan Wang, Hao Feng ç­‰ (10 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†TextPeckerï¼Œä¸€ç§å³æ’å³ç”¨çš„ç»“æ„å¼‚å¸¸æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹éš¾ä»¥æ„ŸçŸ¥æ–‡æœ¬ç»“æ„å¼‚å¸¸ï¼ˆå¦‚æ‰­æ›²ã€æ¨¡ç³Šã€é”™ä½ï¼‰çš„å…³é”®ç“¶é¢ˆï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è§†è§‰æ–‡æœ¬æ¸²æŸ“çš„ç»“æ„ä¿çœŸåº¦ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•ä¸»è¦åŒ…æ‹¬ï¼š1ï¼‰æ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰å­—ç¬¦çº§ç»“æ„å¼‚å¸¸æ ‡æ³¨çš„è¯†åˆ«æ•°æ®é›†ï¼›2ï¼‰å¼€å‘äº†ä¸€ä¸ªç¬”ç”»ç¼–è¾‘åˆæˆå¼•æ“ï¼Œä»¥æ‰©å±•å¯¹ç»“æ„é”™è¯¯çš„è¦†ç›–èŒƒå›´ï¼›3ï¼‰åŸºäºæ­¤ï¼Œè®¾è®¡äº†ä¸€ç§èƒ½å¤Ÿæ„ŸçŸ¥ç»“æ„å¼‚å¸¸çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯å‡è½»å™ªå£°å¥–åŠ±ä¿¡å·ï¼Œå¹¶èƒ½ä¸ä»»ä½•æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨é…åˆä½¿ç”¨ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼ŒTextPeckerèƒ½æŒç»­æå‡å¤šç§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ã€‚å³ä½¿åœ¨å·²å……åˆ†ä¼˜åŒ–çš„Qwen-Imageæ¨¡å‹ä¸Šï¼Œå®ƒä¹Ÿèƒ½åœ¨ä¸­æ–‡æ–‡æœ¬æ¸²æŸ“ä¸­ï¼Œå°†ç»“æ„ä¿çœŸåº¦å¹³å‡æå‡4%ï¼Œè¯­ä¹‰å¯¹é½åº¦å¹³å‡æå‡8.7%ï¼Œä»è€Œåœ¨é«˜ä¿çœŸè§†è§‰æ–‡æœ¬æ¸²æŸ“é¢†åŸŸç¡®ç«‹äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.20903" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.20903.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.20880</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.20880" target="_blank">When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance</a>
      </h3>
      <p class="paper-title-zh">å½“å®‰å…¨å‘ç”Ÿç¢°æ’ï¼šé€šè¿‡è‡ªé€‚åº”å®‰å…¨å¼•å¯¼è§£å†³æ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šç±»åˆ«æœ‰å®³å†²çª</p>
      <p class="paper-authors">Yongli Xiang, Ziming Hong, Zhaoqing Wang, Xiangyu Zhao, Bo Han ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†å†²çªæ„ŸçŸ¥è‡ªé€‚åº”å®‰å…¨å¼•å¯¼ï¼ˆCASGï¼‰æ¡†æ¶ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯†åˆ«å¹¶è§£å†³äº†æ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ä¸­ä¸åŒæœ‰å®³ç±»åˆ«ä¹‹é—´çš„â€œæœ‰å®³å†²çªâ€é—®é¢˜ï¼Œå³åœ¨æŠ‘åˆ¶ä¸€ç±»æœ‰å®³å†…å®¹æ—¶å¯èƒ½æ— æ„é—´åŠ å‰§å¦ä¸€ç±»æœ‰å®³å†…å®¹çš„ç”Ÿæˆã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> CASGæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼š1ï¼‰å†²çªæ„ŸçŸ¥ç±»åˆ«è¯†åˆ«ï¼ˆCaCIï¼‰ï¼Œæ ¹æ®æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„åŠ¨æ€çŠ¶æ€ï¼Œè¯†åˆ«å‡ºä¸ä¹‹æœ€ç›¸å…³çš„å•ä¸€æœ‰å®³ç±»åˆ«ï¼›2ï¼‰å†²çªè§£å†³å¼•å¯¼åº”ç”¨ï¼ˆCrGAï¼‰ï¼Œä»…æ²¿ç€è¯†åˆ«å‡ºçš„å•ä¸€æœ‰å®³ç±»åˆ«æ–¹å‘æ–½åŠ å®‰å…¨å¼•å¯¼ï¼Œé¿å…å¤šç±»åˆ«å¼•å¯¼é—´çš„ç›¸äº’å¹²æ‰°ã€‚è¯¥æ–¹æ³•å¯åŒæ—¶åº”ç”¨äºæ½œåœ¨ç©ºé—´å’Œæ–‡æœ¬ç©ºé—´çš„å®‰å…¨é˜²æŠ¤æœºåˆ¶ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨æ–‡ç”Ÿå›¾å®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCASGå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å°†æœ‰å®³å†…å®¹ç”Ÿæˆç‡é™ä½é«˜è¾¾15.4%ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šç±»åˆ«æœ‰å®³å†²çªé—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ•´ä½“å®‰å…¨æ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to "harmful conflicts" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.20880" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.20880.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>