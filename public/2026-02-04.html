<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-04</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 5</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.01046</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 95/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.01046" target="_blank">ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction</a>
      </h3>
      <p class="paper-title-zh">ReLayoutï¼šé€šè¿‡å…³ç³»æ„ŸçŸ¥è®¾è®¡é‡æ„å®ç°å¤šåŠŸèƒ½ä¸”ä¿æŒç»“æ„çš„è®¾è®¡å¸ƒå±€ç¼–è¾‘</p>
      <p class="paper-authors">Jiawei Lin, Shizhao Sun, Danqing Huang, Ting Liu, Ji Li ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ReLayoutæ¡†æ¶ï¼Œé¦–æ¬¡å®ç°äº†æ— éœ€ä¸‰å…ƒç»„è®­ç»ƒæ•°æ®çš„ã€å¤šåŠŸèƒ½ä¸”èƒ½ä¿æŒå¸ƒå±€ç»“æ„çš„è®¾è®¡è‡ªåŠ¨ç¼–è¾‘ï¼Œå¹¶é€šè¿‡å¼•å…¥å››ç§åŸºæœ¬ç¼–è¾‘æ“ä½œæ ‡å‡†åŒ–äº†ç¼–è¾‘ä»»åŠ¡æ ¼å¼ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆæ„å»ºæè¿°æœªç¼–è¾‘å…ƒç´ é—´ä½ç½®ä¸å°ºå¯¸å…³ç³»çš„å…³ç³»å›¾ï¼Œä½œä¸ºä¿æŒå¸ƒå±€ç»“æ„çš„çº¦æŸï¼›ç„¶åæå‡ºå…³ç³»æ„ŸçŸ¥è®¾è®¡é‡æ„æ–¹æ³•ï¼Œé€šè¿‡è®©æ¨¡å‹å­¦ä¹ ä»å…ƒç´ ã€å…³ç³»å›¾åŠåˆæˆç¼–è¾‘æ“ä½œä¸­é‡å»ºè®¾è®¡ï¼Œä»¥è‡ªç›‘ç£æ–¹å¼æ¨¡æ‹Ÿç¼–è¾‘è¿‡ç¨‹ï¼›é‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œç»Ÿä¸€å¤„ç†å¤šç§ç¼–è¾‘æ“ä½œã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> ReLayoutåœ¨ç¼–è¾‘è´¨é‡ã€å‡†ç¡®æ€§å’Œå¸ƒå±€ç»“æ„ä¿æŒæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå®šæ€§ã€å®šé‡ç»“æœä¸ç”¨æˆ·ç ”ç©¶å‡éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¸”å•ä¸€æ¨¡å‹å³å¯å®ç°å¤šç§ç¼–è¾‘åŠŸèƒ½ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.01046" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.01046.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.03448</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.03448" target="_blank">Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation</a>
      </h3>
      <p class="paper-title-zh">é¢å‘å¤šä¸»ä½“å›¾åƒç”Ÿæˆçš„åˆ†å±‚æ¦‚å¿µ-å¤–è§‚å¼•å¯¼</p>
      <p class="paper-authors">Yijia Xu, Zihao Wang, Jinshi Cui</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†åˆ†å±‚æ¦‚å¿µ-å¤–è§‚å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼çš„ç»“æ„åŒ–ç›‘ç£ï¼Œè§£å†³äº†å¤šä¸»ä½“å›¾åƒç”Ÿæˆä¸­çš„èº«ä»½ä¸ä¸€è‡´å’Œç»„åˆæ§åˆ¶å—é™é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬æŒ‡ä»¤è·Ÿéšå’Œä¸»ä½“ä¸€è‡´æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> 1. åœ¨æ¦‚å¿µå±‚é¢ï¼Œé‡‡ç”¨VAEç‰¹å¾éšæœºä¸¢å¼ƒè®­ç»ƒç­–ç•¥ï¼Œè¿«ä½¿æ¨¡å‹æ›´ä¾èµ–è§†è§‰è¯­è¨€æ¨¡å‹æä¾›çš„é²æ£’è¯­ä¹‰ä¿¡å·ï¼Œä»¥å¢å¼ºæ¦‚å¿µä¸€è‡´æ€§ã€‚2. åœ¨å¤–è§‚å±‚é¢ï¼Œå°†è§†è§‰è¯­è¨€æ¨¡å‹æ¨å¯¼çš„å¯¹åº”å…³ç³»é›†æˆåˆ°æ‰©æ•£Transformerä¸­ï¼Œæ„å»ºå¯¹åº”æ„ŸçŸ¥çš„æ©ç æ³¨æ„åŠ›æ¨¡å—ï¼Œä½¿æ¯ä¸ªæ–‡æœ¬è¯ä»…å…³æ³¨å…¶åŒ¹é…çš„å‚è€ƒåŒºåŸŸï¼Œå®ç°ç²¾ç¡®å±æ€§ç»‘å®šã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸»ä½“å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ–‡æœ¬æŒ‡ä»¤è·Ÿéšå’Œä¸»ä½“ä¸€è‡´æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.03448" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.03448.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.03410</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.03410" target="_blank">UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning</a>
      </h3>
      <p class="paper-title-zh">UnHypeï¼šåŸºäºCLIPå¼•å¯¼çš„è¶…ç½‘ç»œç”¨äºåŠ¨æ€LoRAé—å¿˜å­¦ä¹ </p>
      <p class="paper-authors">Piotr WÃ³jcik, Maksym Petrenko, Wojciech Gromski, PrzemysÅ‚aw Spurek, Maciej Zieba</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†UnHypeæ¡†æ¶ï¼Œé€šè¿‡å°†è¶…ç½‘ç»œå¼•å…¥LoRAè®­ç»ƒï¼Œå®ç°äº†å¯¹æ‰©æ•£æ¨¡å‹ä¸­ç‰¹å®šæ¦‚å¿µï¼ˆå¦‚æœ‰å®³å†…å®¹ï¼‰çš„åŠ¨æ€ã€å¯æ‰©å±•ä¸”è¯­ä¹‰æ„ŸçŸ¥çš„â€œé—å¿˜â€ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ•´ä½“ç”Ÿæˆèƒ½åŠ›ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•å°†è¶…ç½‘ç»œé›†æˆåˆ°å•æ¦‚å¿µå’Œå¤šæ¦‚å¿µçš„LoRAè®­ç»ƒæµç¨‹ä¸­ã€‚è¶…ç½‘ç»œä»¥è¾“å…¥æ–‡æœ¬çš„CLIPåµŒå…¥ä¸ºæ¡ä»¶ï¼ŒåŠ¨æ€ç”Ÿæˆè‡ªé€‚åº”çš„LoRAæƒé‡ï¼Œä»è€Œå®ç°å¯¹ä¸åŒæ¦‚å¿µçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ§åˆ¶ã€‚è¯¥æ¶æ„å¯ç›´æ¥åµŒå…¥Stable Diffusionç­‰ç°ä»£åŸºäºæµçš„æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œå¹¶è¡¨ç°å‡ºç¨³å®šçš„è®­ç»ƒè¡Œä¸ºã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ç‰©ä½“æ“¦é™¤ã€åäººæ“¦é™¤å’Œéœ²éª¨å†…å®¹ç§»é™¤ç­‰å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¯„ä¼°è¡¨æ˜ï¼ŒUnHypeèƒ½æœ‰æ•ˆä¸”çµæ´»åœ°ç§»é™¤ç›®æ ‡æ¦‚å¿µï¼Œåœ¨æ“¦é™¤ç´§å¯†ç›¸å…³æ¦‚å¿µä¸ä¿æŒæ›´å¹¿æ³›è¯­ä¹‰çš„æ³›åŒ–èƒ½åŠ›ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œå¹¶è§£å†³äº†å¤šæ¦‚å¿µåŒæ—¶æ“¦é™¤çš„å¯æ‰©å±•æ€§éš¾é¢˜ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: https://github.com/gmum/UnHype.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.03410" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.03410.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.03339</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.03339" target="_blank">Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability</a>
      </h3>
      <p class="paper-title-zh">å…·æœ‰å¯å­¦ä¹ æ€§æ— ç”Ÿæˆå™¨è¯Šæ–­çš„å¯ç»„åˆè§†è§‰åˆ†è¯å™¨</p>
      <p class="paper-authors">Bingchen Zhao, Qiushan Guo, Ye Wang, Yixuan Huang, Zhonghua Zhai ç­‰ (6 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†CompTokè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ å…·æœ‰å¢å¼ºç»„åˆæ€§çš„è§†è§‰åˆ†è¯å™¨ï¼Œå¹¶è®¾è®¡äº†ä¸¤ç§æ— éœ€ç”Ÿæˆå™¨çš„åº¦é‡æŒ‡æ ‡æ¥è¯Šæ–­åˆ†è¯ç©ºé—´çš„ç»„åˆæ€§å’Œå¯å­¦ä¹ æ€§ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> CompToké‡‡ç”¨åŸºäºä»¤ç‰Œæ¡ä»¶çš„æ‰©æ•£è§£ç å™¨ï¼Œé€šè¿‡InfoGANé£æ ¼çš„ç›®æ ‡å‡½æ•°è®­ç»ƒè¯†åˆ«æ¨¡å‹ï¼Œè¿«ä½¿è§£ç å™¨ä¸å¿½ç•¥ä»»ä½•ä»¤ç‰Œã€‚ä¸ºæå‡ç»„åˆæ§åˆ¶èƒ½åŠ›ï¼Œæ¡†æ¶åœ¨è®­ç»ƒæ—¶å¯¹å›¾åƒé—´ä»¤ç‰Œå­é›†è¿›è¡Œäº¤æ¢ï¼Œå¹¶åˆ©ç”¨å¯¹æŠ—æµæ­£åˆ™åŒ–å™¨æ–½åŠ æµå½¢çº¦æŸï¼Œç¡®ä¿äº¤æ¢ç”Ÿæˆçš„ç»“æœä¿æŒåœ¨è‡ªç„¶å›¾åƒåˆ†å¸ƒä¸Šã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> CompTokåœ¨å›¾åƒç±»åˆ«æ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå…¶ä»¤ç‰Œæ”¯æŒé€šè¿‡äº¤æ¢å®ç°å›¾åƒé«˜å±‚è¯­ä¹‰ç¼–è¾‘ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæå‡æ‰€æå‡ºçš„ç»„åˆæ€§ä¸å¯å­¦ä¹ æ€§åº¦é‡æŒ‡æ ‡ï¼Œå¹¶æ”¯æŒæœ€å…ˆè¿›çš„ç±»åˆ«æ¡ä»¶ç”Ÿæˆå™¨ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>We introduce CompTok, a training framework for learning visual tokenizers whose tokens are enhanced for compositionality. CompTok uses a token-conditioned diffusion decoder. By employing an InfoGAN-style objective, where we train a recognition model to predict the tokens used to condition the diffusion decoder using the decoded images, we enforce the decoder to not ignore any of the tokens. To promote compositional control, besides the original images, CompTok also trains on tokens formed by swapping token subsets between images, enabling more compositional control of the token over the decoder. As the swapped tokens between images do not have ground truth image targets, we apply a manifold constraint via an adversarial flow regularizer to keep unpaired swap generations on the natural-image distribution. The resulting tokenizer not only achieves state-of-the-art performance on image class-conditioned generation, but also demonstrates properties such as swapping tokens between images to achieve high level semantic editing of an image. Additionally, we propose two metrics that measures the landscape of the token space that can be useful to describe not only the compositionality of the tokens, but also how easy to learn the landscape is for a generator to be trained on this space. We show in experiments that CompTok can improve on both of the metrics as well as supporting state-of-the-art generators for class conditioned generation.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.03339" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.03339.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.03220</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.03220" target="_blank">PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation</a>
      </h3>
      <p class="paper-title-zh">PokeFusionæ³¨æ„åŠ›ï¼šå¢å¼ºæ— å‚è€ƒçš„é£æ ¼æ¡ä»¶ç”Ÿæˆ</p>
      <p class="paper-authors">Jingbang Tang</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è§£ç å™¨çº§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆPokeFusion Attentionï¼‰ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€å‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜è´¨é‡ã€é£æ ¼ä¸€è‡´ä¸”ç»“æ„ç¨³å®šçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸»å¹²å®Œå…¨å†»ç»“ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åœ¨æ‰©æ•£è§£ç å™¨å†…éƒ¨ï¼Œé€šè¿‡ä¸€ç§è§£è€¦çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†æ–‡æœ¬è¯­ä¹‰ä¸å­¦ä¹ åˆ°çš„é£æ ¼åµŒå…¥ç›´æ¥èåˆã€‚å®ƒä»…è®­ç»ƒè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚å’Œä¸€ä¸ªç´§å‡‘çš„é£æ ¼æŠ•å½±æ¨¡å—ï¼Œä»è€Œå½¢æˆä¸€ä¸ªå‚æ•°é«˜æ•ˆã€å³æ’å³ç”¨çš„æ§åˆ¶ç»„ä»¶ã€‚è¯¥ç»„ä»¶å¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹æµç¨‹ä¸­ï¼Œå¹¶è¿ç§»åˆ°ä¸åŒçš„ä¸»å¹²ç½‘ç»œä¸Šã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨é£æ ¼åŒ–è§’è‰²ç”ŸæˆåŸºå‡†ï¼ˆå®å¯æ¢¦é£æ ¼ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä»£è¡¨æ€§çš„åŸºäºé€‚é…å™¨çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é£æ ¼ä¿çœŸåº¦ã€è¯­ä¹‰å¯¹é½å’Œè§’è‰²å½¢çŠ¶ä¸€è‡´æ€§æ–¹é¢å‡æœ‰æŒç»­æå‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„å‚æ•°é‡å¼€é”€å’Œç®€å•çš„æ¨ç†è¿‡ç¨‹ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment flexibility.We propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully frozen.PokeFusion Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different backbones.Experiments on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.03220" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.03220.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>