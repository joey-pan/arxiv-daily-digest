<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ“š ArXiv Daily Digest</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Noto+Sans+SC:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #0f0f0f;
            --card-bg: #1a1a1a;
            --card-border: #2a2a2a;
            --text: #e0e0e0;
            --text-muted: #888;
            --accent: #6366f1;
            --accent-hover: #818cf8;
            --success: #22c55e;
        }
        
        * { box-sizing: border-box; margin: 0; padding: 0; }
        
        body {
            font-family: 'Inter', 'Noto Sans SC', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }
        
        header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--card-border);
        }
        
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; }
        
        .subtitle {
            color: var(--text-muted);
            font-size: 1.1rem;
        }
        
        .date {
            font-size: 1.2rem;
            color: var(--accent);
            margin: 1rem 0;
        }
        
        .stats {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .archive-link-top {
            margin-top: 1rem;
            font-size: 0.9rem;
        }

        .archive-link-top a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .paper-card {
            background: var(--card-bg);
            border: 1px solid var(--card-border);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: border-color 0.2s;
        }
        
        .paper-card:hover {
            border-color: var(--accent);
        }
        
        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 0.75rem;
            gap: 0.75rem;
        }

        .left-header {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .category-badge {
            background: var(--accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        
        .paper-id {
            color: var(--text-muted);
            font-size: 0.85rem;
            font-family: monospace;
        }

        .score-badge {
            border-radius: 999px;
            border: 1px solid var(--accent);
            padding: 0.15rem 0.6rem;
            font-size: 0.8rem;
            color: var(--accent);
            background: rgba(99, 102, 241, 0.12);
        }
        
        .paper-title {
            font-size: 1.15rem;
            line-height: 1.4;
            margin-bottom: 0.5rem;
        }
        
        .paper-title a {
            color: var(--text);
            text-decoration: none;
        }
        
        .paper-title a:hover {
            color: var(--accent);
        }
        
        .paper-title-zh {
            color: var(--text-muted);
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }
        
        .paper-authors {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        
        .paper-summary {
            background: rgba(99, 102, 241, 0.1);
            border-radius: 8px;
            padding: 1rem;
            margin-bottom: 1rem;
        }
        
        .summary-section {
            margin-bottom: 0.5rem;
        }
        
        .summary-section:last-child {
            margin-bottom: 0;
        }
        
        .summary-section strong {
            color: var(--accent);
        }
        
        .paper-abstract {
            margin-bottom: 1rem;
        }
        
        .paper-abstract summary {
            cursor: pointer;
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        .paper-abstract p {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: var(--text-muted);
        }
        
        .paper-links {
            display: flex;
            gap: 0.75rem;
        }
        
        .link-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.25rem;
            padding: 0.5rem 1rem;
            background: var(--card-border);
            color: var(--text);
            text-decoration: none;
            border-radius: 6px;
            font-size: 0.85rem;
            transition: background 0.2s;
        }
        
        .link-btn:hover {
            background: var(--accent);
        }
        
        footer {
            text-align: center;
            padding-top: 2rem;
            margin-top: 2rem;
            border-top: 1px solid var(--card-border);
            color: var(--text-muted);
            font-size: 0.9rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            h1 { font-size: 1.75rem; }
            .paper-card { padding: 1rem; }
        }
    </style>
</head>
<body>
    <header>
        <h1>ğŸ“š ArXiv Daily Digest</h1>
        <p class="subtitle">æ¯æ—¥è®ºæ–‡ç²¾é€‰</p>
        <p class="date">ğŸ“… 2026-02-16</p>
        <p class="stats">å…± 5 ç¯‡è®ºæ–‡ | è®¡ç®—æœºè§†è§‰: 3 | æœºå™¨å­¦ä¹ : 2</p>
        <p class="archive-link-top"><a href="archive.html">æŸ¥çœ‹å†å²å½’æ¡£ â†’</a></p>
    </header>
    
    <main>
        
    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.13055</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 95/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.13055" target="_blank">Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation</a>
      </h3>
      <p class="paper-title-zh">Curriculum-DPO++ï¼šé€šè¿‡æ•°æ®å’Œæ¨¡å‹è¯¾ç¨‹è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç›´æ¥åå¥½ä¼˜åŒ–</p>
      <p class="paper-authors">Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, Mubarak Shah</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†Curriculum-DPO++æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ•°æ®çº§å’Œæ¨¡å‹çº§è¯¾ç¨‹å­¦ä¹ ï¼Œæ”¹è¿›äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è®­ç»ƒæ•ˆç‡ä¸æ•ˆæœã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> æ–¹æ³•åœ¨åŸæœ‰æ•°æ®çº§è¯¾ç¨‹ï¼ˆæŒ‰éš¾åº¦ç»„ç»‡å›¾åƒå¯¹ï¼‰çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†æ¨¡å‹çº§è¯¾ç¨‹ï¼š1ï¼‰è®­ç»ƒåˆæœŸä»…ä½¿ç”¨éƒ¨åˆ†å¯è®­ç»ƒå±‚ï¼Œéšåé€æ­¥è§£å†»å±‚ç›´è‡³å®Œæ•´æ¶æ„ï¼›2ï¼‰åŸºäºLoRAå¾®è°ƒæ—¶ï¼ŒåŠ¨æ€å¢åŠ ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œä»è¾ƒå°å®¹é‡å¼€å§‹é€æ­¥æå‡è‡³åŸºå‡†æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ›¿ä»£çš„æ’åºç­–ç•¥ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> åœ¨ä¹ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCurriculum-DPO++åœ¨æ–‡æœ¬å¯¹é½åº¦ã€å›¾åƒç¾è§‚åº¦å’Œäººç±»åå¥½æ–¹é¢å‡ä¼˜äºCurriculum-DPOåŠå…¶ä»–å…ˆè¿›åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œè¯æ˜äº†ç»“åˆæ•°æ®ä¸æ¨¡å‹è¯¾ç¨‹çš„æœ‰æ•ˆæ€§ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.13055" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.13055.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12769</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12769" target="_blank">PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion</a>
      </h3>
      <p class="paper-title-zh">PixelRushï¼šé€šè¿‡ä¸€æ­¥æ‰©æ•£å®ç°è¶…å¿«é€Ÿã€å…è®­ç»ƒçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Hong-Phuc Lai, Phong Nguyen, Anh Tran</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†é¦–ä¸ªå…è°ƒä¼˜çš„å®ç”¨é«˜åˆ†è¾¨ç‡æ–‡ç”Ÿå›¾æ¡†æ¶PixelRushï¼Œåœ¨ä¿æŒå“è¶Šè§†è§‰ä¿çœŸåº¦çš„åŒæ—¶ï¼Œå®ç°äº†æ¯”ç°æœ‰æ–¹æ³•10è‡³35å€çš„ç”Ÿæˆé€Ÿåº¦æå‡ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•åŸºäºå·²æœ‰çš„åˆ†å—æ¨ç†èŒƒå¼ï¼Œä½†æ¶ˆé™¤äº†å¤šæ¬¡åè½¬å’Œå†ç”Ÿå¾ªç¯çš„éœ€æ±‚ï¼Œå®ç°äº†ä½æ­¥æ•°ä¸‹çš„é«˜æ•ˆåˆ†å—å»å™ªã€‚é’ˆå¯¹å°‘æ­¥ç”Ÿæˆä¸­åˆ†å—èåˆäº§ç”Ÿçš„ä¼ªå½±ï¼Œæå‡ºäº†ä¸€ç§æ— ç¼èåˆç­–ç•¥ï¼Œå¹¶é€šè¿‡å™ªå£°æ³¨å…¥æœºåˆ¶ç¼“è§£è¿‡åº¦å¹³æ»‘æ•ˆåº”ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> PixelRushå…·æœ‰æé«˜çš„æ•ˆç‡ï¼Œç”Ÿæˆå•å¼ 4Kå›¾åƒä»…éœ€çº¦20ç§’ï¼Œç›¸æ¯”ç°æœ‰å…ˆè¿›æ–¹æ³•å¤§å¹…åŠ é€Ÿï¼ŒåŒæ—¶é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†å…¶æ€§èƒ½ä¼˜åŠ¿ä¸è¾“å‡ºè´¨é‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12769" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12769.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">æœºå™¨å­¦ä¹ </span>
          <span class="paper-id">2602.12675</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12675" target="_blank">SLA2: Sparse-Linear Attention with Learnable Routing and QAT</a>
      </h3>
      <p class="paper-title-zh">SLA2ï¼šå…·æœ‰å¯å­¦ä¹ è·¯ç”±å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒçš„ç¨€ç–-çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶</p>
      <p class="paper-authors">Jintao Zhang, Haoxu Wang, Kai Jiang, Kaiwen Zheng, Youhe Jiang ç­‰ (9 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æœ¬æ–‡æå‡ºäº†SLA2ï¼Œé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„è·¯ç”±æœºåˆ¶å’Œæ›´ç²¾ç¡®çš„ç¨€ç–-çº¿æ€§æ³¨æ„åŠ›åˆ†è§£å…¬å¼ï¼Œæ”¹è¿›äº†åŸæœ‰çš„ç¨€ç–-çº¿æ€§æ³¨æ„åŠ›æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†é‡åŒ–æ„ŸçŸ¥å¾®è°ƒæ¥é™ä½é‡åŒ–è¯¯å·®ï¼Œä»è€Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> SLA2åŒ…å«ä¸‰ä¸ªå…³é”®è®¾è®¡ï¼šé¦–å…ˆï¼Œä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„è·¯ç”±å™¨åŠ¨æ€å†³å®šæ¯ä¸ªæ³¨æ„åŠ›è®¡ç®—åº”ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›è¿˜æ˜¯çº¿æ€§æ³¨æ„åŠ›ï¼›å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ç§æ›´å¿ å®ã€æ›´ç›´æ¥çš„ç¨€ç–-çº¿æ€§æ³¨æ„åŠ›åˆ†è§£å…¬å¼ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æ¯”ä¾‹ç³»æ•°ç»“åˆä¸¤ä¸ªåˆ†æ”¯ï¼›æœ€åï¼Œè®¾è®¡äº†ç¨€ç–+ä½æ¯”ç‰¹æ³¨æ„åŠ›æ¶æ„ï¼Œé€šè¿‡é‡åŒ–æ„ŸçŸ¥å¾®è°ƒå¼•å…¥ä½æ¯”ç‰¹æ³¨æ„åŠ›ä»¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸Šï¼ŒSLA2èƒ½å¤Ÿå®ç°97%çš„æ³¨æ„åŠ›ç¨€ç–åº¦ï¼Œå¹¶å°†æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æå‡18.6å€ï¼ŒåŒæ—¶ä¿æŒäº†åŸæœ‰çš„ç”Ÿæˆè´¨é‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12675" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12675.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">è®¡ç®—æœºè§†è§‰</span>
          <span class="paper-id">2602.12640</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12640" target="_blank">ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models</a>
      </h3>
      <p class="paper-title-zh">ImageRAGTurboï¼šè¿ˆå‘åŸºäºæ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹çš„ä¸€æ­¥å¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ</p>
      <p class="paper-authors">Peijie Qiu, Hariharan Ramshankar, Arnau Ramisa, RenÃ© Vidal, Amit Kumar K C ç­‰ (7 ä½ä½œè€…)</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºImageRAGTurboï¼Œä¸€ç§é€šè¿‡æ£€ç´¢å¢å¼ºé«˜æ•ˆå¾®è°ƒå°‘æ­¥æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜è´¨é‡ã€ä½å»¶è¿Ÿçš„ä¸€æ­¥å¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ŒåŒæ—¶é¿å…æ˜‚è´µçš„è®­ç»ƒæˆæœ¬ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> è¯¥æ–¹æ³•é¦–å…ˆæ ¹æ®æ–‡æœ¬æç¤ºä»æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬-å›¾åƒå¯¹ï¼Œç”¨äºè¾…åŠ©ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡å°†æ£€ç´¢å†…å®¹æ³¨å…¥UNetå»å™ªå™¨çš„éšç©ºé—´ï¼ˆH-spaceï¼‰æ¥æå‡æç¤ºå¯¹é½åº¦ï¼Œæ— éœ€å¾®è°ƒå³å¯æ”¹å–„ç”Ÿæˆæ•ˆæœã€‚ä¸ºè¿›ä¸€æ­¥æå‡è´¨é‡ï¼Œåœ¨H-spaceä¸­å¼•å…¥å¯è®­ç»ƒçš„é€‚é…å™¨ï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†æ£€ç´¢å†…å®¹ä¸ç›®æ ‡æç¤ºé«˜æ•ˆèåˆã€‚æ•´ä¸ªæ–¹æ³•ä¸“æ³¨äºåœ¨æå°‘çš„å»å™ªæ­¥éª¤ï¼ˆå¦‚ä¸€æ­¥ï¼‰å†…ä¿æŒå›¾åƒè´¨é‡ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¿«é€Ÿæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œèƒ½åœ¨ä¸å¢åŠ å»¶è¿Ÿçš„æƒ…å†µä¸‹ç”Ÿæˆé«˜ä¿çœŸå›¾åƒã€‚å³ä½¿ä»…é€šè¿‡æ£€ç´¢å†…å®¹ç¼–è¾‘éšç©ºé—´è€Œä¸è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿèƒ½æœ‰æ•ˆæå‡æç¤ºå¯¹é½åº¦ï¼›åŠ å…¥å¯è®­ç»ƒé€‚é…å™¨åï¼Œå›¾åƒè´¨é‡å¾—åˆ°è¿›ä¸€æ­¥æ”¹å–„ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12640" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12640.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    

    <article class="paper-card">
      <div class="paper-header">
        <div class="left-header">
          <span class="category-badge">æœºå™¨å­¦ä¹ </span>
          <span class="paper-id">2602.12624</span>
        </div>
        <span class="score-badge">ç›¸å…³æ€§ 85/100</span>
      </div>
      <h3 class="paper-title">
        <a href="https://arxiv.org/abs/2602.12624" target="_blank">Formalizing the Sampling Design Space of Diffusion-Based Generative Models via Adaptive Solvers and Wasserstein-Bounded Timesteps</a>
      </h3>
      <p class="paper-title-zh">é€šè¿‡è‡ªé€‚åº”æ±‚è§£å™¨ä¸Wassersteinæœ‰ç•Œæ—¶é—´æ­¥é•¿å½¢å¼åŒ–æ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„é‡‡æ ·è®¾è®¡ç©ºé—´</p>
      <p class="paper-authors">Sangwoo Jo, Sungjoon Choi</p>
      
      <div class="paper-summary">
        <div class="summary-section"><strong>æ ¸å¿ƒè´¡çŒ®:</strong> æå‡ºäº†ä¸€ä¸ªåä¸ºSDMçš„åŸåˆ™æ€§æ¡†æ¶ï¼Œé€šè¿‡å‡ ä½•è§†è§’å°†æ•°å€¼æ±‚è§£å™¨ä¸æ‰©æ•£è½¨è¿¹çš„å†…åœ¨ç‰¹æ€§å¯¹é½ï¼Œå¹¶å¼•å…¥Wassersteinæœ‰ç•Œçš„ä¼˜åŒ–æ¡†æ¶æ¥å½¢å¼åŒ–æ—¶é—´æ­¥é•¿è°ƒåº¦ï¼Œä»è€Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆ–æ¶æ„ä¿®æ”¹çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½é‡‡æ ·æˆæœ¬ã€‚</div>
        <div class="summary-section"><strong>æ–¹æ³•:</strong> é¦–å…ˆä»åˆ†ææ‰©æ•£è¿‡ç¨‹çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰åŠ¨åŠ›å­¦å‡ºå‘ï¼Œå‘ç°æ—©æœŸé«˜å™ªå£°é˜¶æ®µä½¿ç”¨ä½é˜¶æ±‚è§£å™¨å·²è¶³å¤Ÿï¼Œè€ŒåæœŸéçº¿æ€§å¢å¼ºé˜¶æ®µå¯é€æ­¥é‡‡ç”¨é«˜é˜¶æ±‚è§£å™¨ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¼•å…¥Wassersteinæœ‰ç•Œçš„ä¼˜åŒ–æ¡†æ¶ï¼Œç³»ç»Ÿæ¨å¯¼å‡ºè‡ªé€‚åº”æ—¶é—´æ­¥é•¿ï¼Œä»¥æ˜¾å¼åœ°çº¦æŸå±€éƒ¨ç¦»æ•£åŒ–è¯¯å·®ï¼Œç¡®ä¿é‡‡æ ·è¿‡ç¨‹å¿ å®äºåº•å±‚è¿ç»­åŠ¨åŠ›å­¦ã€‚</div>
        <div class="summary-section"><strong>å…³é”®å‘ç°:</strong> SDMåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨CIFAR-10ä¸ŠFIDä¸º1.93ã€FFHQä¸Šä¸º2.41ã€AFHQv2ä¸Šä¸º1.98ï¼Œä¸”ç›¸æ¯”ç°æœ‰é‡‡æ ·å™¨å‡å°‘äº†å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚è¿™è¡¨æ˜æ‰€æå‡ºçš„è‡ªé€‚åº”æ±‚è§£å™¨é€‰æ‹©ä¸æ—¶é—´æ­¥é•¿è°ƒåº¦æ–¹æ³•èƒ½æœ‰æ•ˆå¹³è¡¡é‡‡æ ·æ•ˆç‡ä¸ç”Ÿæˆè´¨é‡ã€‚</div>
      </div>
      
      <details class="paper-abstract">
        <summary>æŸ¥çœ‹åŸæ–‡æ‘˜è¦</summary>
        <p>Diffusion-based generative models have achieved remarkable performance across various domains, yet their practical deployment is often limited by high sampling costs. While prior work focuses on training objectives or individual solvers, the holistic design of sampling, specifically solver selection and scheduling, remains dominated by static heuristics. In this work, we revisit this challenge through a geometric lens, proposing SDM, a principled framework that aligns the numerical solver with the intrinsic properties of the diffusion trajectory. By analyzing the ODE dynamics, we show that efficient low-order solvers suffice in early high-noise stages while higher-order solvers can be progressively deployed to handle the increasing non-linearity of later stages. Furthermore, we formalize the scheduling by introducing a Wasserstein-bounded optimization framework. This method systematically derives adaptive timesteps that explicitly bound the local discretization error, ensuring the sampling process remains faithful to the underlying continuous dynamics. Without requiring additional training or architectural modifications, SDM achieves state-of-the-art performance across standard benchmarks, including an FID of 1.93 on CIFAR-10, 2.41 on FFHQ, and 1.98 on AFHQv2, with a reduced number of function evaluations compared to existing samplers. Our code is available at https://github.com/aiimaginglab/sdm.</p>
      </details>
      
      <div class="paper-links">
        <a href="https://arxiv.org/abs/2602.12624" target="_blank" class="link-btn">ğŸ“„ arXiv</a>
        <a href="https://arxiv.org/pdf/2602.12624.pdf" target="_blank" class="link-btn">ğŸ“¥ PDF</a>
      </div>
    </article>
    
    </main>
    
    <footer>
        <p>ç”± <a href="https://github.com">ArXiv Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ</p>
        <p>æ•°æ®æ¥æº: <a href="https://arxiv.org">arXiv.org</a> | AI æ€»ç»“: DeepSeek</p>
    </footer>
</body>
</html>